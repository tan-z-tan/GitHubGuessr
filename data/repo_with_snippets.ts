export const githubPopularRepos = [
  {
    "id": "torvalds/linux",
    "org": "torvalds",
    "avatarURL": "https://avatars.githubusercontent.com/u/1024025?v=4",
    "name": "torvalds/linux",
    "url": "https://github.com/torvalds/linux",
    "lang": "C",
    "desc": "The official Linux kernel source code, maintained by Linus Torvalds. It's the foundation for Linux operating systems, with global contributions.",
    "star_num": 157001,
    "fork_num": 49722,
    "snippets": [
      "}\n",
      "static irqreturn_t nfcmrvl_i2c_int_irq_thread_fn(int irq, void *drv_data_ptr)\n{\n\tstruct nfcmrvl_i2c_drv_data *drv_data = drv_data_ptr;\n\tstruct sk_buff *skb = NULL;\n\tint ret;\n\n\tif (!drv_data->priv)\n\t\treturn IRQ_HANDLED;\n\n\tif (test_bit(NFCMRVL_PHY_ERROR, &drv_data->priv->flags))\n\t\treturn IRQ_HANDLED;\n\n\tret = nfcmrvl_i2c_read(drv_data, &skb);\n\n\tswitch (ret) {\n\tcase -EREMOTEIO:\n\t\tset_bit(NFCMRVL_PHY_ERROR, &drv_data->priv->flags);\n\t\tbreak;\n\tcase -ENOMEM:\n\tcase -EBADMSG:\n\t\tnfc_err(&drv_data->i2c->dev, \"read failed %d\\n\", ret);\n\t\tbreak;\n\tdefault:\n\t\tif (nfcmrvl_nci_recv_frame(drv_data->priv, skb) < 0)\n\t\t\tnfc_err(&drv_data->i2c->dev, \"corrupted RX packet\\n\");\n\t\tbreak;\n\t}\n\treturn IRQ_HANDLED;\n}\n\nstatic int nfcmrvl_i2c_nci_open(struct nfcmrvl_private *priv)\n{\n\tstruct nfcmrvl_i2c_drv_data *drv_data = priv->drv_data;\n\n\tif (!drv_data)\n\t\treturn -ENODEV;\n\n\treturn 0;\n}\n\nstatic int nfcmrvl_i2c_nci_close(struct nfcmrvl_private *priv)\n{\n\treturn 0;\n}\n\nstatic int nfcmrvl_i2c_nci_send(struct nfcmrvl_private *priv,\n\t\t\t\tstruct sk_buff *skb)\n{\n\tstruct nfcmrvl_i2c_drv_data *drv_data = priv->drv_data;\n\tint ret;\n\n\tif (test_bit(NFCMRVL_PHY_ERROR, &priv->flags)) {\n\t\tkfree_skb(skb);\n\t\treturn -EREMOTEIO;\n\t}\n\n\tret = i2c_master_send(drv_data->i2c, skb->data, skb->len);\n\n\t/* Retry if chip was in standby */\n\tif (ret == -EREMOTEIO) {\n\t\tnfc_info(drv_data->dev, \"chip may sleep, retry\\n\");\n\t\tusleep_range(6000, 10000);\n\t\tret = i2c_master_send(drv_data->i2c, skb->data, skb->len);\n\t}",
      "static unsigned int siena_mem_map_size(struct efx_nic *efx)\n{\n\treturn FR_CZ_MC_TREG_SMEM +\n\t\tFR_CZ_MC_TREG_SMEM_STEP * FR_CZ_MC_TREG_SMEM_ROWS;\n}\n\nstatic int siena_probe_nic(struct efx_nic *efx)\n{\n\tstruct siena_nic_data *nic_data;\n\tefx_oword_t reg;\n\tint rc;\n\n\t/* Allocate storage for hardware specific data */\n\tnic_data = kzalloc(sizeof(struct siena_nic_data), GFP_KERNEL);\n\tif (!nic_data)\n\t\treturn -ENOMEM;\n\tnic_data->efx = efx;\n\tefx->nic_data = nic_data;\n\n\tif (efx_farch_fpga_ver(efx) != 0) {\n\t\tnetif_err(efx, probe, efx->net_dev,\n\t\t\t  \"Siena FPGA not supported\\n\");\n\t\trc = -ENODEV;\n\t\tgoto fail1;\n\t}\n\n\tefx->max_channels = EFX_MAX_CHANNELS;\n\tefx->max_vis = EFX_MAX_CHANNELS;\n\tefx->max_tx_channels = EFX_MAX_CHANNELS;\n\tefx->tx_queues_per_channel = 4;\n\n\tefx_reado(efx, &reg, FR_AZ_CS_DEBUG);\n\tefx->port_num = EFX_OWORD_FIELD(reg, FRF_CZ_CS_PORT_NUM) - 1;\n\n\trc = efx_siena_mcdi_init(efx);\n\tif (rc)\n\t\tgoto fail1;\n\n\t/* Now we can reset the NIC */\n\trc = efx_siena_mcdi_reset(efx, RESET_TYPE_ALL);\n\tif (rc) {\n\t\tnetif_err(efx, probe, efx->net_dev, \"failed to reset NIC\\n\");\n\t\tgoto fail3;\n\t}\n\n\tsiena_init_wol(efx);\n\n\t/* Allocate memory for INT_KER */\n\trc = efx_siena_alloc_buffer(efx, &efx->irq_status, sizeof(efx_oword_t),\n\t\t\t\t    GFP_KERNEL);\n\tif (rc)\n\t\tgoto fail4;\n\tBUG_ON(efx->irq_status.dma_addr & 0x0f);\n\n\tnetif_dbg(efx, probe, efx->net_dev,\n\t\t  \"INT_KER at %llx (virt %p phys %llx)\\n\",\n\t\t  (unsigned long long)efx->irq_status.dma_addr,\n\t\t  efx->irq_status.addr,\n\t\t  (unsigned long long)virt_to_phys(efx->irq_status.addr));\n\n\t/* Read in the non-volatile configuration */\n\trc = siena_probe_nvconfig(efx);\n\tif (rc == -EINVAL) {\n\t\tnetif_err(efx, probe, efx->net_dev,",
      "\n#include <linux/syscalls.h>\n#include <asm/syscalls.h>\n\n#undef __SYSCALL\n#define __SYSCALL(nr, call)[nr] = (call),\n\n#define sys_fadvise64_64 sys_csky_fadvise64_64\nvoid * const sys_call_table[__NR_syscalls] __page_aligned_data = {\n\t[0 ... __NR_syscalls - 1] = sys_ni_syscall,\n#include <asm/unistd.h>\n};\n",
      "\t\t\tenum dma_data_direction iodir,\n\t\t\tunion lpfc_vmid_io_tag *tag)\n{\n\tstruct lpfc_vmid *vmp = NULL;\n\tint hash, len, rc = -EPERM, i;\n\n\t/* check if QFPA is complete */\n\tif (lpfc_vmid_is_type_priority_tag(vport) &&\n\t    !(vport->vmid_flag & LPFC_VMID_QFPA_CMPL) &&\n\t    (vport->vmid_flag & LPFC_VMID_ISSUE_QFPA)) {\n\t\tvport->work_port_events |= WORKER_CHECK_VMID_ISSUE_QFPA;\n\t\treturn -EAGAIN;\n\t}\n\n\t/* search if the UUID has already been mapped to the VMID */\n\tlen = strlen(uuid);\n\thash = lpfc_vmid_hash_fn(uuid, len);\n\n\t/* search for the VMID in the table */\n\tread_lock(&vport->vmid_lock);\n\tvmp = lpfc_get_vmid_from_hashtable(vport, hash, uuid);\n\n\t/* if found, check if its already registered  */\n\tif (vmp  && vmp->flag & LPFC_VMID_REGISTERED) {\n\t\tread_unlock(&vport->vmid_lock);\n\t\tlpfc_vmid_update_entry(vport, iodir, vmp, tag);\n\t\trc = 0;\n\t} else if (vmp && (vmp->flag & LPFC_VMID_REQ_REGISTER ||\n\t\t\t   vmp->flag & LPFC_VMID_DE_REGISTER)) {\n\t\t/* else if register or dereg request has already been sent */\n\t\t/* Hence VMID tag will not be added for this I/O */\n\t\tread_unlock(&vport->vmid_lock);\n\t\trc = -EBUSY;\n\t} else {\n\t\t/* The VMID was not found in the hashtable. At this point, */\n\t\t/* drop the read lock first before proceeding further */\n\t\tread_unlock(&vport->vmid_lock);\n\t\t/* start the process to obtain one as per the */\n\t\t/* type of the VMID indicated */\n\t\twrite_lock(&vport->vmid_lock);\n\t\tvmp = lpfc_get_vmid_from_hashtable(vport, hash, uuid);\n\n\t\t/* while the read lock was released, in case the entry was */\n\t\t/* added by other context or is in process of being added */\n\t\tif (vmp && vmp->flag & LPFC_VMID_REGISTERED) {\n\t\t\tlpfc_vmid_update_entry(vport, iodir, vmp, tag);\n\t\t\twrite_unlock(&vport->vmid_lock);\n\t\t\treturn 0;\n\t\t} else if (vmp && vmp->flag & LPFC_VMID_REQ_REGISTER) {\n\t\t\twrite_unlock(&vport->vmid_lock);\n\t\t\treturn -EBUSY;\n\t\t}\n\n\t\t/* else search and allocate a free slot in the hash table */\n\t\tif (vport->cur_vmid_cnt < vport->max_vmid) {\n\t\t\tfor (i = 0; i < vport->max_vmid; i++) {\n\t\t\t\tvmp = vport->vmid + i;\n\t\t\t\tif (vmp->flag == LPFC_VMID_SLOT_FREE)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (i == vport->max_vmid)\n\t\t\t\tvmp = NULL;\n\t\t} else {\n\t\t\tvmp = NULL;",
      "MODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Nikolay Aleksandrov <razor@blackwall.org>\");\n",
      "\t\t  __func__, voltdm->name);\n\n\treturn ERR_PTR(-ENODATA);\n}\n\n/**\n * omap_voltage_register_pmic() - API to register PMIC specific data\n * @voltdm:\tpointer to the VDD for which the PMIC specific data is\n *\t\tto be registered\n * @pmic:\tthe structure containing pmic info\n *\n * This API is to be called by the SOC/PMIC file to specify the\n * pmic specific info as present in omap_voltdm_pmic structure.\n */\nint omap_voltage_register_pmic(struct voltagedomain *voltdm,\n\t\t\t       struct omap_voltdm_pmic *pmic)\n{\n\tif (!voltdm || IS_ERR(voltdm)) {\n\t\tpr_warn(\"%s: VDD specified does not exist!\\n\", __func__);\n\t\treturn -EINVAL;\n\t}\n\n\tvoltdm->pmic = pmic;\n\n\treturn 0;\n}\n\n/**\n * omap_voltage_late_init() - Init the various voltage parameters\n *\n * This API is to be called in the later stages of the\n * system boot to init the voltage controller and\n * voltage processors.\n */\nint __init omap_voltage_late_init(void)\n{\n\tstruct voltagedomain *voltdm;\n\n\tif (list_empty(&voltdm_list)) {\n\t\tpr_err(\"%s: Voltage driver support not added\\n\",\n\t\t\t__func__);\n\t\treturn -EINVAL;\n\t}\n\n\tlist_for_each_entry(voltdm, &voltdm_list, node) {\n\t\tstruct clk *sys_ck;\n\n\t\tif (!voltdm->scalable)\n\t\t\tcontinue;\n\n\t\tsys_ck = clk_get(NULL, voltdm->sys_clk.name);\n\t\tif (IS_ERR(sys_ck)) {\n\t\t\tpr_warn(\"%s: Could not get sys clk.\\n\", __func__);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tvoltdm->sys_clk.rate = clk_get_rate(sys_ck);\n\t\tWARN_ON(!voltdm->sys_clk.rate);\n\t\tclk_put(sys_ck);\n\n\t\tif (voltdm->vc) {\n\t\t\tvoltdm->scale = omap_vc_bypass_scale;\n\t\t\tomap_vc_init_channel(voltdm);\n\t\t}\n",
      "\t\t\t\t\t.bytesperline = VGA_WIDTH * 2,\n\t\t\t\t},\n\t\t\t\t[1]\t= {\n\t\t\t\t\t.sizeimage = VGA_WIDTH * VGA_HEIGHT * 2,\n\t\t\t\t\t.bytesperline = VGA_WIDTH * 2,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t};\n\n\tret = ceu_try_fmt(ceudev, &v4l2_fmt);\n\tif (ret)\n\t\treturn ret;\n\n\tceudev->v4l2_pix = v4l2_fmt.fmt.pix_mp;\n\tceudev->field = V4L2_FIELD_NONE;\n\n\treturn 0;\n}\n\n/*\n * ceu_init_mbus_fmt() - Query sensor for supported formats and initialize\n *\t\t\t CEU media bus format used to produce memory formats.\n *\n * Find out if sensor can produce a permutation of 8-bits YUYV bus format.\n * From a single 8-bits YUYV bus format the CEU can produce several memory\n * output formats:\n * - NV[12|21|16|61] through image fetch mode;\n * - YUYV422 if sensor provides YUYV422\n *\n * TODO: Other YUYV422 permutations through data fetch sync mode and DTARY\n * TODO: Binary data (eg. JPEG) and raw formats through data fetch sync mode\n */\nstatic int ceu_init_mbus_fmt(struct ceu_device *ceudev)\n{\n\tstruct ceu_subdev *ceu_sd = ceudev->sd;\n\tstruct ceu_mbus_fmt *mbus_fmt = &ceu_sd->mbus_fmt;\n\tstruct v4l2_subdev *v4l2_sd = ceu_sd->v4l2_sd;\n\tbool yuyv_bus_fmt = false;\n\n\tstruct v4l2_subdev_mbus_code_enum sd_mbus_fmt = {\n\t\t.which = V4L2_SUBDEV_FORMAT_ACTIVE,\n\t\t.index = 0,\n\t};\n\n\t/* Find out if sensor can produce any permutation of 8-bits YUYV422. */\n\twhile (!yuyv_bus_fmt &&\n\t       !v4l2_subdev_call(v4l2_sd, pad, enum_mbus_code,\n\t\t\t\t NULL, &sd_mbus_fmt)) {\n\t\tswitch (sd_mbus_fmt.code) {\n\t\tcase MEDIA_BUS_FMT_YUYV8_2X8:\n\t\tcase MEDIA_BUS_FMT_YVYU8_2X8:\n\t\tcase MEDIA_BUS_FMT_UYVY8_2X8:\n\t\tcase MEDIA_BUS_FMT_VYUY8_2X8:\n\t\t\tyuyv_bus_fmt = true;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t/*\n\t\t\t * Only support 8-bits YUYV bus formats at the moment;\n\t\t\t *\n\t\t\t * TODO: add support for binary formats (data sync\n\t\t\t * fetch mode).\n\t\t\t */\n\t\t\tbreak;",
      "int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)\n{\n\tbool vwc = false;\n\n\tmutex_init(&head->lock);\n\tbio_list_init(&head->requeue_list);\n\tspin_lock_init(&head->requeue_lock);\n\tINIT_WORK(&head->requeue_work, nvme_requeue_work);\n\n\t/*\n\t * Add a multipath node if the subsystems supports multiple controllers.\n\t * We also do this for private namespaces as the namespace sharing flag\n\t * could change after a rescan.\n\t */\n\tif (!(ctrl->subsys->cmic & NVME_CTRL_CMIC_MULTI_CTRL) ||\n\t    !nvme_is_unique_nsid(ctrl, head) || !multipath)\n\t\treturn 0;\n\n\thead->disk = blk_alloc_disk(ctrl->numa_node);\n\tif (!head->disk)\n\t\treturn -ENOMEM;\n\thead->disk->fops = &nvme_ns_head_ops;\n\thead->disk->private_data = head;\n\tsprintf(head->disk->disk_name, \"nvme%dn%d\",\n\t\t\tctrl->subsys->instance, head->instance);\n\n\tblk_queue_flag_set(QUEUE_FLAG_NONROT, head->disk->queue);\n\tblk_queue_flag_set(QUEUE_FLAG_NOWAIT, head->disk->queue);\n\tblk_queue_flag_set(QUEUE_FLAG_IO_STAT, head->disk->queue);\n\t/*\n\t * This assumes all controllers that refer to a namespace either\n\t * support poll queues or not.  That is not a strict guarantee,\n\t * but if the assumption is wrong the effect is only suboptimal\n\t * performance but not correctness problem.\n\t */\n\tif (ctrl->tagset->nr_maps > HCTX_TYPE_POLL &&\n\t    ctrl->tagset->map[HCTX_TYPE_POLL].nr_queues)\n\t\tblk_queue_flag_set(QUEUE_FLAG_POLL, head->disk->queue);\n\n\t/* set to a default value of 512 until the disk is validated */\n\tblk_queue_logical_block_size(head->disk->queue, 512);\n\tblk_set_stacking_limits(&head->disk->queue->limits);\n\tblk_queue_dma_alignment(head->disk->queue, 3);\n\n\t/* we need to propagate up the VMC settings */\n\tif (ctrl->vwc & NVME_CTRL_VWC_PRESENT)\n\t\tvwc = true;\n\tblk_queue_write_cache(head->disk->queue, vwc, vwc);\n\treturn 0;\n}\n\nstatic void nvme_mpath_set_live(struct nvme_ns *ns)\n{\n\tstruct nvme_ns_head *head = ns->head;\n\tint rc;\n\n\tif (!head->disk)\n\t\treturn;\n\n\t/*\n\t * test_and_set_bit() is used because it is protecting against two nvme\n\t * paths simultaneously calling device_add_disk() on the same namespace\n\t * head.\n\t */",
      "#define EEPROM_PRWRITE\t0xa000\t/* write protect register */\n#define EEPROM_PRDS\t0x8000\t/* disable protect register, forever */\n\n#define EEPROM_EPROT\t0x01\t/* Protect register enable */\n#define EEPROM_CSEL\t0x02\t/* Chip select */\n#define EEPROM_ECLK\t0x04\t/* EEPROM clock */\n#define EEPROM_DATO\t0x08\t/* Data out */\n#define EEPROM_DATI\t0x10\t/* Data in */\n\n/* We need to use these functions early... */\n#define delay() ({\t\t\t\t\t\t\\\n\tint x;\t\t\t\t\t\t\t\\\n\tfor (x=0; x<100000; x++) __asm__ __volatile__(\"\"); })\n\n#define eeprom_cs_on(ptr) ({\t\\\n\t__raw_writel(__raw_readl(ptr) & ~EEPROM_DATO, ptr);\t\\\n\t__raw_writel(__raw_readl(ptr) & ~EEPROM_ECLK, ptr);\t\\\n\t__raw_writel(__raw_readl(ptr) & ~EEPROM_EPROT, ptr);\t\\\n\tdelay();\t\t\t\t\t\t\\\n\t__raw_writel(__raw_readl(ptr) | EEPROM_CSEL, ptr);\t\\\n\t__raw_writel(__raw_readl(ptr) | EEPROM_ECLK, ptr); })\n\n\n#define eeprom_cs_off(ptr) ({\t\\\n\t__raw_writel(__raw_readl(ptr) & ~EEPROM_ECLK, ptr);\t\\\n\t__raw_writel(__raw_readl(ptr) & ~EEPROM_CSEL, ptr);\t\\\n\t__raw_writel(__raw_readl(ptr) | EEPROM_EPROT, ptr);\t\\\n\t__raw_writel(__raw_readl(ptr) | EEPROM_ECLK, ptr); })\n\n#define BITS_IN_COMMAND 11\n/*\n * clock in the nvram command and the register number. For the\n * national semiconductor nv ram chip the op code is 3 bits and\n * the address is 6/8 bits.\n */\nstatic inline void eeprom_cmd(unsigned int *ctrl, unsigned cmd, unsigned reg)\n{\n\tunsigned short ser_cmd;\n\tint i;\n\n\tser_cmd = cmd | (reg << (16 - BITS_IN_COMMAND));\n\tfor (i = 0; i < BITS_IN_COMMAND; i++) {\n\t\tif (ser_cmd & (1<<15))\t/* if high order bit set */\n\t\t\t__raw_writel(__raw_readl(ctrl) | EEPROM_DATO, ctrl);\n\t\telse\n\t\t\t__raw_writel(__raw_readl(ctrl) & ~EEPROM_DATO, ctrl);\n\t\t__raw_writel(__raw_readl(ctrl) & ~EEPROM_ECLK, ctrl);\n\t\tdelay();\n\t\t__raw_writel(__raw_readl(ctrl) | EEPROM_ECLK, ctrl);\n\t\tdelay();\n\t\tser_cmd <<= 1;\n\t}\n\t/* see data sheet timing diagram */\n\t__raw_writel(__raw_readl(ctrl) & ~EEPROM_DATO, ctrl);\n}\n\nunsigned short ip22_eeprom_read(unsigned int *ctrl, int reg)\n{\n\tunsigned short res = 0;\n\tint i;\n\n\t__raw_writel(__raw_readl(ctrl) & ~EEPROM_EPROT, ctrl);\n\teeprom_cs_on(ctrl);\n\teeprom_cmd(ctrl, EEPROM_READ, reg);"
    ]
  },
  {
    "id": "microsoft/vscode",
    "org": "microsoft",
    "avatarURL": "https://avatars.githubusercontent.com/u/6154722?v=4",
    "name": "microsoft/vscode",
    "url": "https://github.com/microsoft/vscode",
    "lang": "TypeScript",
    "desc": "Visual Studio Code is a code editor redefined and optimized for building and debugging modern web and cloud applications.",
    "star_num": 150874,
    "fork_num": 26816,
    "snippets": [
      "\t\t\tdimension: {\n\t\t\t\twidth: 0,\n\t\t\t\theight: 0\n\t\t\t},\n\t\t}, {\n\t\t\tcontributions: this.notebookEditor.creationOptions.cellEditorContributions\n\t\t});\n\n\t\ttemplateDisposables.add(editor);\n\n\t\tconst outputContainer = new FastDomNode(DOM.append(container, $('.output')));\n\t\tconst cellOutputCollapsedContainer = DOM.append(outputContainer.domNode, $('.output-collapse-container'));\n\t\tconst outputShowMoreContainer = new FastDomNode(DOM.append(container, $('.output-show-more-container')));\n\t\tconst focusIndicatorRight = new FastDomNode(DOM.append(container, DOM.$('.cell-focus-indicator.cell-focus-indicator-side.cell-focus-indicator-right')));\n\t\tconst focusSinkElement = DOM.append(container, $('.cell-editor-focus-sink'));\n\t\tfocusSinkElement.setAttribute('tabindex', '0');\n\t\tconst bottomCellToolbarContainer = DOM.append(container, $('.cell-bottom-toolbar-container'));\n\t\tconst focusIndicatorBottom = new FastDomNode(DOM.append(container, $('.cell-focus-indicator.cell-focus-indicator-bottom')));\n\n\t\tconst scopedInstaService = this.instantiationService.createChild(new ServiceCollection([IContextKeyService, contextKeyService]));\n\t\tconst rootClassDelegate = {\n\t\t\ttoggle: (className: string, force?: boolean) => container.classList.toggle(className, force)\n\t\t};\n\t\tconst titleToolbar = templateDisposables.add(scopedInstaService.createInstance(\n\t\t\tCellTitleToolbarPart,\n\t\t\ttitleToolbarContainer,\n\t\t\trootClassDelegate,\n\t\t\tthis.notebookEditor.creationOptions.menuIds.cellTitleToolbar,\n\t\t\tthis.notebookEditor.creationOptions.menuIds.cellDeleteToolbar,\n\t\t\tthis.notebookEditor));\n\n\t\tconst focusIndicatorPart = templateDisposables.add(new CellFocusIndicator(this.notebookEditor, titleToolbar, focusIndicatorTop, focusIndicatorLeft, focusIndicatorRight, focusIndicatorBottom));\n\t\tconst cellParts = new CellPartsCollection([\n\t\t\tfocusIndicatorPart,\n\t\t\ttemplateDisposables.add(scopedInstaService.createInstance(CellEditorStatusBar, this.notebookEditor, container, editorPart, editor)),\n\t\t\ttemplateDisposables.add(scopedInstaService.createInstance(CellProgressBar, editorPart, cellInputCollapsedContainer)),\n\t\t\ttemplateDisposables.add(scopedInstaService.createInstance(RunToolbar, this.notebookEditor, contextKeyService, container, runButtonContainer)),\n\t\t\ttemplateDisposables.add(new CellDecorations(rootContainer, decorationContainer)),\n\t\t\ttemplateDisposables.add(scopedInstaService.createInstance(CellComments, this.notebookEditor, cellCommentPartContainer)),\n\t\t\ttemplateDisposables.add(scopedInstaService.createInstance(CellExecutionPart, this.notebookEditor, executionOrderLabel)),\n\t\t\ttemplateDisposables.add(scopedInstaService.createInstance(CollapsedCellOutput, this.notebookEditor, cellOutputCollapsedContainer)),\n\t\t\ttemplateDisposables.add(new CollapsedCellInput(this.notebookEditor, cellInputCollapsedContainer)),\n\t\t\ttemplateDisposables.add(new CellFocusPart(container, focusSinkElement, this.notebookEditor)),\n\t\t\ttemplateDisposables.add(new CellDragAndDropPart(container)),\n\t\t\ttemplateDisposables.add(scopedInstaService.createInstance(CellContextKeyPart, this.notebookEditor)),\n\t\t], [\n\t\t\ttitleToolbar,\n\t\t\ttemplateDisposables.add(scopedInstaService.createInstance(BetweenCellToolbar, this.notebookEditor, titleToolbarContainer, bottomCellToolbarContainer))\n\t\t]);\n\n\t\ttemplateDisposables.add(cellParts);\n\n\t\tconst templateData: CodeCellRenderTemplate = {\n\t\t\trootContainer,\n\t\t\teditorPart,\n\t\t\tcellInputCollapsedContainer,\n\t\t\tcellOutputCollapsedContainer,\n\t\t\tinstantiationService: scopedInstaService,\n\t\t\tcontainer,\n\t\t\tcellContainer,\n\t\t\tfocusSinkElement,\n\t\t\toutputContainer,\n\t\t\toutputShowMoreContainer,\n\t\t\teditor,",
      "import { isExportableSessionData } from 'vs/workbench/contrib/chat/common/chatModel';\nimport { IChatService } from 'vs/workbench/contrib/chat/common/chatService';\nimport { IEditorService } from 'vs/workbench/services/editor/common/editorService';\n\nconst defaultFileName = 'chat.json';\nconst filters = [{ name: localize('chat.file.label', \"Chat Session\"), extensions: ['json'] }];\n\nexport function registerChatExportActions() {\n\tregisterAction2(class ExportChatAction extends Action2 {\n\t\tconstructor() {\n\t\t\tsuper({\n\t\t\t\tid: 'workbench.action.chat.export',\n\t\t\t\tcategory: CHAT_CATEGORY,\n\t\t\t\ttitle: {\n\t\t\t\t\tvalue: localize('chat.export.label', \"Export Session\") + '...',\n\t\t\t\t\toriginal: 'Export Session...'\n\t\t\t\t},\n\t\t\t\tprecondition: CONTEXT_PROVIDER_EXISTS,\n\t\t\t\tf1: true,\n\t\t\t});\n\t\t}\n\t\tasync run(accessor: ServicesAccessor, ...args: any[]) {\n\t\t\tconst widgetService = accessor.get(IChatWidgetService);\n\t\t\tconst fileDialogService = accessor.get(IFileDialogService);\n\t\t\tconst fileService = accessor.get(IFileService);\n\t\t\tconst chatService = accessor.get(IChatService);\n\n\t\t\tconst widget = widgetService.lastFocusedWidget;\n\t\t\tif (!widget || !widget.viewModel) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst defaultUri = joinPath(await fileDialogService.defaultFilePath(), defaultFileName);\n\t\t\tconst result = await fileDialogService.showSaveDialog({\n\t\t\t\tdefaultUri,\n\t\t\t\tfilters\n\t\t\t});\n\t\t\tif (!result) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst model = chatService.getSession(widget.viewModel.sessionId);\n\t\t\tif (!model) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t// Using toJSON on the model\n\t\t\tconst content = VSBuffer.fromString(JSON.stringify(model.toExport(), undefined, 2));\n\t\t\tawait fileService.writeFile(result, content);\n\t\t}\n\t});\n\n\tregisterAction2(class ImportChatAction extends Action2 {\n\t\tconstructor() {\n\t\t\tsuper({\n\t\t\t\tid: 'workbench.action.chat.import',\n\t\t\t\ttitle: {\n\t\t\t\t\tvalue: localize('chat.import.label', \"Import Session\") + '...',\n\t\t\t\t\toriginal: 'Import Session...'\n\t\t\t\t},\n\t\t\t\tcategory: CHAT_CATEGORY,\n\t\t\t\tprecondition: CONTEXT_PROVIDER_EXISTS,\n\t\t\t\tf1: true,\n\t\t\t});",
      "\n\tprivate _handlePool = 0;\n\tprivate _disposables = new DisposableStore();\n\tprivate _insets = new Map<number, { editor: vscode.TextEditor; inset: vscode.WebviewEditorInset; onDidReceiveMessage: Emitter<any> }>();\n\n\tconstructor(\n\t\tprivate readonly _proxy: MainThreadEditorInsetsShape,\n\t\tprivate readonly _editors: ExtHostEditors,\n\t\tprivate readonly _remoteInfo: WebviewRemoteInfo\n\t) {\n\n\t\t// dispose editor inset whenever the hosting editor goes away\n\t\tthis._disposables.add(_editors.onDidChangeVisibleTextEditors(() => {\n\t\t\tconst visibleEditor = _editors.getVisibleTextEditors();\n\t\t\tfor (const value of this._insets.values()) {\n\t\t\t\tif (visibleEditor.indexOf(value.editor) < 0) {\n\t\t\t\t\tvalue.inset.dispose(); // will remove from `this._insets`\n\t\t\t\t}\n\t\t\t}\n\t\t}));\n\t}\n\n\tdispose(): void {\n\t\tthis._insets.forEach(value => value.inset.dispose());\n\t\tthis._disposables.dispose();\n\t}\n\n\tcreateWebviewEditorInset(editor: vscode.TextEditor, line: number, height: number, options: vscode.WebviewOptions | undefined, extension: IExtensionDescription): vscode.WebviewEditorInset {\n\n\t\tlet apiEditor: ExtHostTextEditor | undefined;\n\t\tfor (const candidate of this._editors.getVisibleTextEditors(true)) {\n\t\t\tif (candidate.value === editor) {\n\t\t\t\tapiEditor = <ExtHostTextEditor>candidate;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!apiEditor) {\n\t\t\tthrow new Error('not a visible editor');\n\t\t}\n\n\t\tconst that = this;\n\t\tconst handle = this._handlePool++;\n\t\tconst onDidReceiveMessage = new Emitter<any>();\n\t\tconst onDidDispose = new Emitter<void>();\n\n\t\tconst webview = new class implements vscode.Webview {\n\n\t\t\tprivate _html: string = '';\n\t\t\tprivate _options: vscode.WebviewOptions = Object.create(null);\n\n\t\t\tasWebviewUri(resource: vscode.Uri): vscode.Uri {\n\t\t\t\treturn asWebviewUri(resource, that._remoteInfo);\n\t\t\t}\n\n\t\t\tget cspSource(): string {\n\t\t\t\treturn webviewGenericCspSource;\n\t\t\t}\n\n\t\t\tset options(value: vscode.WebviewOptions) {\n\t\t\t\tthis._options = value;\n\t\t\t\tthat._proxy.$setOptions(handle, value);\n\t\t\t}\n\n\t\t\tget options(): vscode.WebviewOptions {",
      "import { ICodeEditorService } from 'vs/editor/browser/services/codeEditorService';\nimport { QuickOutlineNLS } from 'vs/editor/common/standaloneStrings';\nimport { Event } from 'vs/base/common/event';\nimport { EditorAction, registerEditorAction } from 'vs/editor/browser/editorExtensions';\nimport { EditorContextKeys } from 'vs/editor/common/editorContextKeys';\nimport { KeyMod, KeyCode } from 'vs/base/common/keyCodes';\nimport { KeybindingWeight } from 'vs/platform/keybinding/common/keybindingsRegistry';\nimport { ServicesAccessor } from 'vs/platform/instantiation/common/instantiation';\nimport { IQuickInputService, ItemActivation } from 'vs/platform/quickinput/common/quickInput';\nimport { IOutlineModelService } from 'vs/editor/contrib/documentSymbols/browser/outlineModel';\nimport { ILanguageFeaturesService } from 'vs/editor/common/services/languageFeatures';\n\nexport class StandaloneGotoSymbolQuickAccessProvider extends AbstractGotoSymbolQuickAccessProvider {\n\n\tprotected readonly onDidActiveTextEditorControlChange = Event.None;\n\n\tconstructor(\n\t\t@ICodeEditorService private readonly editorService: ICodeEditorService,\n\t\t@ILanguageFeaturesService languageFeaturesService: ILanguageFeaturesService,\n\t\t@IOutlineModelService outlineModelService: IOutlineModelService,\n\t) {\n\t\tsuper(languageFeaturesService, outlineModelService);\n\t}\n\n\tprotected get activeTextEditorControl() {\n\t\treturn this.editorService.getFocusedCodeEditor() ?? undefined;\n\t}\n}\n\nexport class GotoSymbolAction extends EditorAction {\n\n\tstatic readonly ID = 'editor.action.quickOutline';\n\n\tconstructor() {\n\t\tsuper({\n\t\t\tid: GotoSymbolAction.ID,\n\t\t\tlabel: QuickOutlineNLS.quickOutlineActionLabel,\n\t\t\talias: 'Go to Symbol...',\n\t\t\tprecondition: EditorContextKeys.hasDocumentSymbolProvider,\n\t\t\tkbOpts: {\n\t\t\t\tkbExpr: EditorContextKeys.focus,\n\t\t\t\tprimary: KeyMod.CtrlCmd | KeyMod.Shift | KeyCode.KeyO,\n\t\t\t\tweight: KeybindingWeight.EditorContrib\n\t\t\t},\n\t\t\tcontextMenuOpts: {\n\t\t\t\tgroup: 'navigation',\n\t\t\t\torder: 3\n\t\t\t}\n\t\t});\n\t}\n\n\trun(accessor: ServicesAccessor): void {\n\t\taccessor.get(IQuickInputService).quickAccess.show(AbstractGotoSymbolQuickAccessProvider.PREFIX, { itemActivation: ItemActivation.NONE });\n\t}\n}\n\nregisterEditorAction(GotoSymbolAction);\n\nRegistry.as<IQuickAccessRegistry>(Extensions.Quickaccess).registerQuickAccessProvider({\n\tctor: StandaloneGotoSymbolQuickAccessProvider,\n\tprefix: AbstractGotoSymbolQuickAccessProvider.PREFIX,\n\thelpEntries: [\n\t\t{ description: QuickOutlineNLS.quickOutlineActionLabel, prefix: AbstractGotoSymbolQuickAccessProvider.PREFIX, commandId: GotoSymbolAction.ID },\n\t\t{ description: QuickOutlineNLS.quickOutlineByCategoryActionLabel, prefix: AbstractGotoSymbolQuickAccessProvider.PREFIX_BY_CATEGORY }",
      "\t\t\t<IParsedQuery>{\n\t\t\t\ttags: ['modified'],\n\t\t\t\textensionFilters: [],\n\t\t\t\tquery: 'test',\n\t\t\t\tfeatureFilters: [],\n\t\t\t\tidFilters: [],\n\t\t\t\tlanguageFilter: undefined\n\t\t\t});\n\n\t\ttestParseQuery(\n\t\t\t'query has @ for some reason',\n\t\t\t<IParsedQuery>{\n\t\t\t\ttags: [],\n\t\t\t\textensionFilters: [],\n\t\t\t\tquery: 'query has @ for some reason',\n\t\t\t\tfeatureFilters: [],\n\t\t\t\tidFilters: [],\n\t\t\t\tlanguageFilter: undefined\n\t\t\t});\n\n\t\ttestParseQuery(\n\t\t\t'@ext:github.vscode-pull-request-github',\n\t\t\t<IParsedQuery>{\n\t\t\t\ttags: [],\n\t\t\t\textensionFilters: ['github.vscode-pull-request-github'],\n\t\t\t\tquery: '',\n\t\t\t\tfeatureFilters: [],\n\t\t\t\tidFilters: [],\n\t\t\t\tlanguageFilter: undefined\n\t\t\t});\n\n\t\ttestParseQuery(\n\t\t\t'@ext:github.vscode-pull-request-github,vscode.git',\n\t\t\t<IParsedQuery>{\n\t\t\t\ttags: [],\n\t\t\t\textensionFilters: ['github.vscode-pull-request-github', 'vscode.git'],\n\t\t\t\tquery: '',\n\t\t\t\tfeatureFilters: [],\n\t\t\t\tidFilters: [],\n\t\t\t\tlanguageFilter: undefined\n\t\t\t});\n\t\ttestParseQuery(\n\t\t\t'@feature:scm',\n\t\t\t<IParsedQuery>{\n\t\t\t\ttags: [],\n\t\t\t\textensionFilters: [],\n\t\t\t\tfeatureFilters: ['scm'],\n\t\t\t\tquery: '',\n\t\t\t\tidFilters: [],\n\t\t\t\tlanguageFilter: undefined\n\t\t\t});\n\n\t\ttestParseQuery(\n\t\t\t'@feature:scm,terminal',\n\t\t\t<IParsedQuery>{\n\t\t\t\ttags: [],\n\t\t\t\textensionFilters: [],\n\t\t\t\tfeatureFilters: ['scm', 'terminal'],\n\t\t\t\tquery: '',\n\t\t\t\tidFilters: [],\n\t\t\t\tlanguageFilter: undefined\n\t\t\t});\n\t\ttestParseQuery(\n\t\t\t'@id:files.autoSave',",
      "\t\tlet dirname;\n\t\tif (resource.scheme === Schemas.file) {\n\t\t\tdirname = URI.file(paths.dirname(originalFSPath(resource))).path;\n\t\t} else {\n\t\t\tdirname = paths.posix.dirname(resource.path);\n\t\t\tif (resource.authority && dirname.length && dirname.charCodeAt(0) !== CharCode.Slash) {\n\t\t\t\tconsole.error(`dirname(\"${resource.toString})) resulted in a relative path`);\n\t\t\t\tdirname = '/'; // If a URI contains an authority component, then the path component must either be empty or begin with a CharCode.Slash (\"/\") character\n\t\t\t}\n\t\t}\n\t\treturn resource.with({\n\t\t\tpath: dirname\n\t\t});\n\t}\n\n\tnormalizePath(resource: URI): URI {\n\t\tif (!resource.path.length) {\n\t\t\treturn resource;\n\t\t}\n\t\tlet normalizedPath: string;\n\t\tif (resource.scheme === Schemas.file) {\n\t\t\tnormalizedPath = URI.file(paths.normalize(originalFSPath(resource))).path;\n\t\t} else {\n\t\t\tnormalizedPath = paths.posix.normalize(resource.path);\n\t\t}\n\t\treturn resource.with({\n\t\t\tpath: normalizedPath\n\t\t});\n\t}\n\n\trelativePath(from: URI, to: URI): string | undefined {\n\t\tif (from.scheme !== to.scheme || !isEqualAuthority(from.authority, to.authority)) {\n\t\t\treturn undefined;\n\t\t}\n\t\tif (from.scheme === Schemas.file) {\n\t\t\tconst relativePath = paths.relative(originalFSPath(from), originalFSPath(to));\n\t\t\treturn isWindows ? extpath.toSlashes(relativePath) : relativePath;\n\t\t}\n\t\tlet fromPath = from.path || '/';\n\t\tconst toPath = to.path || '/';\n\t\tif (this._ignorePathCasing(from)) {\n\t\t\t// make casing of fromPath match toPath\n\t\t\tlet i = 0;\n\t\t\tfor (const len = Math.min(fromPath.length, toPath.length); i < len; i++) {\n\t\t\t\tif (fromPath.charCodeAt(i) !== toPath.charCodeAt(i)) {\n\t\t\t\t\tif (fromPath.charAt(i).toLowerCase() !== toPath.charAt(i).toLowerCase()) {\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tfromPath = toPath.substr(0, i) + fromPath.substr(i);\n\t\t}\n\t\treturn paths.posix.relative(fromPath, toPath);\n\t}\n\n\tresolvePath(base: URI, path: string): URI {\n\t\tif (base.scheme === Schemas.file) {\n\t\t\tconst newURI = URI.file(paths.resolve(originalFSPath(base), path));\n\t\t\treturn base.with({\n\t\t\t\tauthority: newURI.authority,\n\t\t\t\tpath: newURI.path\n\t\t\t});\n\t\t}\n\t\tpath = extpath.toPosixPath(path); // we allow path to be a windows path",
      "/*---------------------------------------------------------------------------------------------\n *  Copyright (c) Microsoft Corporation. All rights reserved.\n *  Licensed under the MIT License. See License.txt in the project root for license information.\n *--------------------------------------------------------------------------------------------*/\n\nimport { registerSharedProcessRemoteService } from 'vs/platform/ipc/electron-sandbox/services';\nimport { ISharedProcessTunnelService, ipcSharedProcessTunnelChannelName } from 'vs/platform/remote/common/sharedProcessTunnelService';\n\nregisterSharedProcessRemoteService(ISharedProcessTunnelService, ipcSharedProcessTunnelChannelName);\n",
      "\t\t}\n\t\treturn result;\n\t}\n\n\tstatic Max(...positions: Position[]): Position {\n\t\tif (positions.length === 0) {\n\t\t\tthrow new TypeError();\n\t\t}\n\t\tlet result = positions[0];\n\t\tfor (let i = 1; i < positions.length; i++) {\n\t\t\tconst p = positions[i];\n\t\t\tif (p.isAfter(result!)) {\n\t\t\t\tresult = p;\n\t\t\t}\n\t\t}\n\t\treturn result;\n\t}\n\n\tstatic isPosition(other: any): other is Position {\n\t\tif (!other) {\n\t\t\treturn false;\n\t\t}\n\t\tif (other instanceof Position) {\n\t\t\treturn true;\n\t\t}\n\t\tconst { line, character } = <Position>other;\n\t\tif (typeof line === 'number' && typeof character === 'number') {\n\t\t\treturn true;\n\t\t}\n\t\treturn false;\n\t}\n\n\tstatic of(obj: vscode.Position): Position {\n\t\tif (obj instanceof Position) {\n\t\t\treturn obj;\n\t\t} else if (this.isPosition(obj)) {\n\t\t\treturn new Position(obj.line, obj.character);\n\t\t}\n\t\tthrow new Error('Invalid argument, is NOT a position-like object');\n\t}\n\n\tprivate _line: number;\n\tprivate _character: number;\n\n\tget line(): number {\n\t\treturn this._line;\n\t}\n\n\tget character(): number {\n\t\treturn this._character;\n\t}\n\n\tconstructor(line: number, character: number) {\n\t\tif (line < 0) {\n\t\t\tthrow illegalArgument('line must be non-negative');\n\t\t}\n\t\tif (character < 0) {\n\t\t\tthrow illegalArgument('character must be non-negative');\n\t\t}\n\t\tthis._line = line;\n\t\tthis._character = character;\n\t}\n\n\tisBefore(other: Position): boolean {",
      "\t\tif (!textSelection) {\n\t\t\treturn undefined;\n\t\t}\n\n\t\treturn new TextEditorPaneSelection(new Selection(textSelection.startLineNumber, textSelection.startColumn, textSelection.endLineNumber ?? textSelection.startLineNumber, textSelection.endColumn ?? textSelection.startColumn));\n\t}\n}\n\nexport interface ITestInstantiationService extends IInstantiationService {\n\tstub<T>(service: ServiceIdentifier<T>, ctor: any): T;\n}\n\nexport class TestWorkingCopyService extends WorkingCopyService {\n\ttestUnregisterWorkingCopy(workingCopy: IWorkingCopy): void {\n\t\treturn super.unregisterWorkingCopy(workingCopy);\n\t}\n}\n\nexport function workbenchInstantiationService(\n\toverrides?: {\n\t\tenvironmentService?: (instantiationService: IInstantiationService) => IEnvironmentService;\n\t\tfileService?: (instantiationService: IInstantiationService) => IFileService;\n\t\tworkingCopyBackupService?: (instantiationService: IInstantiationService) => IWorkingCopyBackupService;\n\t\tconfigurationService?: (instantiationService: IInstantiationService) => TestConfigurationService;\n\t\ttextFileService?: (instantiationService: IInstantiationService) => ITextFileService;\n\t\tpathService?: (instantiationService: IInstantiationService) => IPathService;\n\t\teditorService?: (instantiationService: IInstantiationService) => IEditorService;\n\t\tcontextKeyService?: (instantiationService: IInstantiationService) => IContextKeyService;\n\t\ttextEditorService?: (instantiationService: IInstantiationService) => ITextEditorService;\n\t},\n\tdisposables: Pick<DisposableStore, 'add'> = new DisposableStore()\n): TestInstantiationService {\n\tconst instantiationService = disposables.add(new TestInstantiationService(new ServiceCollection([ILifecycleService, new TestLifecycleService()])));\n\n\tinstantiationService.stub(IEditorWorkerService, new TestEditorWorkerService());\n\tinstantiationService.stub(IWorkingCopyService, disposables.add(new TestWorkingCopyService()));\n\tconst environmentService = overrides?.environmentService ? overrides.environmentService(instantiationService) : TestEnvironmentService;\n\tinstantiationService.stub(IEnvironmentService, environmentService);\n\tinstantiationService.stub(IWorkbenchEnvironmentService, environmentService);\n\tconst contextKeyService = overrides?.contextKeyService ? overrides.contextKeyService(instantiationService) : instantiationService.createInstance(MockContextKeyService);\n\tinstantiationService.stub(IContextKeyService, contextKeyService);\n\tinstantiationService.stub(IProgressService, new TestProgressService());\n\tconst workspaceContextService = new TestContextService(TestWorkspace);\n\tinstantiationService.stub(IWorkspaceContextService, workspaceContextService);\n\tconst configService = overrides?.configurationService ? overrides.configurationService(instantiationService) : new TestConfigurationService({\n\t\tfiles: {\n\t\t\tparticipants: {\n\t\t\t\ttimeout: 60000\n\t\t\t}\n\t\t}\n\t});\n\tinstantiationService.stub(IConfigurationService, configService);\n\tinstantiationService.stub(ITextResourceConfigurationService, new TestTextResourceConfigurationService(configService));\n\tinstantiationService.stub(IUntitledTextEditorService, disposables.add(instantiationService.createInstance(UntitledTextEditorService)));\n\tinstantiationService.stub(IStorageService, disposables.add(new TestStorageService()));\n\tinstantiationService.stub(IRemoteAgentService, new TestRemoteAgentService());\n\tinstantiationService.stub(ILanguageDetectionService, new TestLanguageDetectionService());\n\tinstantiationService.stub(IPathService, overrides?.pathService ? overrides.pathService(instantiationService) : new TestPathService());\n\tconst layoutService = new TestLayoutService();\n\tinstantiationService.stub(IWorkbenchLayoutService, layoutService);\n\tinstantiationService.stub(IDialogService, new TestDialogService());\n\tconst accessibilityService = new TestAccessibilityService();\n\tinstantiationService.stub(IAccessibilityService, accessibilityService);\n\tinstantiationService.stub(IFileDialogService, instantiationService.createInstance(TestFileDialogService));",
      "\t\t\tdefault:\n\t\t\t\treturn '';\n\t\t}\n\t}\n}\n"
    ]
  },
  {
    "id": "facebook/react",
    "org": "facebook",
    "avatarURL": "https://avatars.githubusercontent.com/u/69631?v=4",
    "name": "facebook/react",
    "url": "https://github.com/facebook/react",
    "lang": "JavaScript",
    "desc": "A declarative, efficient, and flexible JavaScript library for building user interfaces.",
    "star_num": 212750,
    "fork_num": 44665,
    "snippets": [
      "  }, []);\n  return [isDarkMode, () => {}];\n}\n//# sourceMappingURL=data:application/json;charset=utf-8;base64,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",
      "/**\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n *\n * @flow\n */\n\nexport {\n  __SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED,\n  act as unstable_act,\n  Children,\n  Component,\n  Fragment,\n  Profiler,\n  PureComponent,\n  StrictMode,\n  Suspense,\n  cloneElement,\n  createContext,\n  createElement,\n  createFactory,\n  createRef,\n  createServerContext,\n  use,\n  forwardRef,\n  isValidElement,\n  lazy,\n  memo,\n  cache,\n  startTransition,\n  unstable_Cache,\n  unstable_TracingMarker,\n  unstable_DebugTracingMode,\n  unstable_LegacyHidden,\n  unstable_Offscreen,\n  unstable_Scope,\n  unstable_SuspenseList,\n  unstable_getCacheSignal,\n  unstable_getCacheForType,\n  unstable_useCacheRefresh,\n  unstable_useMemoCache,\n  useId,\n  useCallback,\n  useContext,\n  useDebugValue,\n  useDeferredValue,\n  useEffect,\n  experimental_useEffectEvent,\n  useImperativeHandle,\n  useLayoutEffect,\n  useInsertionEffect,\n  useMemo,\n  experimental_useOptimistic,\n  useReducer,\n  useRef,\n  useState,\n  useSyncExternalStore,\n  useTransition,\n  version,\n} from './src/React';\nexport {jsx, jsxs, jsxDEV} from './src/jsx/ReactJSX';\n",
      "      transform(`\nError('This is not a real error message.');\n`)\n    ).toMatchSnapshot();\n  });\n\n  it(\n    \"should output FIXME for errors that don't have a matching error \" +\n      'code, unless opted out with a comment',\n    () => {\n      // TODO: Since this only detects one of many ways to disable a lint\n      // rule, we should instead search for a custom directive (like\n      // no-minify-errors) instead of ESLint. Will need to update our lint\n      // rule to recognize the same directive.\n      expect(\n        transform(`\n// eslint-disable-next-line react-internal/prod-error-codes\nError('This is not a real error message.');\n`)\n      ).toMatchSnapshot();\n    }\n  );\n\n  it('should not touch other calls or new expressions', () => {\n    expect(\n      transform(`\nnew NotAnError();\nNotAnError();\n`)\n    ).toMatchSnapshot();\n  });\n\n  it('should support interpolating arguments with template strings', () => {\n    expect(\n      transform(`\nnew Error(\\`Expected \\${foo} target to be an array; got \\${bar}\\`);\n`)\n    ).toMatchSnapshot();\n  });\n\n  it('should support interpolating arguments with concatenation', () => {\n    expect(\n      transform(`\nnew Error('Expected ' + foo + ' target to be an array; got ' + bar);\n`)\n    ).toMatchSnapshot();\n  });\n\n  it('should support error constructors with concatenated messages', () => {\n    expect(\n      transform(`\nnew Error(\\`Expected \\${foo} target to \\` + \\`be an array; got \\${bar}\\`);\n`)\n    ).toMatchSnapshot();\n  });\n\n  it('handles escaped backticks in template string', () => {\n    expect(\n      transform(`\nnew Error(\\`Expected \\\\\\`\\$\\{listener\\}\\\\\\` listener to be a function, instead got a value of \\\\\\`\\$\\{type\\}\\\\\\` type.\\`);\n`)\n    ).toMatchSnapshot();\n  });\n",
      "/**\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n *\n * @flow\n */\n\nexport * from './src/dom/create-event-handle/Focus';\n",
      "          parseHookNames={parseHookNames}\n          store={store}\n          toggleParseHookNames={toggleParseHookNames}\n        />\n\n        <InspectedElementContextTree\n          bridge={bridge}\n          element={element}\n          inspectedElement={inspectedElement}\n          store={store}\n        />\n\n        {enableStyleXFeatures && (\n          <InspectedElementStyleXPlugin\n            bridge={bridge}\n            element={element}\n            inspectedElement={inspectedElement}\n            store={store}\n          />\n        )}\n\n        <InspectedElementErrorsAndWarningsTree\n          bridge={bridge}\n          element={element}\n          inspectedElement={inspectedElement}\n          store={store}\n        />\n\n        <NativeStyleEditor />\n\n        {showRenderedBy && (\n          <div\n            className={styles.Owners}\n            data-testname=\"InspectedElementView-Owners\">\n            <div className={styles.OwnersHeader}>rendered by</div>\n            {showOwnersList &&\n              ((owners: any): Array<SerializedElement>).map(owner => (\n                <OwnerView\n                  key={owner.id}\n                  displayName={owner.displayName || 'Anonymous'}\n                  hocDisplayNames={owner.hocDisplayNames}\n                  id={owner.id}\n                  isInStore={store.containsElement(owner.id)}\n                  type={owner.type}\n                />\n              ))}\n            {rootType !== null && (\n              <div className={styles.OwnersMetaField}>{rootType}</div>\n            )}\n            {rendererLabel !== null && (\n              <div className={styles.OwnersMetaField}>{rendererLabel}</div>\n            )}\n          </div>\n        )}\n\n        {source !== null && (\n          <Source fileName={source.fileName} lineNumber={source.lineNumber} />\n        )}\n      </div>\n\n      {isContextMenuEnabledForInspectedElement && (\n        <ContextMenu id=\"InspectedElement\">\n          {({path, type: pathType}) => {\n            const copyInspectedElementPath = () => {",
      "    ) {\n      shouldClear = true;\n    }\n  } else {\n    // If we have any remaining hydratable nodes, we need to delete them now.\n    // We only do this deeper than head and body since they tend to have random\n    // other nodes in them. We also ignore components with pure text content in\n    // side of them. We also don't delete anything inside the root container.\n    if (\n      fiber.tag !== HostRoot &&\n      (fiber.tag !== HostComponent ||\n        (shouldDeleteUnhydratedTailInstances(fiber.type) &&\n          !shouldSetTextContent(fiber.type, fiber.memoizedProps)))\n    ) {\n      shouldClear = true;\n    }\n  }\n  if (shouldClear) {\n    let nextInstance = nextHydratableInstance;\n    if (nextInstance) {\n      if (shouldClientRenderOnMismatch(fiber)) {\n        warnIfUnhydratedTailNodes(fiber);\n        throwOnHydrationMismatch(fiber);\n      } else {\n        while (nextInstance) {\n          deleteHydratableInstance(fiber, nextInstance);\n          nextInstance = getNextHydratableSibling(nextInstance);\n        }\n      }\n    }\n  }\n  popToNextHostParent(fiber);\n  if (fiber.tag === SuspenseComponent) {\n    nextHydratableInstance = skipPastDehydratedSuspenseInstance(fiber);\n  } else {\n    nextHydratableInstance = hydrationParentFiber\n      ? getNextHydratableSibling(fiber.stateNode)\n      : null;\n  }\n  return true;\n}\n\nfunction hasUnhydratedTailNodes(): boolean {\n  return isHydrating && nextHydratableInstance !== null;\n}\n\nfunction warnIfUnhydratedTailNodes(fiber: Fiber) {\n  let nextInstance = nextHydratableInstance;\n  while (nextInstance) {\n    warnUnhydratedInstance(fiber, nextInstance);\n    nextInstance = getNextHydratableSibling(nextInstance);\n  }\n}\n\nfunction resetHydrationState(): void {\n  if (!supportsHydration) {\n    return;\n  }\n\n  hydrationParentFiber = null;\n  nextHydratableInstance = null;\n  isHydrating = false;\n  didSuspendOrErrorDEV = false;\n}",
      "      }\n    } catch (err) {\n      // Weird environments may exist.\n      // This code needs a higher fault tolerance\n      // because it runs even with closed DevTools.\n      // TODO: should we catch errors in all injected code, and not just this part?\n    }\n    return 'production';\n  }\n\n  function checkDCE(fn: Function) {\n    // This runs for production versions of React.\n    // Needs to be super safe.\n    try {\n      // $FlowFixMe[method-unbinding]\n      const toString = Function.prototype.toString;\n      const code = toString.call(fn);\n\n      // This is a string embedded in the passed function under DEV-only\n      // condition. However the function executes only in PROD. Therefore,\n      // if we see it, dead code elimination did not work.\n      if (code.indexOf('^_^') > -1) {\n        // Remember to report during next injection.\n        hasDetectedBadDCE = true;\n\n        // Bonus: throw an exception hoping that it gets picked up by a reporting system.\n        // Not synchronously so that it doesn't break the calling code.\n        setTimeout(function () {\n          throw new Error(\n            'React is running in production mode, but dead code ' +\n              'elimination has not been applied. Read how to correctly ' +\n              'configure React for production: ' +\n              'https://reactjs.org/link/perf-use-production-build',\n          );\n        });\n      }\n    } catch (err) {}\n  }\n\n  // NOTE: KEEP IN SYNC with src/backend/utils.js\n  function formatWithStyles(\n    inputArgs: $ReadOnlyArray<any>,\n    style?: string,\n  ): $ReadOnlyArray<any> {\n    if (\n      inputArgs === undefined ||\n      inputArgs === null ||\n      inputArgs.length === 0 ||\n      // Matches any of %c but not %%c\n      (typeof inputArgs[0] === 'string' &&\n        inputArgs[0].match(/([^%]|^)(%c)/g)) ||\n      style === undefined\n    ) {\n      return inputArgs;\n    }\n\n    // Matches any of %(o|O|d|i|s|f), but not %%(o|O|d|i|s|f)\n    const REGEXP = /([^%]|^)((%%)*)(%([oOdisf]))/g;\n    if (typeof inputArgs[0] === 'string' && inputArgs[0].match(REGEXP)) {\n      return [`%c${inputArgs[0]}`, style, ...inputArgs.slice(1)];\n    } else {\n      const firstArg = inputArgs.reduce((formatStr, elem, i) => {\n        if (i > 0) {\n          formatStr += ' ';",
      " *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n *\n * @emails react-core\n * @jest-environment node\n */\n'use strict';\n\nlet React;\nlet ReactNoop;\nlet waitForAll;\nlet act;\n\ndescribe('ReactSuspense', () => {\n  beforeEach(() => {\n    jest.resetModules();\n\n    React = require('react');\n    ReactNoop = require('react-noop-renderer');\n\n    const InternalTestUtils = require('internal-test-utils');\n    waitForAll = InternalTestUtils.waitForAll;\n    act = InternalTestUtils.act;\n  });\n\n  function createThenable() {\n    let completed = false;\n    let resolve;\n    const promise = new Promise(res => {\n      resolve = () => {\n        completed = true;\n        res();\n      };\n    });\n    const PromiseComp = () => {\n      if (!completed) {\n        throw promise;\n      }\n      return 'Done';\n    };\n    return {promise, resolve, PromiseComp};\n  }\n\n  // Warning don't fire in production, so this test passes in prod even if\n  // the suspenseCallback feature is not enabled\n  // @gate www || !__DEV__\n  it('check type', async () => {\n    const {PromiseComp} = createThenable();\n\n    const elementBadType = (\n      <React.Suspense suspenseCallback={1} fallback={'Waiting'}>\n        <PromiseComp />\n      </React.Suspense>\n    );\n\n    ReactNoop.render(elementBadType);\n    await expect(async () => await waitForAll([])).toErrorDev([\n      'Warning: Unexpected type for suspenseCallback.',\n    ]);\n\n    const elementMissingCallback = (\n      <React.Suspense fallback={'Waiting'}>\n        <PromiseComp />",
      "      // This class uses legacy context, which triggers warnings,\n      // the procedures for which use a Map to store fibers.\n      class Boundary extends React.Component {\n        state = {didError: false};\n        componentDidCatch() {\n          this.setState({didError: true});\n        }\n        static contextTypes = {\n          color: () => null,\n        };\n        render() {\n          return this.state.didError ? null : <Thing />;\n        }\n      }\n      ReactNoop.render(\n        <React.StrictMode>\n          <Boundary />\n        </React.StrictMode>,\n      );\n      await expect(async () => {\n        await waitForAll([]);\n      }).toErrorDev([\n        'Legacy context API has been detected within a strict-mode tree',\n      ]);\n    }\n\n    // First, verify that this code path normally receives Fibers as keys,\n    // and that they're not extensible.\n    jest.resetModules();\n    let receivedNonExtensibleObjects;\n    // eslint-disable-next-line no-extend-native\n    Map.prototype.set = function (key) {\n      if (typeof key === 'object' && key !== null) {\n        if (!Object.isExtensible(key)) {\n          receivedNonExtensibleObjects = true;\n        }\n      }\n      return realMapSet.apply(this, arguments);\n    };\n    React = require('react');\n    ReactNoop = require('react-noop-renderer');\n    Scheduler = require('scheduler');\n    let InternalTestUtils = require('internal-test-utils');\n    waitForAll = InternalTestUtils.waitForAll;\n    waitFor = InternalTestUtils.waitFor;\n    waitForThrow = InternalTestUtils.waitForThrow;\n    assertLog = InternalTestUtils.assertLog;\n\n    try {\n      receivedNonExtensibleObjects = false;\n      await triggerCodePathThatUsesFibersAsMapKeys();\n    } finally {\n      // eslint-disable-next-line no-extend-native\n      Map.prototype.set = realMapSet;\n    }\n    // If this fails, find another code path in Fiber\n    // that passes Fibers as keys to Maps.\n    // Note that we only expect them to be non-extensible\n    // in development.\n    expect(receivedNonExtensibleObjects).toBe(__DEV__);\n\n    // Next, verify that a Map polyfill that \"writes\" to keys\n    // doesn't cause a failure.\n    jest.resetModules();",
      "\nmodule.exports = CustomSequencer;\n"
    ]
  },
  {
    "id": "angular/angular",
    "org": "angular",
    "avatarURL": "https://avatars.githubusercontent.com/u/139426?v=4",
    "name": "angular/angular",
    "url": "https://github.com/angular/angular",
    "lang": "TypeScript",
    "desc": "The core infrastructure backend (BaaS) framework for Angular.",
    "star_num": 89989,
    "fork_num": 24177,
    "snippets": [
      "})\nexport class TransferStateComponentOnInit implements OnInit {\n  responseOne: string = '';\n\n  constructor(private readonly httpClient: HttpClient) {}\n\n  ngOnInit(): void {\n    // Test that HTTP cache works when HTTP call is made in a lifecycle hook.\n    this.httpClient.get<any>('http://localhost:4206/api').subscribe((response) => {\n      this.responseOne = response.data;\n    });\n  }\n}\n",
      "\n    expect(webPlayer.keyframes).toEqual([\n      new Map<string, string|number>([['height', '0px'], ['offset', 0]]),\n      new Map<string, string|number>([['height', '100px'], ['offset', 1]])\n    ]);\n\n    webPlayer.finish();\n\n    cmp.exp = false;\n    fixture.detectChanges();\n    engine.flush();\n\n    expect(engine.players.length).toEqual(1);\n    webPlayer =\n        (engine.players[0] as TransitionAnimationPlayer).getRealPlayer() as ɵWebAnimationsPlayer;\n\n    expect(webPlayer.keyframes).toEqual([\n      new Map<string, string|number>([['height', '100px'], ['offset', 0]]),\n      new Map<string, string|number>([['height', '0px'], ['offset', 1]])\n    ]);\n  });\n\n  it('should compute (!) animation styles for a container that is being inserted', () => {\n    @Component({\n      selector: 'ani-cmp',\n      template: `\n          <div @auto *ngIf=\"exp\">\n            <div style=\"line-height:20px;\">1</div>\n            <div style=\"line-height:20px;\">2</div>\n            <div style=\"line-height:20px;\">3</div>\n            <div style=\"line-height:20px;\">4</div>\n            <div style=\"line-height:20px;\">5</div>\n          </div>\n        `,\n      animations: [trigger(\n          'auto',\n          [transition(':enter', [style({height: '!'}), animate(1000, style({height: '120px'}))])])]\n    })\n    class Cmp {\n      public exp: boolean = false;\n    }\n\n    TestBed.configureTestingModule({declarations: [Cmp]});\n\n    const engine = TestBed.inject(ɵAnimationEngine);\n    const fixture = TestBed.createComponent(Cmp);\n    const cmp = fixture.componentInstance;\n\n    cmp.exp = true;\n    fixture.detectChanges();\n    engine.flush();\n\n    expect(engine.players.length).toEqual(1);\n    let webPlayer =\n        (engine.players[0] as TransitionAnimationPlayer).getRealPlayer() as ɵWebAnimationsPlayer;\n\n    expect(webPlayer.keyframes).toEqual([\n      new Map<string, string|number>([['height', '100px'], ['offset', 0]]),\n      new Map<string, string|number>([['height', '120px'], ['offset', 1]])\n    ]);\n  });\n\n  it('should compute pre (!) and post (*) animation styles with different dom states', () => {\n    @Component({",
      "\n// #endregion former examples that might be worth documenting\n\n",
      " * @license\n * Copyright Google LLC All Rights Reserved.\n *\n * Use of this source code is governed by an MIT-style license that can be\n * found in the LICENSE file at https://angular.io/license\n */\n\nimport * as o from '../../../../output/output_ast';\nimport * as ir from '../../ir';\nimport type {CompilationJob} from '../compilation';\n\nexport function phasePureLiteralStructures(job: CompilationJob): void {\n  for (const view of job.units) {\n    for (const op of view.update) {\n      ir.transformExpressionsInOp(op, (expr, flags) => {\n        if (flags & ir.VisitorContextFlag.InChildOperation) {\n          return expr;\n        }\n\n        if (expr instanceof o.LiteralArrayExpr) {\n          return transformLiteralArray(expr);\n        } else if (expr instanceof o.LiteralMapExpr) {\n          return transformLiteralMap(expr);\n        }\n\n        return expr;\n      }, ir.VisitorContextFlag.None);\n    }\n  }\n}\n\nfunction transformLiteralArray(expr: o.LiteralArrayExpr): o.Expression {\n  const derivedEntries: o.Expression[] = [];\n  const nonConstantArgs: o.Expression[] = [];\n  for (const entry of expr.entries) {\n    if (entry.isConstant()) {\n      derivedEntries.push(entry);\n    } else {\n      const idx = nonConstantArgs.length;\n      nonConstantArgs.push(entry);\n      derivedEntries.push(new ir.PureFunctionParameterExpr(idx));\n    }\n  }\n  return new ir.PureFunctionExpr(o.literalArr(derivedEntries), nonConstantArgs);\n}\n\nfunction transformLiteralMap(expr: o.LiteralMapExpr): o.Expression {\n  let derivedEntries: o.LiteralMapEntry[] = [];\n  const nonConstantArgs: o.Expression[] = [];\n  for (const entry of expr.entries) {\n    if (entry.value.isConstant()) {\n      derivedEntries.push(entry);\n    } else {\n      const idx = nonConstantArgs.length;\n      nonConstantArgs.push(entry.value);\n      derivedEntries.push(new o.LiteralMapEntry(\n          entry.key,\n          new ir.PureFunctionParameterExpr(idx),\n          entry.quoted,\n          ));\n    }\n  }\n  return new ir.PureFunctionExpr(o.literalMap(derivedEntries), nonConstantArgs);\n}",
      "  selector: 'admin-users'\n})\nexport class UsersComponent {}\n// #enddocregion example\n",
      "\n/**\n * Creates a TypeScript program instance for a TypeScript project within\n * the virtual file system tree.\n * @param tree Virtual file system tree that contains the source files.\n * @param tsconfigPath Virtual file system path that resolves to the TypeScript project.\n * @param basePath Base path for the virtual file system tree.\n * @param fakeFileRead Optional file reader function. Can be used to overwrite files in\n *   the TypeScript program, or to add in-memory files (e.g. to add global types).\n * @param additionalFiles Additional file paths that should be added to the program.\n */\nexport function createMigrationProgram(\n    tree: Tree, tsconfigPath: string, basePath: string, fakeFileRead?: FakeReadFileFn,\n    additionalFiles?: string[]) {\n  const {rootNames, options, host} =\n      createProgramOptions(tree, tsconfigPath, basePath, fakeFileRead, additionalFiles);\n  return ts.createProgram(rootNames, options, host);\n}\n\n/**\n * Creates the options necessary to instantiate a TypeScript program.\n * @param tree Virtual file system tree that contains the source files.\n * @param tsconfigPath Virtual file system path that resolves to the TypeScript project.\n * @param basePath Base path for the virtual file system tree.\n * @param fakeFileRead Optional file reader function. Can be used to overwrite files in\n *   the TypeScript program, or to add in-memory files (e.g. to add global types).\n * @param additionalFiles Additional file paths that should be added to the program.\n * @param optionOverrides Overrides of the parsed compiler options.\n */\nexport function createProgramOptions(\n    tree: Tree, tsconfigPath: string, basePath: string, fakeFileRead?: FakeReadFileFn,\n    additionalFiles?: string[], optionOverrides?: ts.CompilerOptions) {\n  // Resolve the tsconfig path to an absolute path. This is needed as TypeScript otherwise\n  // is not able to resolve root directories in the given tsconfig. More details can be found\n  // in the following issue: https://github.com/microsoft/TypeScript/issues/37731.\n  tsconfigPath = resolve(basePath, tsconfigPath);\n  const parsed = parseTsconfigFile(tsconfigPath, dirname(tsconfigPath));\n  const options = optionOverrides ? {...parsed.options, ...optionOverrides} : parsed.options;\n  const host = createMigrationCompilerHost(tree, options, basePath, fakeFileRead);\n  return {rootNames: parsed.fileNames.concat(additionalFiles || []), options, host};\n}\n\nfunction createMigrationCompilerHost(\n    tree: Tree, options: ts.CompilerOptions, basePath: string,\n    fakeRead?: FakeReadFileFn): ts.CompilerHost {\n  const host = ts.createCompilerHost(options, true);\n  const defaultReadFile = host.readFile;\n\n  // We need to overwrite the host \"readFile\" method, as we want the TypeScript\n  // program to be based on the file contents in the virtual file tree. Otherwise\n  // if we run multiple migrations we might have intersecting changes and\n  // source files.\n  host.readFile = fileName => {\n    const treeRelativePath = relative(basePath, fileName);\n    let result: string|undefined = fakeRead?.(treeRelativePath);\n\n    if (typeof result !== 'string') {\n      // If the relative path resolved to somewhere outside of the tree, fall back to\n      // TypeScript's default file reading function since the `tree` will throw an error.\n      result = treeRelativePath.startsWith('..') ? defaultReadFile.call(host, fileName) :\n                                                   tree.read(treeRelativePath)?.toString();\n    }\n\n    // Strip BOM as otherwise TSC methods (Ex: getWidth) will return an offset,",
      "})\nexport class SvgApp {\n}\n\n@NgModule({bootstrap: [SvgApp], declarations: [SvgApp, SvgGroup], imports: [BrowserModule]})\nexport class ExampleModule {\n}\n\nplatformBrowserDynamic().bootstrapModule(ExampleModule);\n",
      "platformBrowserDynamic().bootstrapModule(AppModule);\n\nconst HEROES: Hero[] = [\n  { id: 1, name: 'Bombasto' },\n  { id: 2, name: 'Tornado' },\n  { id: 3, name: 'Magneta' }\n];\n\nfunction getHeroes(): Promise<Hero[]> {\n  return Promise.resolve(HEROES); // TODO: get hero data from the server;\n}\n",
      "}\n\n/**\n * Represents an attribute that has been extracted for inclusion in the consts array.\n */\nexport interface ExtractedAttributeOp extends Op<CreateOp> {\n  kind: OpKind.ExtractedAttribute;\n\n  /**\n   * The `XrefId` of the template-like element the extracted attribute will belong to.\n   */\n  target: XrefId;\n\n  /**\n   *  The kind of binding represented by this extracted attribute.\n   */\n  bindingKind: BindingKind;\n\n  /**\n   * The name of the extracted attribute.\n   */\n  name: string;\n\n  /**\n   * The value expression of the extracted attribute.\n   */\n  expression: o.Expression|null;\n}\n\n/**\n * Create an `ExtractedAttributeOp`.\n */\nexport function createExtractedAttributeOp(\n    target: XrefId, bindingKind: BindingKind, name: string,\n    expression: o.Expression|null): ExtractedAttributeOp {\n  return {\n    kind: OpKind.ExtractedAttribute,\n    target,\n    bindingKind,\n    name,\n    expression,\n    ...NEW_OP,\n  };\n}\n\n/**\n * Represents an i18n message that has been extracted for inclusion in the consts array.\n */\nexport interface ExtractedMessageOp extends Op<CreateOp> {\n  kind: OpKind.ExtractedMessage;\n\n  /**\n   * A reference to the i18n op this message was extracted from.\n   */\n  owner: XrefId;\n\n  /**\n   * The message expression.\n   */\n  expression: o.Expression;\n\n  /**\n   * The statements to construct the message.\n   */",
      "    availableUnits: 99,\n    wifi: true,\n    laundry: false,\n  };\n}\n\n"
    ]
  },
  {
    "id": "tensorflow/tensorflow",
    "org": "tensorflow",
    "avatarURL": "https://avatars.githubusercontent.com/u/15658638?v=4",
    "name": "tensorflow/tensorflow",
    "url": "https://github.com/tensorflow/tensorflow",
    "lang": "Python",
    "desc": "An open-source platform for machine learning.",
    "star_num": 177438,
    "fork_num": 88887,
    "snippets": [
      "# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"estimator_lib python module.\n\nImporting from tensorflow.python.estimator is unsupported\nand will soon break!\n\"\"\"\n# pylint: disable=unused-import,g-bad-import-order,g-import-not-at-top,wildcard-import\n\nfrom tensorflow_estimator.python.estimator import estimator_lib\n\n# Include attrs that start with single underscore.\n_HAS_DYNAMIC_ATTRIBUTES = True\nestimator_lib.__all__ = [\n    s for s in dir(estimator_lib) if not s.startswith('__')\n]\n\nfrom tensorflow_estimator.python.estimator.estimator_lib import *\n",
      "\ndef extract_valid_libs(filepath):\n  \"\"\"Evaluate syslibs_configure.bzl, return the VALID_LIBS set from that file.\"\"\"\n\n  # Stub only\n  def repository_rule(**kwargs):  # pylint: disable=unused-variable\n    del kwargs\n\n  # Populates VALID_LIBS\n  with open(filepath, 'r') as f:\n    f_globals = {'repository_rule': repository_rule}\n    f_locals = {}\n    exec(f.read(), f_globals, f_locals)  # pylint: disable=exec-used\n\n  return set(f_locals['VALID_LIBS'])\n\n\ndef extract_system_builds(filepath):\n  \"\"\"Extract the 'name' argument of all rules with a system_build_file argument.\"\"\"\n  lib_names = []\n  system_build_files = []\n  current_name = None\n  with open(filepath, 'r') as f:\n    for line in f:\n      line = line.strip()\n      if line.startswith('name = '):\n        current_name = line[7:-1].strip('\"')\n      elif line.startswith('system_build_file = '):\n        lib_names.append(current_name)\n        # Split at '=' to extract rhs, then extract value between quotes\n        system_build_spec = line.split('=')[-1].split('\"')[1]\n        assert system_build_spec.startswith('//')\n        system_build_files.append(system_build_spec[2:].replace(':', os.sep))\n  return lib_names, system_build_files\n\n\nsyslibs = extract_valid_libs(syslibs_configure_path)\n\nsyslibs_from_workspace = set()\nsystem_build_files_from_workspace = []\nfor current_path in glob.glob(workspace_glob) + glob.glob(third_party_glob):\n  cur_lib_names, build_files = extract_system_builds(current_path)\n  syslibs_from_workspace.update(cur_lib_names)\n  system_build_files_from_workspace.extend(build_files)\n\nmissing_build_files = [\n    file for file in system_build_files_from_workspace\n    if not os.path.isfile(os.path.join(tf_source_path, file))\n]\n\nhas_error = False\n\nif missing_build_files:\n  has_error = True\n  print('Missing system build files: ' + ', '.join(missing_build_files))\n\nif syslibs != syslibs_from_workspace:\n  has_error = True\n  # Libs present in workspace files but not in the allowlist\n  missing_syslibs = syslibs_from_workspace - syslibs\n  if missing_syslibs:\n    libs = ', '.join(sorted(missing_syslibs))\n    print('Libs missing from syslibs_configure: ' + libs)\n  # Libs present in the allow list but not in workspace files",
      "# ==============================================================================\n\"\"\"Test configs for tile.\"\"\"\nimport tensorflow as tf\nfrom tensorflow.lite.testing.zip_test_utils import create_tensor_data\nfrom tensorflow.lite.testing.zip_test_utils import make_zip_of_tests\nfrom tensorflow.lite.testing.zip_test_utils import register_make_test_function\n\n\n@register_make_test_function()\ndef make_tile_tests(options):\n  \"\"\"Make a set of tests to do tile.\"\"\"\n  test_parameters = [\n      {\n          \"input_dtype\": [tf.float32, tf.int32, tf.bool, tf.string],\n          \"input_shape\": [[3, 2, 1], [2, 2, 2]],\n          \"multiplier_dtype\": [tf.int32, tf.int64],\n          \"multiplier_shape\": [[3]]\n      },\n      {\n          \"input_dtype\": [tf.float32, tf.int32],\n          \"input_shape\": [[]],\n          \"multiplier_dtype\": [tf.int32, tf.int64],\n          \"multiplier_shape\": [[0]]\n      },\n      {\n          \"input_dtype\": [tf.float32],\n          \"input_shape\": [[3, 2, 1]],\n          \"multiplier_dtype\": [tf.int32, tf.int64],\n          \"multiplier_shape\": [[3]],\n          \"fully_quantize\": [True],\n          # The input range is used to create representative dataset for both\n          # input and multiplier so it needs to be positive.\n          \"input_range\": [(1, 10)],\n      }\n  ]\n\n  def build_graph(parameters):\n    \"\"\"Build the tile op testing graph.\"\"\"\n    input_value = tf.compat.v1.placeholder(\n        dtype=parameters[\"input_dtype\"],\n        shape=parameters[\"input_shape\"],\n        name=\"input\")\n    multiplier_value = tf.compat.v1.placeholder(\n        dtype=parameters[\"multiplier_dtype\"],\n        shape=parameters[\"multiplier_shape\"],\n        name=\"multiplier\")\n    out = tf.tile(input_value, multiplier_value)\n    return [input_value, multiplier_value], [out]\n\n  def build_inputs(parameters, sess, inputs, outputs):\n    min_value, max_value = parameters.get(\"input_range\", (-10, 10))\n    input_value = create_tensor_data(\n        parameters[\"input_dtype\"],\n        parameters[\"input_shape\"],\n        min_value=min_value,\n        max_value=max_value)\n    multipliers_value = create_tensor_data(\n        parameters[\"multiplier_dtype\"],\n        parameters[\"multiplier_shape\"],\n        min_value=0)\n    return [input_value, multipliers_value], sess.run(\n        outputs,\n        feed_dict={\n            inputs[0]: input_value,",
      "\nimport enum\nimport functools\nimport pprint\nimport shutil\nimport sys\nimport tempfile\nimport time\nimport warnings\n\nfrom absl import logging\n\nfrom google.protobuf import text_format as _text_format\nfrom google.protobuf.message import DecodeError\nfrom tensorflow.core.framework import graph_pb2 as _graph_pb2\nfrom tensorflow.lite.experimental.microfrontend.python.ops import audio_microfrontend_op  # pylint: disable=unused-import\nfrom tensorflow.lite.python import conversion_metadata_schema_py_generated as conversion_metdata_fb\nfrom tensorflow.lite.python import lite_constants as constants\nfrom tensorflow.lite.python.convert import convert_graphdef as _convert_graphdef\nfrom tensorflow.lite.python.convert import convert_graphdef_with_arrays as _convert_graphdef_with_arrays\nfrom tensorflow.lite.python.convert import convert_jax_hlo as _convert_jax_hlo\nfrom tensorflow.lite.python.convert import convert_saved_model as _convert_saved_model\nfrom tensorflow.lite.python.convert import ConverterError  # pylint: disable=unused-import\nfrom tensorflow.lite.python.convert import deduplicate_readonly_buffers as _deduplicate_readonly_buffers\nfrom tensorflow.lite.python.convert import mlir_quantize as _mlir_quantize\nfrom tensorflow.lite.python.convert import mlir_sparsify as _mlir_sparsify\nfrom tensorflow.lite.python.convert import OpsSet\nfrom tensorflow.lite.python.convert import toco_convert  # pylint: disable=unused-import\nfrom tensorflow.lite.python.convert_phase import Component\nfrom tensorflow.lite.python.convert_phase import convert_phase\nfrom tensorflow.lite.python.convert_phase import SubComponent\nfrom tensorflow.lite.python.convert_saved_model import freeze_saved_model as _freeze_saved_model\nfrom tensorflow.lite.python.interpreter import Interpreter  # pylint: disable=unused-import\nfrom tensorflow.lite.python.interpreter import load_delegate  # pylint: disable=unused-import\nfrom tensorflow.lite.python.interpreter import OpResolverType  # pylint: disable=unused-import\nfrom tensorflow.lite.python.metrics import metrics\nfrom tensorflow.lite.python.op_hint import convert_op_hints_to_stubs  # pylint: disable=unused-import\nfrom tensorflow.lite.python.op_hint import is_ophint_converted as _is_ophint_converted\nfrom tensorflow.lite.python.op_hint import OpHint  # pylint: disable=unused-import\nfrom tensorflow.lite.python.optimize import calibrator as _calibrator\nfrom tensorflow.lite.python.util import _xla_computation\nfrom tensorflow.lite.python.util import build_debug_info_func as _build_debug_info_func\nfrom tensorflow.lite.python.util import convert_debug_info_func as _convert_debug_info_func\nfrom tensorflow.lite.python.util import freeze_graph as _freeze_graph\nfrom tensorflow.lite.python.util import get_debug_info as _get_debug_info\nfrom tensorflow.lite.python.util import get_grappler_config as _get_grappler_config\nfrom tensorflow.lite.python.util import get_sparsity_modes as _get_sparsity_modes\nfrom tensorflow.lite.python.util import get_tensor_name as _get_tensor_name\nfrom tensorflow.lite.python.util import get_tensors_from_tensor_names as _get_tensors_from_tensor_names\nfrom tensorflow.lite.python.util import get_tf_type_name as _get_tf_type_name\nfrom tensorflow.lite.python.util import is_frozen_graph as _is_frozen_graph\nfrom tensorflow.lite.python.util import model_input_signature as _model_input_signature\nfrom tensorflow.lite.python.util import modify_model_io_type as _modify_model_io_type\nfrom tensorflow.lite.python.util import populate_conversion_metadata as _populate_conversion_metadata\nfrom tensorflow.lite.python.util import run_graph_optimizations as _run_graph_optimizations\nfrom tensorflow.lite.python.util import set_tensor_shapes as _set_tensor_shapes\nfrom tensorflow.lite.python.util import trace_model_call as _trace_model_call\nfrom tensorflow.lite.tools import flatbuffer_utils\nfrom tensorflow.lite.tools.optimize.debugging.python.debugger import QuantizationDebugger  # pylint: disable=unused-import\nfrom tensorflow.lite.tools.optimize.debugging.python.debugger import QuantizationDebugOptions  # pylint: disable=unused-import\nfrom tensorflow.python.client import session as _session\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function as _def_function\nfrom tensorflow.python.eager import function as _function",
      "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functional operations for RaggedTensors.\"\"\"\n\nfrom tensorflow.python.ops import map_fn as map_fn_lib\nfrom tensorflow.python.ops.ragged import ragged_tensor\nfrom tensorflow.python.util import nest\n\n\ndef map_fn(fn,\n           elems,\n           dtype=None,\n           parallel_iterations=None,\n           back_prop=True,\n           swap_memory=False,\n           infer_shape=True,\n           name=None):\n  \"\"\"map on the list of tensors unpacked from `elems` on dimension 0.\n\n  The simplest version of `map_fn` repeatedly applies the callable `fn` to a\n  sequence of elements from first to last. The elements are made of the\n  tensors unpacked from `elems`. `dtype` is the data type of the return\n  value of `fn`. Users must provide `dtype` if it is different from\n  the data type of `elems`.\n\n  Suppose that `elems` is unpacked into `values`, a list of tensors. The shape\n  of the result tensor is `[values.shape[0]] + fn(values[0]).shape`.\n\n  This method also allows multi-arity `elems` and output of `fn`.  If `elems`\n  is a (possibly nested) list or tuple of tensors, then each of these tensors\n  must have a matching first (unpack) dimension.  The signature of `fn` may\n  match the structure of `elems`.  That is, if `elems` is\n  `(t1, [t2, t3, [t4, t5]])`, then an appropriate signature for `fn` is:\n  `fn = lambda (t1, [t2, t3, [t4, t5]]):`.\n\n  Furthermore, `fn` may emit a different structure than its input.  For example,\n  `fn` may look like: `fn = lambda t1: return (t1 + 1, t1 - 1)`.  In this case,\n  the `dtype` parameter is not optional: `dtype` must be a type or (possibly\n  nested) tuple of types matching the output of `fn`.\n\n  To apply a functional operation to the nonzero elements of a SparseTensor\n  one of the following methods is recommended. First, if the function is\n  expressible as TensorFlow ops, use\n\n  ```python\n    result = SparseTensor(input.indices, fn(input.values), input.dense_shape)\n  ```\n\n  If, however, the function is not expressible as a TensorFlow op, then use\n\n  ```python\n  result = SparseTensor(",
      "    @compiled_fn\n    def outer(resource1):\n      with ops.device('/device:CPU:0'):\n        r1, res1 = inner(resource1)\n      return r1, res1\n\n    r1, res1 = outer(g1)\n\n    self.assertEqual(r1.numpy(), 10.0)\n    self.assertRegex(r1.backing_device, 'CPU')\n\n    def check_handle(handle, expected_value):\n      self.assertRegex(handle.backing_device, 'CPU')\n      tensor = gen_resource_variable_ops.read_variable_op(\n          handle, dtypes.float32\n      )\n      self.assertEqual(tensor.numpy(), expected_value)\n\n    # Check that handles returned from functions are on CPU and an op using\n    # the resource handle is correctly placed on the device backing the\n    # resource.\n    check_handle(res1, 5.0)\n\n  @test_util.run_gpu_only\n  def testComplexInputOutputDevicePattern(self):\n    \"\"\"Tests input/output mapping logic in partitioning.\"\"\"\n    with ops.device('/device:CPU:0'):\n      rc0 = resource_variable_ops.ResourceVariable(2.0)\n      rc1 = resource_variable_ops.ResourceVariable(3.0)\n      cc0 = array_ops.identity(5.0)\n      cc1 = array_ops.identity(7.0)\n    with ops.device('/device:GPU:0'):\n      rg0 = resource_variable_ops.ResourceVariable(11.0)\n      rg1 = resource_variable_ops.ResourceVariable(13.0)\n      cg0 = array_ops.identity(17.0)\n      cg1 = array_ops.identity(19.0)\n\n    # Make sure tensors are on expected devices.\n    for tensor in [cc0, cc1]:\n      self.assertRegex(tensor.backing_device, 'CPU:0')\n    for tensor in [cg0, cg1]:\n      self.assertRegex(tensor.backing_device, 'GPU:0')\n\n    @compiled_fn\n    def func(rc0, cc0, cg0, rc1, cg1, rg0, rg1, cc1):\n      with ops.device('/device:CPU:0'):\n        m1 = rc0 * cg0\n      with ops.device('/device:GPU:0'):\n        m2 = rg0 * cc0\n\n      with ops.device('/device:CPU:0'):\n        r1 = 1000.0 * m2 + rc1 * cg1\n      with ops.device('/device:GPU:0'):\n        r2 = 1000.0 * m1 + rg1 * cc1\n\n      return r1, r2, m2, m1\n\n    r1, r2, m2, m1 = func(rc0, cc0, cg0, rc1, cg1, rg0, rg1, cc1)\n    self.assertRegex(m1.backing_device, 'CPU')\n    self.assertRegex(r1.backing_device, 'CPU')\n    self.assertRegex(m2.backing_device, 'GPU')\n    self.assertRegex(r2.backing_device, 'GPU')\n    self.assertEqual(m1.numpy(), 34.0)\n    self.assertEqual(r1.numpy(), 55000.0 + 3.0 * 19.0)",
      "    # IndexedSlices. It means the shapes of the three tensors of the\n    # IndexedSlices are (shape, [shape[0]], [shape.ndims]).\"\n    indices_shape = shape[:1]\n    dense_shape = tensor_shape.TensorShape([None]).concatenate(shape[1:])\n    if self._dense_shape is None:\n      dense_shape_dtype = None\n    else:\n      dense_shape_dtype = self._dense_shape.dtype\n    return IndexedSlicesSpec(dense_shape, self.dtype, self._indices.dtype,\n                             dense_shape_dtype, indices_shape)\n\n  def consumers(self):\n    return self._consumers()\n\n\nIndexedSlicesValue = collections.namedtuple(\n    \"IndexedSlicesValue\", [\"values\", \"indices\", \"dense_shape\"])\n\n\n@tf_export(\"IndexedSlicesSpec\")\nclass IndexedSlicesSpec(type_spec.TypeSpec):\n  \"\"\"Type specification for a `tf.IndexedSlices`.\"\"\"\n\n  __slots__ = [\"_shape\", \"_values_dtype\", \"_indices_dtype\",\n               \"_dense_shape_dtype\", \"_indices_shape\"]\n\n  value_type = property(lambda self: IndexedSlices)\n\n  def __init__(self, shape=None, dtype=dtypes.float32,\n               indices_dtype=dtypes.int64, dense_shape_dtype=None,\n               indices_shape=None):\n    \"\"\"Constructs a type specification for a `tf.IndexedSlices`.\n\n    Args:\n      shape: The dense shape of the `IndexedSlices`, or `None` to allow any\n        dense shape.\n      dtype: `tf.DType` of values in the `IndexedSlices`.\n      indices_dtype: `tf.DType` of the `indices` in the `IndexedSlices`.  One\n        of `tf.int32` or `tf.int64`.\n      dense_shape_dtype: `tf.DType` of the `dense_shape` in the `IndexedSlices`.\n        One of `tf.int32`, `tf.int64`, or `None` (if the `IndexedSlices` has\n        no `dense_shape` tensor).\n      indices_shape: The shape of the `indices` component, which indicates\n        how many slices are in the `IndexedSlices`.\n    \"\"\"\n    self._shape = tensor_shape.as_shape(shape)\n    self._values_dtype = dtypes.as_dtype(dtype)\n    self._indices_dtype = dtypes.as_dtype(indices_dtype)\n    if dense_shape_dtype is None:\n      self._dense_shape_dtype = None\n    else:\n      self._dense_shape_dtype = dtypes.as_dtype(dense_shape_dtype)\n    self._indices_shape = tensor_shape.as_shape(indices_shape).with_rank(1)\n\n  def _serialize(self):\n    return (self._shape, self._values_dtype, self._indices_dtype,\n            self._dense_shape_dtype, self._indices_shape)\n\n  @property\n  def _component_specs(self):\n    value_shape = self._indices_shape.concatenate(self._shape[1:])\n    specs = [\n        tensor_spec.TensorSpec(value_shape, self._values_dtype),\n        tensor_spec.TensorSpec(self._indices_shape, self._indices_dtype)]",
      "    def test_fn(y):\n      if all(x for x in y):\n        return\n\n    node = self._parse_and_analyze(test_fn)\n    fn_body = node.body\n\n    self.assertHasLiveIn(fn_body[0], ('all', 'y'))\n\n  def test_live_in_list_comprehension(self):\n\n    def test_fn(y):\n      if [x for x in y]:\n        return\n\n    node = self._parse_and_analyze(test_fn)\n    fn_body = node.body\n\n    self.assertHasLiveIn(fn_body[0], ('y',))\n\n  def test_live_in_list_comprehension_expression(self):\n\n    def test_fn(y, s):\n      s += foo([x for x in y])  # pylint:disable=undefined-variable\n\n    node = self._parse_and_analyze(test_fn)\n    fn_body = node.body\n\n    self.assertHasLiveIn(fn_body[0], ('y', 'foo', 's'))\n\n  def test_live_in_set_comprehension(self):\n\n    def test_fn(y):\n      if {x for x in y}:\n        return\n\n    node = self._parse_and_analyze(test_fn)\n    fn_body = node.body\n\n    self.assertHasLiveIn(fn_body[0], ('y',))\n\n  def test_live_in_dict_comprehension(self):\n\n    def test_fn(y):\n      if {k: v for k, v in y}:\n        return\n\n    node = self._parse_and_analyze(test_fn)\n    fn_body = node.body\n\n    self.assertHasLiveIn(fn_body[0], ('y',))\n\n  def test_global_symbol(self):\n\n    def test_fn(c):\n      global global_a\n      global global_b\n      if global_a:\n        global_b = c\n      else:\n        global_b = c\n      return global_b\n\n    node = self._parse_and_analyze(test_fn)",
      "    return data\n\n  @parameterized.parameters(dtypes.int32, dtypes.int64)\n  def testSimpleGather(self, indices_dtype):\n    data = np.array([0, 1, 2, 3, 7, 5, 8, 9, 10, 11, 15, 13])\n    indices = [3, 4]\n    with self.session():\n      for dtype in _TEST_TYPES:\n        params_np = self._buildParams(data, dtype)\n        params = constant_op.constant(params_np)\n        indices_tf = constant_op.constant(indices, dtype=indices_dtype)\n        gather_t = array_ops.batch_gather(params, indices_tf)\n        expected_result = np.array([3, 7])\n        np_val = self._buildParams(expected_result, dtype)\n        gather_val = self.evaluate(gather_t)\n        self.assertAllEqual(np_val, gather_val)\n        self.assertEqual(np_val.shape, gather_t.get_shape())\n\n  @parameterized.parameters(dtypes.int32, dtypes.int64)\n  def test2DArray(self, indices_dtype):\n    data = np.array([[0, 1, 2, 3, 7, 5], [8, 9, 10, 11, 15, 13]])\n    indices = [[3], [4]]\n    with self.session():\n      for dtype in _TEST_TYPES:\n        params_np = self._buildParams(data, dtype)\n        params = constant_op.constant(params_np)\n        indices_tf = constant_op.constant(indices, dtype=indices_dtype)\n        gather_t = array_ops.batch_gather(params, indices_tf)\n        expected_result = np.array([[3], [15]])\n        np_val = self._buildParams(expected_result, dtype)\n        gather_val = self.evaluate(gather_t)\n        self.assertAllEqual(np_val, gather_val)\n        self.assertEqual(np_val.shape, gather_t.get_shape())\n\n  def testHigherRank(self):\n    data = np.array([[[0, 1, 2], [3, 7, 5]], [[8, 9, 10], [11, 15, 13]]])\n    indices = [[[2, 0], [1, 2]], [[2, 0], [0, 1]]]\n    with self.session():\n      for dtype in _TEST_TYPES:\n        params_np = self._buildParams(data, dtype)\n        params = constant_op.constant(params_np)\n        indices_tf = constant_op.constant(indices)\n        gather_t = array_ops.batch_gather(params, indices_tf)\n        gather_val = self.evaluate(gather_t)\n        expected_result = np.array([[[2, 0], [7, 5]], [[10, 8], [11, 15]]])\n        np_val = self._buildParams(expected_result, dtype)\n        self.assertAllEqual(np_val, gather_val)\n        self.assertEqual(np_val.shape, gather_t.get_shape())\n\n  def testString(self):\n    params = np.array([[b\"asdf\", b\"zxcv\"], [b\"qwer\", b\"uiop\"]])\n    with self.cached_session():\n      indices_tf = constant_op.constant([1])\n      self.assertAllEqual(\n          [[b\"qwer\", b\"uiop\"]],\n          self.evaluate(array_ops.batch_gather(params, indices_tf)))\n\n  def testUnknownIndices(self):\n    # This test needs a placeholder which means we need to construct a graph.\n    with ops.Graph().as_default():\n      params = constant_op.constant([[0, 1, 2]])\n      indices = array_ops.placeholder(dtypes.int32, shape=[None, None])\n      gather_t = array_ops.batch_gather(params, indices)\n      self.assertEqual([1, None], gather_t.get_shape().as_list())",
      "                  dtypes.as_dtype(dtypes._np_float8_e4m3fn))\n    with self.assertRaises(TypeError):\n      dtypes.as_dtype(np.dtype([(\"f1\", np.uint), (\"f2\", np.int32)]))\n\n    class AnObject(object):\n      dtype = \"f4\"\n\n    self.assertIs(dtypes.float32, dtypes.as_dtype(AnObject))\n\n    class AnotherObject(object):\n      dtype = np.dtype(np.complex64)\n\n    self.assertIs(dtypes.complex64, dtypes.as_dtype(AnotherObject))\n\n  def testRealDtype(self):\n    for dtype in [\n        dtypes.float32, dtypes.float64, dtypes.bool, dtypes.uint8, dtypes.int8,\n        dtypes.int16, dtypes.int32, dtypes.int64, dtypes.float8_e5m2,\n        dtypes.float8_e4m3fn\n    ]:\n      self.assertIs(dtype.real_dtype, dtype)\n    self.assertIs(dtypes.complex64.real_dtype, dtypes.float32)\n    self.assertIs(dtypes.complex128.real_dtype, dtypes.float64)\n\n  def testStringConversion(self):\n    self.assertIs(dtypes.float32, dtypes.as_dtype(\"float32\"))\n    self.assertIs(dtypes.float64, dtypes.as_dtype(\"float64\"))\n    self.assertIs(dtypes.int32, dtypes.as_dtype(\"int32\"))\n    self.assertIs(dtypes.uint8, dtypes.as_dtype(\"uint8\"))\n    self.assertIs(dtypes.uint16, dtypes.as_dtype(\"uint16\"))\n    self.assertIs(dtypes.int16, dtypes.as_dtype(\"int16\"))\n    self.assertIs(dtypes.int8, dtypes.as_dtype(\"int8\"))\n    self.assertIs(dtypes.string, dtypes.as_dtype(\"string\"))\n    self.assertIs(dtypes.complex64, dtypes.as_dtype(\"complex64\"))\n    self.assertIs(dtypes.complex128, dtypes.as_dtype(\"complex128\"))\n    self.assertIs(dtypes.int64, dtypes.as_dtype(\"int64\"))\n    self.assertIs(dtypes.bool, dtypes.as_dtype(\"bool\"))\n    self.assertIs(dtypes.qint8, dtypes.as_dtype(\"qint8\"))\n    self.assertIs(dtypes.quint8, dtypes.as_dtype(\"quint8\"))\n    self.assertIs(dtypes.qint32, dtypes.as_dtype(\"qint32\"))\n    self.assertIs(dtypes.bfloat16, dtypes.as_dtype(\"bfloat16\"))\n    self.assertIs(dtypes.float8_e5m2, dtypes.as_dtype(\"float8_e5m2\"))\n    self.assertIs(dtypes.float8_e4m3fn, dtypes.as_dtype(\"float8_e4m3fn\"))\n    self.assertIs(dtypes.float32_ref, dtypes.as_dtype(\"float32_ref\"))\n    self.assertIs(dtypes.float64_ref, dtypes.as_dtype(\"float64_ref\"))\n    self.assertIs(dtypes.int32_ref, dtypes.as_dtype(\"int32_ref\"))\n    self.assertIs(dtypes.uint8_ref, dtypes.as_dtype(\"uint8_ref\"))\n    self.assertIs(dtypes.int16_ref, dtypes.as_dtype(\"int16_ref\"))\n    self.assertIs(dtypes.int8_ref, dtypes.as_dtype(\"int8_ref\"))\n    self.assertIs(dtypes.string_ref, dtypes.as_dtype(\"string_ref\"))\n    self.assertIs(dtypes.complex64_ref, dtypes.as_dtype(\"complex64_ref\"))\n    self.assertIs(dtypes.complex128_ref, dtypes.as_dtype(\"complex128_ref\"))\n    self.assertIs(dtypes.int64_ref, dtypes.as_dtype(\"int64_ref\"))\n    self.assertIs(dtypes.bool_ref, dtypes.as_dtype(\"bool_ref\"))\n    self.assertIs(dtypes.qint8_ref, dtypes.as_dtype(\"qint8_ref\"))\n    self.assertIs(dtypes.quint8_ref, dtypes.as_dtype(\"quint8_ref\"))\n    self.assertIs(dtypes.qint32_ref, dtypes.as_dtype(\"qint32_ref\"))\n    self.assertIs(dtypes.bfloat16_ref, dtypes.as_dtype(\"bfloat16_ref\"))\n    self.assertIs(dtypes.float8_e5m2_ref, dtypes.as_dtype(\"float8_e5m2_ref\"))\n    self.assertIs(dtypes.float8_e4m3fn_ref,\n                  dtypes.as_dtype(\"float8_e4m3fn_ref\"))\n    with self.assertRaises(TypeError):\n      dtypes.as_dtype(\"not_a_type\")\n"
    ]
  },
  {
    "id": "twbs/bootstrap",
    "org": "twbs",
    "avatarURL": "https://avatars.githubusercontent.com/u/2918581?v=4",
    "name": "twbs/bootstrap",
    "url": "https://github.com/twbs/bootstrap",
    "lang": "HTML, CSS, JavaScript",
    "desc": "The most popular HTML, CSS, and JavaScript framework for developing responsive, mobile-first projects on the web.",
    "star_num": 165185,
    "fork_num": 78848,
    "snippets": [
      "   * Constants\n   */\n\n  const NAME = 'popover';\n  const SELECTOR_TITLE = '.popover-header';\n  const SELECTOR_CONTENT = '.popover-body';\n  const Default = {\n    ...Tooltip.Default,\n    content: '',\n    offset: [0, 8],\n    placement: 'right',\n    template: '<div class=\"popover\" role=\"tooltip\">' + '<div class=\"popover-arrow\"></div>' + '<h3 class=\"popover-header\"></h3>' + '<div class=\"popover-body\"></div>' + '</div>',\n    trigger: 'click'\n  };\n  const DefaultType = {\n    ...Tooltip.DefaultType,\n    content: '(null|string|element|function)'\n  };\n\n  /**\n   * Class definition\n   */\n\n  class Popover extends Tooltip {\n    // Getters\n    static get Default() {\n      return Default;\n    }\n    static get DefaultType() {\n      return DefaultType;\n    }\n    static get NAME() {\n      return NAME;\n    }\n\n    // Overrides\n    _isWithContent() {\n      return this._getTitle() || this._getContent();\n    }\n\n    // Private\n    _getContentForTemplate() {\n      return {\n        [SELECTOR_TITLE]: this._getTitle(),\n        [SELECTOR_CONTENT]: this._getContent()\n      };\n    }\n    _getContent() {\n      return this._resolvePossibleFunction(this._config.content);\n    }\n\n    // Static\n    static jQueryInterface(config) {\n      return this.each(function () {\n        const data = Popover.getOrCreateInstance(this, config);\n        if (typeof config !== 'string') {\n          return;\n        }\n        if (typeof data[config] === 'undefined') {\n          throw new TypeError(`No method named \"${config}\"`);\n        }\n        data[config]();\n      });\n    }",
      "---\nlayout: examples\ntitle: Masonry example\nextra_js:\n  - src: \"https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js\"\n    integrity: \"sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D\"\n    async: true\n---\n\n<main class=\"container py-5\">\n  <h1>Bootstrap and Masonry</h1>\n  <p class=\"lead\">Integrate <a href=\"https://masonry.desandro.com/\">Masonry</a> with the Bootstrap grid system and cards component.</p>\n\n  <p>Masonry is not included in Bootstrap. Add it by including the JavaScript plugin manually, or using a CDN like so:</p>\n\n  <pre><code>\n&lt;script src=&quot;https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js&quot; integrity=&quot;sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D&quot; crossorigin=&quot;anonymous&quot; async&gt;&lt;/script&gt;\n  </code></pre>\n\n  <p>By adding <code>data-masonry='{\"percentPosition\": true }'</code> to the <code>.row</code> wrapper, we can combine the powers of Bootstrap's responsive grid and Masonry's positioning.</p>\n\n  <hr class=\"my-5\">\n\n  <div class=\"row\" data-masonry='{\"percentPosition\": true }'>\n    <div class=\"col-sm-6 col-lg-4 mb-4\">\n      <div class=\"card\">\n        {{< placeholder width=\"100%\" height=\"200\" class=\"card-img-top\" text=\"Image cap\" >}}\n        <div class=\"card-body\">\n          <h5 class=\"card-title\">Card title that wraps to a new line</h5>\n          <p class=\"card-text\">This is a longer card with supporting text below as a natural lead-in to additional content. This content is a little bit longer.</p>\n        </div>\n      </div>\n    </div>\n    <div class=\"col-sm-6 col-lg-4 mb-4\">\n      <div class=\"card p-3\">\n        <figure class=\"p-3 mb-0\">\n          <blockquote class=\"blockquote\">\n            <p>A well-known quote, contained in a blockquote element.</p>\n          </blockquote>\n          <figcaption class=\"blockquote-footer mb-0 text-body-secondary\">\n            Someone famous in <cite title=\"Source Title\">Source Title</cite>\n          </figcaption>\n        </figure>\n      </div>\n    </div>\n    <div class=\"col-sm-6 col-lg-4 mb-4\">\n      <div class=\"card\">\n        {{< placeholder width=\"100%\" height=\"200\" class=\"card-img-top\" text=\"Image cap\" >}}\n        <div class=\"card-body\">\n          <h5 class=\"card-title\">Card title</h5>\n          <p class=\"card-text\">This card has supporting text below as a natural lead-in to additional content.</p>\n          <p class=\"card-text\"><small class=\"text-body-secondary\">Last updated 3 mins ago</small></p>\n        </div>\n      </div>\n    </div>\n    <div class=\"col-sm-6 col-lg-4 mb-4\">\n      <div class=\"card text-bg-primary text-center p-3\">\n        <figure class=\"mb-0\">\n          <blockquote class=\"blockquote\">\n            <p>A well-known quote, contained in a blockquote element.</p>\n          </blockquote>\n          <figcaption class=\"blockquote-footer mb-0 text-white\">\n            Someone famous in <cite title=\"Source Title\">Source Title</cite>\n          </figcaption>",
      "    '<div class=\"bd-code-snippet\">',\n    '  <div class=\"bd-clipboard\">',\n    '    <button type=\"button\" class=\"btn-clipboard\">',\n    '      <svg class=\"bi\" role=\"img\" aria-label=\"Copy\"><use xlink:href=\"#clipboard\"/></svg>',\n    '    </button>',\n    '  </div>',\n    '</div>'\n  ].join('')\n\n  // Wrap programmatically code blocks and add copy btn.\n  document.querySelectorAll('.highlight')\n    .forEach(element => {\n      // Ignore examples made by shortcode\n      if (!element.closest('.bd-example-snippet')) {\n        element.insertAdjacentHTML('beforebegin', btnHtml)\n        element.previousElementSibling.append(element)\n      }\n    })\n\n  /**\n   *\n   * @param {string} selector\n   * @param {string} title\n   */\n  function snippetButtonTooltip(selector, title) {\n    document.querySelectorAll(selector).forEach(btn => {\n      bootstrap.Tooltip.getOrCreateInstance(btn, { title })\n    })\n  }\n\n  snippetButtonTooltip('.btn-clipboard', btnTitle)\n  snippetButtonTooltip('.btn-edit', btnEdit)\n\n  const clipboard = new ClipboardJS('.btn-clipboard', {\n    target: trigger => trigger.closest('.bd-code-snippet').querySelector('.highlight'),\n    text: trigger => trigger.closest('.bd-code-snippet').querySelector('.highlight').textContent.trimEnd()\n  })\n\n  clipboard.on('success', event => {\n    const iconFirstChild = event.trigger.querySelector('.bi').firstElementChild\n    const tooltipBtn = bootstrap.Tooltip.getInstance(event.trigger)\n    const namespace = 'http://www.w3.org/1999/xlink'\n    const originalXhref = iconFirstChild.getAttributeNS(namespace, 'href')\n    const originalTitle = event.trigger.title\n\n    tooltipBtn.setContent({ '.tooltip-inner': 'Copied!' })\n    event.trigger.addEventListener('hidden.bs.tooltip', () => {\n      tooltipBtn.setContent({ '.tooltip-inner': btnTitle })\n    }, { once: true })\n    event.clearSelection()\n    iconFirstChild.setAttributeNS(namespace, 'href', originalXhref.replace('clipboard', 'check2'))\n\n    setTimeout(() => {\n      iconFirstChild.setAttributeNS(namespace, 'href', originalXhref)\n      event.trigger.title = originalTitle\n    }, 2000)\n  })\n\n  clipboard.on('error', event => {\n    const modifierKey = /mac/i.test(navigator.userAgent) ? '\\u2318' : 'Ctrl-'\n    const fallbackMsg = `Press ${modifierKey}C to copy`\n    const tooltipBtn = bootstrap.Tooltip.getInstance(event.trigger)\n\n    tooltipBtn.setContent({ '.tooltip-inner': fallbackMsg })",
      "<!doctype html>\n<html lang=\"en\" data-bs-theme=\"auto\">\n  <head>\n    {{ partial \"header\" . }}\n  </head>\n  {{ block \"body_override\" . }}<body>{{ end }}\n    {{ partial \"skippy\" . }}\n    {{ partial \"icons\" . }}\n\n    {{ partial \"docs-navbar\" . }}\n\n    {{ block \"main\" . }}\n    {{ end }}\n\n    {{ partial \"footer\" . }}\n    {{ partial \"scripts\" . }}\n\n    {{ block \"footer\" . }}\n    {{ end }}\n  </body>\n</html>\n",
      "const path = require('node:path')\n\nconst runnerPath = path.join(__dirname, 'runner').replace(/\\\\/g, '/')\n\nrequire.extensions['.scss'] = (module, filename) => {\n  const normalizedFilename = filename.replace(/\\\\/g, '/')\n\n  return module._compile(`\n    const runner = require('${runnerPath}')\n    runner('${normalizedFilename}', { describe, it })\n    `, filename)\n}\n",
      "        </li>\n      </ul>\n      <form class=\"d-flex\" role=\"search\">\n        <input class=\"form-control me-2\" type=\"search\" placeholder=\"Search\" aria-label=\"Search\">\n        <button class=\"btn btn-outline-success\" type=\"submit\">Search</button>\n      </form>\n    </div>\n  </div>\n</nav>\n\n<main class=\"container\">\n  <div class=\"bg-body-tertiary p-5 rounded\">\n    <h1>Navbar example</h1>\n    <p class=\"lead\">This example is a quick exercise to illustrate how fixed to top navbar works. As you scroll, it will remain fixed to the top of your browser’s viewport.</p>\n    <a class=\"btn btn-lg btn-primary\" href=\"{{< docsref \"/components/navbar\" >}}\" role=\"button\">View navbar docs &raquo;</a>\n  </div>\n</main>\n",
      "    trapElement: 'element'\n  };\n\n  /**\n   * Class definition\n   */\n\n  class FocusTrap extends Config {\n    constructor(config) {\n      super();\n      this._config = this._getConfig(config);\n      this._isActive = false;\n      this._lastTabNavDirection = null;\n    }\n\n    // Getters\n    static get Default() {\n      return Default;\n    }\n    static get DefaultType() {\n      return DefaultType;\n    }\n    static get NAME() {\n      return NAME;\n    }\n\n    // Public\n    activate() {\n      if (this._isActive) {\n        return;\n      }\n      if (this._config.autofocus) {\n        this._config.trapElement.focus();\n      }\n      EventHandler.off(document, EVENT_KEY); // guard against infinite focus loop\n      EventHandler.on(document, EVENT_FOCUSIN, event => this._handleFocusin(event));\n      EventHandler.on(document, EVENT_KEYDOWN_TAB, event => this._handleKeydown(event));\n      this._isActive = true;\n    }\n    deactivate() {\n      if (!this._isActive) {\n        return;\n      }\n      this._isActive = false;\n      EventHandler.off(document, EVENT_KEY);\n    }\n\n    // Private\n    _handleFocusin(event) {\n      const {\n        trapElement\n      } = this._config;\n      if (event.target === document || event.target === trapElement || trapElement.contains(event.target)) {\n        return;\n      }\n      const elements = SelectorEngine.focusableChildren(trapElement);\n      if (elements.length === 0) {\n        trapElement.focus();\n      } else if (this._lastTabNavDirection === TAB_NAV_BACKWARD) {\n        elements[elements.length - 1].focus();\n      } else {\n        elements[0].focus();\n      }\n    }",
      "  }\n\n  // Private\n  _configAfterMerge(config) {\n    // TODO: on v6 target should be given explicitly & remove the {target: 'ss-target'} case\n    config.target = getElement(config.target) || document.body\n\n    // TODO: v6 Only for backwards compatibility reasons. Use rootMargin only\n    config.rootMargin = config.offset ? `${config.offset}px 0px -30%` : config.rootMargin\n\n    if (typeof config.threshold === 'string') {\n      config.threshold = config.threshold.split(',').map(value => Number.parseFloat(value))\n    }\n\n    return config\n  }\n\n  _maybeEnableSmoothScroll() {\n    if (!this._config.smoothScroll) {\n      return\n    }\n\n    // unregister any previous listeners\n    EventHandler.off(this._config.target, EVENT_CLICK)\n\n    EventHandler.on(this._config.target, EVENT_CLICK, SELECTOR_TARGET_LINKS, event => {\n      const observableSection = this._observableSections.get(event.target.hash)\n      if (observableSection) {\n        event.preventDefault()\n        const root = this._rootElement || window\n        const height = observableSection.offsetTop - this._element.offsetTop\n        if (root.scrollTo) {\n          root.scrollTo({ top: height, behavior: 'smooth' })\n          return\n        }\n\n        // Chrome 60 doesn't support `scrollTo`\n        root.scrollTop = height\n      }\n    })\n  }\n\n  _getNewObserver() {\n    const options = {\n      root: this._rootElement,\n      threshold: this._config.threshold,\n      rootMargin: this._config.rootMargin\n    }\n\n    return new IntersectionObserver(entries => this._observerCallback(entries), options)\n  }\n\n  // The logic of selection\n  _observerCallback(entries) {\n    const targetElement = entry => this._targetLinks.get(`#${entry.target.id}`)\n    const activate = entry => {\n      this._previousScrollData.visibleEntryTop = entry.target.offsetTop\n      this._process(targetElement(entry))\n    }\n\n    const parentScrollTop = (this._rootElement || document.documentElement).scrollTop\n    const userScrollsDown = parentScrollTop >= this._previousScrollData.parentScrollTop\n    this._previousScrollData.parentScrollTop = parentScrollTop\n",
      " * JavaScript for Bootstrap's docs (https://getbootstrap.com/)\n * Copyright 2011-2023 The Bootstrap Authors\n * Licensed under the Creative Commons Attribution 3.0 Unported License.\n * For details, see https://creativecommons.org/licenses/by/3.0/.\n */\n\n(() => {\n  'use strict'\n\n  // Scroll the active sidebar link into view\n  const sidenav = document.querySelector('.bd-sidebar')\n  const sidenavActiveLink = document.querySelector('.bd-links-nav .active')\n\n  if (sidenav && sidenavActiveLink) {\n    const sidenavHeight = sidenav.clientHeight\n    const sidenavActiveLinkTop = sidenavActiveLink.offsetTop\n    const sidenavActiveLinkHeight = sidenavActiveLink.clientHeight\n    const viewportTop = sidenavActiveLinkTop\n    const viewportBottom = viewportTop - sidenavHeight + sidenavActiveLinkHeight\n\n    if (sidenav.scrollTop > viewportTop || sidenav.scrollTop < viewportBottom) {\n      sidenav.scrollTop = viewportTop - (sidenavHeight / 2) + (sidenavActiveLinkHeight / 2)\n    }\n  }\n})()\n",
      "  */\n(function (global, factory) {\n  typeof exports === 'object' && typeof module !== 'undefined' ? module.exports = factory(require('./base-component.js'), require('./dom/event-handler.js'), require('./util/index.js')) :\n  typeof define === 'function' && define.amd ? define(['./base-component', './dom/event-handler', './util/index'], factory) :\n  (global = typeof globalThis !== 'undefined' ? globalThis : global || self, global.Button = factory(global.BaseComponent, global.EventHandler, global.Index));\n})(this, (function (BaseComponent, EventHandler, index_js) { 'use strict';\n\n  /**\n   * --------------------------------------------------------------------------\n   * Bootstrap button.js\n   * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE)\n   * --------------------------------------------------------------------------\n   */\n\n\n  /**\n   * Constants\n   */\n\n  const NAME = 'button';\n  const DATA_KEY = 'bs.button';\n  const EVENT_KEY = `.${DATA_KEY}`;\n  const DATA_API_KEY = '.data-api';\n  const CLASS_NAME_ACTIVE = 'active';\n  const SELECTOR_DATA_TOGGLE = '[data-bs-toggle=\"button\"]';\n  const EVENT_CLICK_DATA_API = `click${EVENT_KEY}${DATA_API_KEY}`;\n\n  /**\n   * Class definition\n   */\n\n  class Button extends BaseComponent {\n    // Getters\n    static get NAME() {\n      return NAME;\n    }\n\n    // Public\n    toggle() {\n      // Toggle class and sync the `aria-pressed` attribute with the return value of the `.toggle()` method\n      this._element.setAttribute('aria-pressed', this._element.classList.toggle(CLASS_NAME_ACTIVE));\n    }\n\n    // Static\n    static jQueryInterface(config) {\n      return this.each(function () {\n        const data = Button.getOrCreateInstance(this);\n        if (config === 'toggle') {\n          data[config]();\n        }\n      });\n    }\n  }\n\n  /**\n   * Data API implementation\n   */\n\n  EventHandler.on(document, EVENT_CLICK_DATA_API, SELECTOR_DATA_TOGGLE, event => {\n    event.preventDefault();\n    const button = event.target.closest(SELECTOR_DATA_TOGGLE);\n    const data = Button.getOrCreateInstance(button);\n    data.toggle();\n  });"
    ]
  },
  {
    "id": "kubernetes/kubernetes",
    "org": "kubernetes",
    "avatarURL": "https://avatars.githubusercontent.com/u/13629408?v=4",
    "name": "kubernetes/kubernetes",
    "url": "https://github.com/kubernetes/kubernetes",
    "lang": "Go",
    "desc": "Production-Grade Container Scheduling and Management.",
    "star_num": 101446,
    "fork_num": 37656,
    "snippets": [
      "/*\nCopyright 2018 The Kubernetes Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage polymorphichelpers\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\n\tappsv1 \"k8s.io/api/apps/v1\"\n\tappsv1beta1 \"k8s.io/api/apps/v1beta1\"\n\tappsv1beta2 \"k8s.io/api/apps/v1beta2\"\n\textensionsv1beta1 \"k8s.io/api/extensions/v1beta1\"\n\t\"k8s.io/apimachinery/pkg/runtime\"\n\t\"k8s.io/kubectl/pkg/scheme\"\n)\n\nfunc defaultObjectResumer(obj runtime.Object) ([]byte, error) {\n\tswitch obj := obj.(type) {\n\tcase *extensionsv1beta1.Deployment:\n\t\tif !obj.Spec.Paused {\n\t\t\treturn nil, errors.New(\"is not paused\")\n\t\t}\n\t\tobj.Spec.Paused = false\n\t\treturn runtime.Encode(scheme.Codecs.LegacyCodec(extensionsv1beta1.SchemeGroupVersion), obj)\n\n\tcase *appsv1.Deployment:\n\t\tif !obj.Spec.Paused {\n\t\t\treturn nil, errors.New(\"is not paused\")\n\t\t}\n\t\tobj.Spec.Paused = false\n\t\treturn runtime.Encode(scheme.Codecs.LegacyCodec(appsv1.SchemeGroupVersion), obj)\n\n\tcase *appsv1beta2.Deployment:\n\t\tif !obj.Spec.Paused {\n\t\t\treturn nil, errors.New(\"is not paused\")\n\t\t}\n\t\tobj.Spec.Paused = false\n\t\treturn runtime.Encode(scheme.Codecs.LegacyCodec(appsv1beta2.SchemeGroupVersion), obj)\n\n\tcase *appsv1beta1.Deployment:\n\t\tif !obj.Spec.Paused {\n\t\t\treturn nil, errors.New(\"is not paused\")\n\t\t}\n\t\tobj.Spec.Paused = false\n\t\treturn runtime.Encode(scheme.Codecs.LegacyCodec(appsv1beta1.SchemeGroupVersion), obj)\n\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"resuming is not supported\")\n\t}\n}",
      "\tpb \"go.etcd.io/etcd/api/v3/etcdserverpb\"\n\n\thumanize \"github.com/dustin/go-humanize\"\n\t\"go.uber.org/zap\"\n)\n\nconst (\n\t// DefaultQuotaBytes is the number of bytes the backend Size may\n\t// consume before exceeding the space quota.\n\tDefaultQuotaBytes = int64(2 * 1024 * 1024 * 1024) // 2GB\n\t// MaxQuotaBytes is the maximum number of bytes suggested for a backend\n\t// quota. A larger quota may lead to degraded performance.\n\tMaxQuotaBytes = int64(8 * 1024 * 1024 * 1024) // 8GB\n)\n\n// Quota represents an arbitrary quota against arbitrary requests. Each request\n// costs some charge; if there is not enough remaining charge, then there are\n// too few resources available within the quota to apply the request.\ntype Quota interface {\n\t// Available judges whether the given request fits within the quota.\n\tAvailable(req interface{}) bool\n\t// Cost computes the charge against the quota for a given request.\n\tCost(req interface{}) int\n\t// Remaining is the amount of charge left for the quota.\n\tRemaining() int64\n}\n\ntype passthroughQuota struct{}\n\nfunc (*passthroughQuota) Available(interface{}) bool { return true }\nfunc (*passthroughQuota) Cost(interface{}) int       { return 0 }\nfunc (*passthroughQuota) Remaining() int64           { return 1 }\n\ntype backendQuota struct {\n\ts               *EtcdServer\n\tmaxBackendBytes int64\n}\n\nconst (\n\t// leaseOverhead is an estimate for the cost of storing a lease\n\tleaseOverhead = 64\n\t// kvOverhead is an estimate for the cost of storing a key's metadata\n\tkvOverhead = 256\n)\n\nvar (\n\t// only log once\n\tquotaLogOnce sync.Once\n\n\tDefaultQuotaSize = humanize.Bytes(uint64(DefaultQuotaBytes))\n\tmaxQuotaSize     = humanize.Bytes(uint64(MaxQuotaBytes))\n)\n\n// NewBackendQuota creates a quota layer with the given storage limit.\nfunc NewBackendQuota(s *EtcdServer, name string) Quota {\n\tlg := s.Logger()\n\tquotaBackendBytes.Set(float64(s.Cfg.QuotaBackendBytes))\n\n\tif s.Cfg.QuotaBackendBytes < 0 {\n\t\t// disable quotas if negative\n\t\tquotaLogOnce.Do(func() {\n\t\t\tlg.Info(\n\t\t\t\t\"disabled backend quota\",\n\t\t\t\tzap.String(\"quota-name\", name),",
      "\t\tw:                 w,\n\t\tpageOffset:        pageOffset,\n\t\tpageBytes:         pageBytes,\n\t\tbuf:               make([]byte, defaultBufferBytes+pageBytes),\n\t\tbufWatermarkBytes: defaultBufferBytes,\n\t}\n}\n\nfunc (pw *PageWriter) Write(p []byte) (n int, err error) {\n\tif len(p)+pw.bufferedBytes <= pw.bufWatermarkBytes {\n\t\t// no overflow\n\t\tcopy(pw.buf[pw.bufferedBytes:], p)\n\t\tpw.bufferedBytes += len(p)\n\t\treturn len(p), nil\n\t}\n\t// complete the slack page in the buffer if unaligned\n\tslack := pw.pageBytes - ((pw.pageOffset + pw.bufferedBytes) % pw.pageBytes)\n\tif slack != pw.pageBytes {\n\t\tpartial := slack > len(p)\n\t\tif partial {\n\t\t\t// not enough data to complete the slack page\n\t\t\tslack = len(p)\n\t\t}\n\t\t// special case: writing to slack page in buffer\n\t\tcopy(pw.buf[pw.bufferedBytes:], p[:slack])\n\t\tpw.bufferedBytes += slack\n\t\tn = slack\n\t\tp = p[slack:]\n\t\tif partial {\n\t\t\t// avoid forcing an unaligned flush\n\t\t\treturn n, nil\n\t\t}\n\t}\n\t// buffer contents are now page-aligned; clear out\n\tif err = pw.Flush(); err != nil {\n\t\treturn n, err\n\t}\n\t// directly write all complete pages without copying\n\tif len(p) > pw.pageBytes {\n\t\tpages := len(p) / pw.pageBytes\n\t\tc, werr := pw.w.Write(p[:pages*pw.pageBytes])\n\t\tn += c\n\t\tif werr != nil {\n\t\t\treturn n, werr\n\t\t}\n\t\tp = p[pages*pw.pageBytes:]\n\t}\n\t// write remaining tail to buffer\n\tc, werr := pw.Write(p)\n\tn += c\n\treturn n, werr\n}\n\n// Flush flushes buffered data.\nfunc (pw *PageWriter) Flush() error {\n\t_, err := pw.flush()\n\treturn err\n}\n\n// FlushN flushes buffered data and returns the number of written bytes.\nfunc (pw *PageWriter) FlushN() (int, error) {\n\treturn pw.flush()\n}\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage events\n\nimport (\n\t\"testing\"\n\n\t\"k8s.io/kubernetes/test/integration/framework\"\n)\n\nfunc TestMain(m *testing.M) {\n\tframework.EtcdMain(m.Run)\n}\n",
      "\n// AddToScheme builds the kubeadm ComponentConfig scheme using all known ComponentConfig versions.\nfunc AddToScheme(scheme *runtime.Scheme) {\n\tfor _, handler := range known {\n\t\tutilruntime.Must(handler.AddToScheme(scheme))\n\t}\n}\n",
      ")\n",
      "package longpath\n\nimport (\n\t\"path/filepath\"\n\t\"strings\"\n)\n\n// LongAbs makes a path absolute and returns it in NT long path form.\nfunc LongAbs(path string) (string, error) {\n\tif strings.HasPrefix(path, `\\\\?\\`) || strings.HasPrefix(path, `\\\\.\\`) {\n\t\treturn path, nil\n\t}\n\tif !filepath.IsAbs(path) {\n\t\tabsPath, err := filepath.Abs(path)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tpath = absPath\n\t}\n\tif strings.HasPrefix(path, `\\\\`) {\n\t\treturn `\\\\?\\UNC\\` + path[2:], nil\n\t}\n\treturn `\\\\?\\` + path, nil\n}\n",
      "\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage types\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestGetContainerName(t *testing.T) {\n\tvar cases = []struct {\n\t\tlabels        map[string]string\n\t\tcontainerName string\n\t}{\n\t\t{\n\t\t\tlabels: map[string]string{\n\t\t\t\t\"io.kubernetes.container.name\": \"c1\",\n\t\t\t},\n\t\t\tcontainerName: \"c1\",\n\t\t},\n\t\t{\n\t\t\tlabels: map[string]string{\n\t\t\t\t\"io.kubernetes.container.name\": \"c2\",\n\t\t\t},\n\t\t\tcontainerName: \"c2\",\n\t\t},\n\t}\n\tfor _, data := range cases {\n\t\tcontainerName := GetContainerName(data.labels)\n\t\tassert.Equal(t, data.containerName, containerName)\n\t}\n}\n\nfunc TestGetPodName(t *testing.T) {\n\tvar cases = []struct {\n\t\tlabels  map[string]string\n\t\tpodName string\n\t}{\n\t\t{\n\t\t\tlabels: map[string]string{\n\t\t\t\t\"io.kubernetes.pod.name\": \"p1\",\n\t\t\t},\n\t\t\tpodName: \"p1\",\n\t\t},\n\t\t{\n\t\t\tlabels: map[string]string{\n\t\t\t\t\"io.kubernetes.pod.name\": \"p2\",\n\t\t\t},\n\t\t\tpodName: \"p2\",\n\t\t},\n\t}",
      "//sys\tgetsockopt(s int, level int, name int, val unsafe.Pointer, vallen *_Socklen) (err error)\n//sys\tsetsockopt(s int, level int, name int, val unsafe.Pointer, vallen uintptr) (err error)\n//sysnb\tsocket(domain int, typ int, proto int) (fd int, err error)\n//sysnb\tsocketpair(domain int, typ int, proto int, fd *[2]int32) (err error)\n//sysnb\tgetpeername(fd int, rsa *RawSockaddrAny, addrlen *_Socklen) (err error)\n//sysnb\tgetsockname(fd int, rsa *RawSockaddrAny, addrlen *_Socklen) (err error)\n//sys\trecvfrom(fd int, p []byte, flags int, from *RawSockaddrAny, fromlen *_Socklen) (n int, err error)\n//sys\tsendto(s int, buf []byte, flags int, to unsafe.Pointer, addrlen _Socklen) (err error)\n//sys\trecvmsg(s int, msg *Msghdr, flags int) (n int, err error)\n//sys\tsendmsg(s int, msg *Msghdr, flags int) (n int, err error)\n//sys\tmmap(addr uintptr, length uintptr, prot int, flags int, fd int, offset int64) (xaddr uintptr, err error)\n\n//sys\tfutimesat(dirfd int, path string, times *[2]Timeval) (err error)\n//sysnb\tGettimeofday(tv *Timeval) (err error)\n\nfunc Time(t *Time_t) (tt Time_t, err error) {\n\tvar tv Timeval\n\terr = Gettimeofday(&tv)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tif t != nil {\n\t\t*t = Time_t(tv.Sec)\n\t}\n\treturn Time_t(tv.Sec), nil\n}\n\n//sys\tUtime(path string, buf *Utimbuf) (err error)\n//sys\tutimes(path string, times *[2]Timeval) (err error)\n\nfunc setTimespec(sec, nsec int64) Timespec {\n\treturn Timespec{Sec: sec, Nsec: nsec}\n}\n\nfunc setTimeval(sec, usec int64) Timeval {\n\treturn Timeval{Sec: sec, Usec: usec}\n}\n\nfunc Ioperm(from int, num int, on int) (err error) {\n\treturn ENOSYS\n}\n\nfunc Iopl(level int) (err error) {\n\treturn ENOSYS\n}\n\ntype stat_t struct {\n\tDev        uint32\n\tPad0       [3]int32\n\tIno        uint64\n\tMode       uint32\n\tNlink      uint32\n\tUid        uint32\n\tGid        uint32\n\tRdev       uint32\n\tPad1       [3]uint32\n\tSize       int64\n\tAtime      uint32\n\tAtime_nsec uint32\n\tMtime      uint32\n\tMtime_nsec uint32\n\tCtime      uint32\n\tCtime_nsec uint32\n\tBlksize    uint32"
    ]
  },
  {
    "id": "docker/docker-ce",
    "org": "docker",
    "avatarURL": "https://avatars.githubusercontent.com/u/5429470?v=4",
    "name": "docker/docker-ce",
    "url": "https://github.com/docker/docker-ce",
    "lang": "Go",
    "desc": "Docker CE (Community Edition) is the open source container runtime.",
    "star_num": 5669,
    "fork_num": 1551,
    "snippets": [
      "\treturn nil, nil\n}\n",
      "// Pads p to a multiple of k using PKCS #7 standard block padding.\n// See http://tools.ietf.org/html/rfc5652#section-6.3.\nfunc pad(q, p []byte, k int) int {\n\tn := len(p)/k*k + k\n\tcopy(q, p)\n\tc := byte(n - len(p))\n\tfor i := len(p); i < n; i++ {\n\t\tq[i] = c\n\t}\n\treturn n\n}\n\n// Removes PKCS #7 standard block padding from p.\n// See http://tools.ietf.org/html/rfc5652#section-6.3.\n// This function is the inverse of pad.\n// If the padding is not well-formed, unpad returns nil.\nfunc unpad(p []byte) []byte {\n\tc := p[len(p)-1]\n\tfor i := len(p) - int(c); i < len(p); i++ {\n\t\tif i < 0 || p[i] != c {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn p[:len(p)-int(c)]\n}\n\nfunc b64enc(src []byte) []byte {\n\tdst := make([]byte, encoding.EncodedLen(len(src)))\n\tencoding.Encode(dst, src)\n\treturn dst\n}\n\nfunc b64dec(src []byte) []byte {\n\tdst := make([]byte, encoding.DecodedLen(len(src)))\n\tn, err := encoding.Decode(dst, src)\n\tif err != nil {\n\t\treturn nil\n\t}\n\treturn dst[:n]\n}\n\nfunc genhmac(q, p, k []byte) {\n\th := hmac.New(sha256.New, k)\n\th.Write(p)\n\th.Sum(q)\n}\n\n// EncryptAndSign encrypts and signs msg with key k and returns the resulting\n// fernet token. If msg contains text, the text should be encoded\n// with UTF-8 to follow fernet convention.\nfunc EncryptAndSign(msg []byte, k *Key) (tok []byte, err error) {\n\tiv := make([]byte, aes.BlockSize)\n\tif _, err := io.ReadFull(rand.Reader, iv); err != nil {\n\t\treturn nil, err\n\t}\n\tb := make([]byte, encodedLen(len(msg)))\n\tn := gen(b, msg, iv, time.Now(), k)\n\ttok = make([]byte, encoding.EncodedLen(n))\n\tencoding.Encode(tok, b[:n])\n\treturn tok, nil\n}\n\n// VerifyAndDecrypt verifies that tok is a valid fernet token that was signed\n// with a key in k at most ttl time ago only if ttl is greater than zero.",
      "\tcopy(sa.Addr[:], src)\n}\n",
      "}\n\n// ApplyUncompressedLayer parses a diff in the standard layer format from\n// `layer`, and applies it to the directory `dest`. The stream `layer`\n// can only be uncompressed.\n// Returns the size in bytes of the contents of the layer.\nfunc ApplyUncompressedLayer(dest string, layer io.Reader, options *archive.TarOptions) (int64, error) {\n\treturn applyLayerHandler(dest, layer, options, false)\n}\n",
      "\t\t\tif err := cache.CachePolicyRetain(ref.ImmutableRef); err != nil {\n\t\t\t\treturn solver.CacheResult{}, err\n\t\t\t}\n\t\t\tref.ImmutableRef.Metadata().Commit()\n\t\t}\n\t}\n\treturn solver.CacheResult{ID: ref.ID(), CreatedAt: createdAt}, nil\n}\nfunc (s *cacheResultStorage) Load(ctx context.Context, res solver.CacheResult) (solver.Result, error) {\n\treturn s.load(ctx, res.ID, false)\n}\n\nfunc (s *cacheResultStorage) getWorkerRef(id string) (Worker, string, error) {\n\tworkerID, refID, err := parseWorkerRef(id)\n\tif err != nil {\n\t\treturn nil, \"\", err\n\t}\n\tw, err := s.wc.Get(workerID)\n\tif err != nil {\n\t\treturn nil, \"\", err\n\t}\n\treturn w, refID, nil\n}\n\nfunc (s *cacheResultStorage) load(ctx context.Context, id string, hidden bool) (solver.Result, error) {\n\tw, refID, err := s.getWorkerRef(id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif refID == \"\" {\n\t\treturn NewWorkerRefResult(nil, w), nil\n\t}\n\tref, err := w.LoadRef(ctx, refID, hidden)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn NewWorkerRefResult(ref, w), nil\n}\n\nfunc (s *cacheResultStorage) LoadRemote(ctx context.Context, res solver.CacheResult, g session.Group) (*solver.Remote, error) {\n\tw, refID, err := s.getWorkerRef(res.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tref, err := w.LoadRef(ctx, refID, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer ref.Release(context.TODO())\n\twref := WorkerRef{ref, w}\n\tremote, err := wref.GetRemote(ctx, false, compression.Default, g)\n\tif err != nil {\n\t\treturn nil, nil // ignore error. loadRemote is best effort\n\t}\n\treturn remote, nil\n}\nfunc (s *cacheResultStorage) Exists(id string) bool {\n\tref, err := s.load(context.TODO(), id, true)\n\tif err != nil {\n\t\treturn false\n\t}\n\tref.Release(context.TODO())\n\treturn true\n}",
      "// limitations under the License.\n\npackage prometheus\n\n// Collector is the interface implemented by anything that can be used by\n// Prometheus to collect metrics. A Collector has to be registered for\n// collection. See Registerer.Register.\n//\n// The stock metrics provided by this package (Gauge, Counter, Summary,\n// Histogram, Untyped) are also Collectors (which only ever collect one metric,\n// namely itself). An implementer of Collector may, however, collect multiple\n// metrics in a coordinated fashion and/or create metrics on the fly. Examples\n// for collectors already implemented in this library are the metric vectors\n// (i.e. collection of multiple instances of the same Metric but with different\n// label values) like GaugeVec or SummaryVec, and the ExpvarCollector.\ntype Collector interface {\n\t// Describe sends the super-set of all possible descriptors of metrics\n\t// collected by this Collector to the provided channel and returns once\n\t// the last descriptor has been sent. The sent descriptors fulfill the\n\t// consistency and uniqueness requirements described in the Desc\n\t// documentation.\n\t//\n\t// It is valid if one and the same Collector sends duplicate\n\t// descriptors. Those duplicates are simply ignored. However, two\n\t// different Collectors must not send duplicate descriptors.\n\t//\n\t// Sending no descriptor at all marks the Collector as “unchecked”,\n\t// i.e. no checks will be performed at registration time, and the\n\t// Collector may yield any Metric it sees fit in its Collect method.\n\t//\n\t// This method idempotently sends the same descriptors throughout the\n\t// lifetime of the Collector. It may be called concurrently and\n\t// therefore must be implemented in a concurrency safe way.\n\t//\n\t// If a Collector encounters an error while executing this method, it\n\t// must send an invalid descriptor (created with NewInvalidDesc) to\n\t// signal the error to the registry.\n\tDescribe(chan<- *Desc)\n\t// Collect is called by the Prometheus registry when collecting\n\t// metrics. The implementation sends each collected metric via the\n\t// provided channel and returns once the last metric has been sent. The\n\t// descriptor of each sent metric is one of those returned by Describe\n\t// (unless the Collector is unchecked, see above). Returned metrics that\n\t// share the same descriptor must differ in their variable label\n\t// values.\n\t//\n\t// This method may be called concurrently and must therefore be\n\t// implemented in a concurrency safe way. Blocking occurs at the expense\n\t// of total performance of rendering all registered metrics. Ideally,\n\t// Collector implementations support concurrent readers.\n\tCollect(chan<- Metric)\n}\n\n// DescribeByCollect is a helper to implement the Describe method of a custom\n// Collector. It collects the metrics from the provided Collector and sends\n// their descriptors to the provided channel.\n//\n// If a Collector collects the same metrics throughout its lifetime, its\n// Describe method can simply be implemented as:\n//\n//   func (c customCollector) Describe(ch chan<- *Desc) {\n//   \tDescribeByCollect(c, ch)\n//   }\n//",
      "\tDisruptedPods map[string]metav1.Time `json:\"disruptedPods,omitempty\" protobuf:\"bytes,2,rep,name=disruptedPods\"`\n\n\t// Number of pod disruptions that are currently allowed.\n\tPodDisruptionsAllowed int32 `json:\"disruptionsAllowed\" protobuf:\"varint,3,opt,name=disruptionsAllowed\"`\n\n\t// current number of healthy pods\n\tCurrentHealthy int32 `json:\"currentHealthy\" protobuf:\"varint,4,opt,name=currentHealthy\"`\n\n\t// minimum desired number of healthy pods\n\tDesiredHealthy int32 `json:\"desiredHealthy\" protobuf:\"varint,5,opt,name=desiredHealthy\"`\n\n\t// total number of pods counted by this disruption budget\n\tExpectedPods int32 `json:\"expectedPods\" protobuf:\"varint,6,opt,name=expectedPods\"`\n}\n\n// +genclient\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n\n// PodDisruptionBudget is an object to define the max disruption that can be caused to a collection of pods\ntype PodDisruptionBudget struct {\n\tmetav1.TypeMeta `json:\",inline\"`\n\t// +optional\n\tmetav1.ObjectMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"`\n\n\t// Specification of the desired behavior of the PodDisruptionBudget.\n\t// +optional\n\tSpec PodDisruptionBudgetSpec `json:\"spec,omitempty\" protobuf:\"bytes,2,opt,name=spec\"`\n\t// Most recently observed status of the PodDisruptionBudget.\n\t// +optional\n\tStatus PodDisruptionBudgetStatus `json:\"status,omitempty\" protobuf:\"bytes,3,opt,name=status\"`\n}\n\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n\n// PodDisruptionBudgetList is a collection of PodDisruptionBudgets.\ntype PodDisruptionBudgetList struct {\n\tmetav1.TypeMeta `json:\",inline\"`\n\t// +optional\n\tmetav1.ListMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"`\n\tItems           []PodDisruptionBudget `json:\"items\" protobuf:\"bytes,2,rep,name=items\"`\n}\n\n// +genclient\n// +genclient:noVerbs\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n\n// Eviction evicts a pod from its node subject to certain policies and safety constraints.\n// This is a subresource of Pod.  A request to cause such an eviction is\n// created by POSTing to .../pods/<pod name>/evictions.\ntype Eviction struct {\n\tmetav1.TypeMeta `json:\",inline\"`\n\n\t// ObjectMeta describes the pod that is being evicted.\n\t// +optional\n\tmetav1.ObjectMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"`\n\n\t// DeleteOptions may be provided\n\t// +optional\n\tDeleteOptions *metav1.DeleteOptions `json:\"deleteOptions,omitempty\" protobuf:\"bytes,2,opt,name=deleteOptions\"`\n}\n\n// +genclient\n// +genclient:nonNamespaced\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object",
      "\t\"fmt\"\n\t\"os\"\n\n\t\"github.com/cilium/ebpf\"\n)\n\ntype cgroupAttachFlags uint32\n\n// cgroup attach flags\nconst (\n\tflagAllowOverride cgroupAttachFlags = 1 << iota\n\tflagAllowMulti\n\tflagReplace\n)\n\ntype CgroupOptions struct {\n\t// Path to a cgroupv2 folder.\n\tPath string\n\t// One of the AttachCgroup* constants\n\tAttach ebpf.AttachType\n\t// Program must be of type CGroup*, and the attach type must match Attach.\n\tProgram *ebpf.Program\n}\n\n// AttachCgroup links a BPF program to a cgroup.\nfunc AttachCgroup(opts CgroupOptions) (Link, error) {\n\tcgroup, err := os.Open(opts.Path)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"can't open cgroup: %s\", err)\n\t}\n\n\tclone, err := opts.Program.Clone()\n\tif err != nil {\n\t\tcgroup.Close()\n\t\treturn nil, err\n\t}\n\n\tvar cg Link\n\tcg, err = newLinkCgroup(cgroup, opts.Attach, clone)\n\tif errors.Is(err, ErrNotSupported) {\n\t\tcg, err = newProgAttachCgroup(cgroup, opts.Attach, clone, flagAllowMulti)\n\t}\n\tif errors.Is(err, ErrNotSupported) {\n\t\tcg, err = newProgAttachCgroup(cgroup, opts.Attach, clone, flagAllowOverride)\n\t}\n\tif err != nil {\n\t\tcgroup.Close()\n\t\tclone.Close()\n\t\treturn nil, err\n\t}\n\n\treturn cg, nil\n}\n\n// LoadPinnedCgroup loads a pinned cgroup from a bpffs.\nfunc LoadPinnedCgroup(fileName string, opts *ebpf.LoadPinOptions) (Link, error) {\n\tlink, err := LoadPinnedRawLink(fileName, CgroupType, opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &linkCgroup{*link}, nil\n}\n",
      "\tcase reparseTagMountPoint:\n\t\tisMountPoint = true\n\tcase reparseTagSymlink:\n\tdefault:\n\t\treturn nil, &UnsupportedReparsePointError{tag}\n\t}\n\tnameOffset := 8 + binary.LittleEndian.Uint16(b[4:6])\n\tif !isMountPoint {\n\t\tnameOffset += 4\n\t}\n\tnameLength := binary.LittleEndian.Uint16(b[6:8])\n\tname := make([]uint16, nameLength/2)\n\terr := binary.Read(bytes.NewReader(b[nameOffset:nameOffset+nameLength]), binary.LittleEndian, &name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &ReparsePoint{string(utf16.Decode(name)), isMountPoint}, nil\n}\n\nfunc isDriveLetter(c byte) bool {\n\treturn (c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')\n}\n\n// EncodeReparsePoint encodes a Win32 REPARSE_DATA_BUFFER structure describing a symlink or\n// mount point.\nfunc EncodeReparsePoint(rp *ReparsePoint) []byte {\n\t// Generate an NT path and determine if this is a relative path.\n\tvar ntTarget string\n\trelative := false\n\tif strings.HasPrefix(rp.Target, `\\\\?\\`) {\n\t\tntTarget = `\\??\\` + rp.Target[4:]\n\t} else if strings.HasPrefix(rp.Target, `\\\\`) {\n\t\tntTarget = `\\??\\UNC\\` + rp.Target[2:]\n\t} else if len(rp.Target) >= 2 && isDriveLetter(rp.Target[0]) && rp.Target[1] == ':' {\n\t\tntTarget = `\\??\\` + rp.Target\n\t} else {\n\t\tntTarget = rp.Target\n\t\trelative = true\n\t}\n\n\t// The paths must be NUL-terminated even though they are counted strings.\n\ttarget16 := utf16.Encode([]rune(rp.Target + \"\\x00\"))\n\tntTarget16 := utf16.Encode([]rune(ntTarget + \"\\x00\"))\n\n\tsize := int(unsafe.Sizeof(reparseDataBuffer{})) - 8\n\tsize += len(ntTarget16)*2 + len(target16)*2\n\n\ttag := uint32(reparseTagMountPoint)\n\tif !rp.IsMountPoint {\n\t\ttag = reparseTagSymlink\n\t\tsize += 4 // Add room for symlink flags\n\t}\n\n\tdata := reparseDataBuffer{\n\t\tReparseTag:           tag,\n\t\tReparseDataLength:    uint16(size),\n\t\tSubstituteNameOffset: 0,\n\t\tSubstituteNameLength: uint16((len(ntTarget16) - 1) * 2),\n\t\tPrintNameOffset:      uint16(len(ntTarget16) * 2),\n\t\tPrintNameLength:      uint16((len(target16) - 1) * 2),\n\t}\n\n\tvar b bytes.Buffer\n\tbinary.Write(&b, binary.LittleEndian, &data)",
      "}\n"
    ]
  },
  {
    "id": "nodejs/node",
    "org": "nodejs",
    "avatarURL": "https://avatars.githubusercontent.com/u/9950313?v=4",
    "name": "nodejs/node",
    "url": "https://github.com/nodejs/node",
    "lang": "JavaScript",
    "desc": "Node.js JavaScript runtime.",
    "star_num": 97512,
    "fork_num": 26805,
    "snippets": [
      "// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nassertThrows(\"$=function anonymous() { /*noex*/do {} while(({ get x(x) { break ; }, set x() { (undefined);} })); }\");\n\nfunction foo() {\n  assertThrows(\"$=function anonymous() { /*noex*/do {} while(({ get x(x) { break ; }, set x() { (undefined);} })); }\");\n}\nfoo();\n\nassertThrows(\"$=function anonymous() { /*noex*/do {} while(({ get x(x) { break ; }, set x() { (undefined);} })); }\");\n\nxeval = function(s) { eval(s); }\nxeval('$=function(){L: {break L;break L;}};');\n",
      "}\n\nfunction SparseSmiCopyWithinSetup() {\n  array = [];\n  for (let i = 0; i < kArraySize; i += 10) array[i] = i;\n}\n\nfunction SparseStringCopyWithinSetup() {\n  array = [];\n  for (let i = 0; i < kArraySize; i += 10) array[i] = `Item no. ${i}`;\n}\n\n})();\n",
      "// Flags: --wasm-debug-mask-for-testing=1\n\nd8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');\n\nconst builder = new WasmModuleBuilder();\nbuilder.addType(makeSig([kWasmI32, kWasmI32, kWasmI32], [kWasmI32]));\nbuilder.addMemory(16, 32, false);\n// Generate function 1 (out of 1).\nbuilder.addFunction(undefined, 0 /* sig */)\n  .addBodyWithEnd([\n// signature: i_iii\n// body:\nkExprI32Const, 0x65,  // i32.const\nkExprI32Const, 0x61,  // i32.const\nkExprI64Const, 0x42,  // i64.const\nkAtomicPrefix, kExprI32AtomicWait, 0x00, 0x0b,  // memory.atomic.wait32\n]);\nbuilder.addExport('main', 0);\nassertThrows(function() { builder.instantiate(); }, WebAssembly.CompileError);\n",
      "// Copyright 2019 the V8 project authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be\n// found in the LICENSE file.\n\nlet {session, contextGroup, Protocol} = InspectorTest.start(\"Tests that Runtime.evaluate has the correct error line number for 'new Function(...)'\");\n\nvar message = { expression: \"new Function('(0)()')();\" };\n\nProtocol.Runtime.evaluate(message)\n  .then(message => InspectorTest.logMessage(message))\n  .then(() => InspectorTest.completeTest());\n",
      "// Copyright 2019 the V8 project authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be\n// found in the LICENSE file.\n\nfunction x() {\n  class Foo {\n    constructor () {\n      class Bar {\n        x = this.#foo;\n      }\n    }\n  }\n}\n",
      "  // Works on a non-object-mode stream\n  (async () => {\n    const firstBuffer = Buffer.from([1, 2, 3]);\n    const secondBuffer = Buffer.from([4, 5, 6]);\n    const stream = Readable.from(\n      [firstBuffer, secondBuffer],\n      { objectMode: false });\n    const result = await stream.toArray();\n    assert.strictEqual(Array.isArray(result), true);\n    assert.deepStrictEqual(result, [firstBuffer, secondBuffer]);\n  })().then(common.mustCall());\n}\n\n{\n  // Works on an asynchronous stream\n  (async () => {\n    const tests = [\n      [],\n      [1],\n      [1, 2, 3],\n      Array(100).fill().map((_, i) => i),\n    ];\n    for (const test of tests) {\n      const stream = Readable.from(test).map((x) => Promise.resolve(x));\n      const result = await stream.toArray();\n      assert.deepStrictEqual(result, test);\n    }\n  })().then(common.mustCall());\n}\n\n{\n  // Support for AbortSignal\n  const ac = new AbortController();\n  let stream;\n  assert.rejects(async () => {\n    stream = Readable.from([1, 2, 3]).map(async (x) => {\n      if (x === 3) {\n        await new Promise(() => {}); // Explicitly do not pass signal here\n      }\n      return Promise.resolve(x);\n    });\n    await stream.toArray({ signal: ac.signal });\n  }, {\n    name: 'AbortError',\n  }).then(common.mustCall(() => {\n    // Only stops toArray, does not destroy the stream\n    assert(stream.destroyed, false);\n  }));\n  ac.abort();\n}\n{\n  // Test result is a Promise\n  const result = Readable.from([1, 2, 3, 4, 5]).toArray();\n  assert.strictEqual(result instanceof Promise, true);\n}\n{\n  // Error cases\n  assert.rejects(async () => {\n    await Readable.from([1]).toArray(1);\n  }, /ERR_INVALID_ARG_TYPE/).then(common.mustCall());\n\n  assert.rejects(async () => {\n    await Readable.from([1]).toArray({\n      signal: true",
      "        errors: [{ message: /looks up the Symbol\\.replace property/ }],\n      },\n      {\n        code: 'StringPrototypeReplaceAll(\"some string\", new RegExp(\"some regex\"), \"some replacement\")',\n        errors: [{ message: /looks up the Symbol\\.replace property/ }],\n      },\n      {\n        code: 'StringPrototypeSearch(\"some string\", /some regex/)',\n        errors: [{ message: /SafeStringPrototypeSearch/ }],\n      },\n      {\n        code: 'StringPrototypeSplit(\"some string\", /some regex/)',\n        errors: [{ message: /looks up the Symbol\\.split property/ }],\n      },\n      {\n        code: 'new Proxy({}, {})',\n        errors: [{ message: /null-prototype/ }]\n      },\n      {\n        code: 'new Proxy({}, { [`__proto__`]: null })',\n        errors: [{ message: /null-prototype/ }]\n      },\n      {\n        code: 'new Proxy({}, { __proto__: Object.prototype })',\n        errors: [{ message: /null-prototype/ }]\n      },\n      {\n        code: 'new Proxy({}, { ...{ __proto__: null } })',\n        errors: [{ message: /null-prototype/ }]\n      },\n      {\n        code: 'PromisePrototypeCatch(promise, ()=>{})',\n        errors: [{ message: /\\bPromisePrototypeThen\\b/ }]\n      },\n      {\n        code: 'PromiseAll([])',\n        errors: [{ message: /\\bSafePromiseAll\\b/ }]\n      },\n      {\n        code: 'async function fn(){await SafePromiseAll([])}',\n        errors: [{ message: /\\bSafePromiseAllReturnVoid\\b/ }]\n      },\n      {\n        code: 'async function fn(){await SafePromiseAllSettled([])}',\n        errors: [{ message: /\\bSafePromiseAllSettledReturnVoid\\b/ }]\n      },\n      {\n        code: 'PromiseAllSettled([])',\n        errors: [{ message: /\\bSafePromiseAllSettled\\b/ }]\n      },\n      {\n        code: 'PromiseAny([])',\n        errors: [{ message: /\\bSafePromiseAny\\b/ }]\n      },\n      {\n        code: 'PromiseRace([])',\n        errors: [{ message: /\\bSafePromiseRace\\b/ }]\n      },\n      {\n        code: 'ArrayPrototypeConcat([])',\n        errors: [{ message: /\\bisConcatSpreadable\\b/ }]\n      },\n    ]\n  });",
      "      if (i === 13 && deopt) {\n        %DeoptimizeNow();\n      }\n      return false;\n    });\n  }\n  %PrepareFunctionForOptimization(lazyDeopt);\n  lazyDeopt();\n  lazyDeopt();\n  %OptimizeFunctionOnNextCall(lazyDeopt);\n  lazyDeopt();\n  lazyDeopt(true);\n  lazyDeopt();\n  assertEquals(1500, result);\n})();\n\n// Lazy deopt from runtime call from non-inline callback function.\n(() => {\n  const a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n             20, 21, 22, 23, 24, 25];\n  let result = 0;\n  function lazyDeopt(deopt) {\n    function callback(v, i) {\n      result += i;\n      if (i === 13 && deopt) {\n        %DeoptimizeNow();\n      }\n      return false;\n    }\n    %NeverOptimizeFunction(callback);\n    a.findIndex(callback);\n  }\n  %PrepareFunctionForOptimization(lazyDeopt);\n  lazyDeopt();\n  lazyDeopt();\n  %OptimizeFunctionOnNextCall(lazyDeopt);\n  lazyDeopt();\n  lazyDeopt(true);\n  lazyDeopt();\n  assertEquals(1500, result);\n})();\n\n// Call to a.findIndex is done inside a try-catch block and the callback function\n// being called actually throws.\n(() => {\n  const a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n             20, 21, 22, 23, 24, 25];\n  let caught = false;\n  function lazyDeopt(deopt) {\n    try {\n      a.findIndex((v, i) => {\n        if (i === 1 && deopt) {\n          throw(\"a\");\n        }\n        return false;\n      });\n    } catch (e) {\n      caught = true;\n    }\n  }\n  %PrepareFunctionForOptimization(lazyDeopt);\n  lazyDeopt();\n  lazyDeopt();\n  %OptimizeFunctionOnNextCall(lazyDeopt);",
      "'use strict';\n\nconst common = require('../common.js');\n\nconst bench = common.createBenchmark(main, {\n  charsPerLine: [76],\n  linesCount: [8 << 16],\n  n: [32],\n});\n\nfunction main({ charsPerLine, linesCount, n }) {\n  const bytesCount = charsPerLine * linesCount / 4 * 3;\n\n  const line = `${'abcd'.repeat(charsPerLine / 4)}\\n`;\n  const data = line.repeat(linesCount);\n  // eslint-disable-next-line node-core/no-unescaped-regexp-dot\n  data.match(/./);  // Flatten the string\n  const buffer = Buffer.alloc(bytesCount, line, 'base64');\n\n  bench.start();\n  for (let i = 0; i < n; i++) {\n    buffer.base64Write(data, 0, bytesCount);\n  }\n  bench.end(n);\n}\n",
      "const assert = require('assert');\nconst http = require('http');\n\nconst server = http.createServer();\nserver.on('request', function(req, res) {\n  assert.strictEqual(req.headers.foo, 'bar');\n  res.end('ok');\n  server.close();\n});\nserver.listen(0, '127.0.0.1', function() {\n  const req = http.request({\n    method: 'GET',\n    host: '127.0.0.1',\n    port: this.address().port,\n  });\n  req.setHeader('foo', 'bar');\n  req.flushHeaders();\n});\n"
    ]
  },
  {
    "id": "electron/electron",
    "org": "electron",
    "avatarURL": "https://avatars.githubusercontent.com/u/13409222?v=4",
    "name": "electron/electron",
    "url": "https://github.com/electron/electron",
    "lang": "C++, JavaScript",
    "desc": "Build cross-platform desktop apps with JavaScript, HTML, and CSS.",
    "star_num": 109033,
    "fork_num": 15032,
    "snippets": [
      "  const containerClient = blobServiceClient.getContainerClient(containerName);\n  const blockBlobClient = containerClient.getBlockBlobClient(blobKey);\n  blockBlobClient.uploadFile(file)\n    .then((uploadBlobResponse) => {\n      console.log(`Upload block blob ${blobKey} successfully: https://artifacts.electronjs.org/${key}`, uploadBlobResponse.requestId);\n    })\n    .catch((err) => {\n      console.error(err);\n      anErrorOccurred = true;\n    })\n    .then(() => next(done));\n}\nnext(() => {\n  process.exit(anErrorOccurred ? 1 : 0);\n});\n",
      "      contextIsolation: false,\n      nodeIntegration: true\n    }\n  });\n  w.webContents.once('render-process-gone', (_, details) => {\n    if (details.reason === 'crashed') {\n      process.exit(0);\n    } else {\n      process.exit(details.exitCode);\n    }\n  });\n  await w.webContents.loadURL('about:blank');\n  w.webContents.executeJavaScript('process.crash()');\n});\n",
      "  })\n})\n\napp.on('window-all-closed', () => {\n  if (process.platform !== 'darwin') {\n    app.quit()\n  }\n})\n",
      "        FROM: 'child'\n      }\n    });\n  } else {\n    child = utilityProcess.fork(path.join(__dirname, 'test.js'));\n  }\n  child.on('message', (data) => {\n    process.stdout.write(data);\n    process.stdout.end();\n  });\n  child.on('exit', () => {\n    app.quit();\n  });\n});\n",
      "  }\n})\n",
      "app.whenReady().then(createWindow)\n\napp.on('window-all-closed', () => {\n  app.clearRecentDocuments()\n  if (process.platform !== 'darwin') {\n    app.quit()\n  }\n})\n\napp.on('activate', () => {\n  if (BrowserWindow.getAllWindows().length === 0) {\n    createWindow()\n  }\n})\n",
      "    }\n  }\n};\n\nchrome.runtime.onMessage.addListener((request, sender, sendResponse) => {\n  handleRequest(request, sender, sendResponse);\n  return true;\n});\n",
      "const asar = require('@electron/asar');\nconst assert = require('node:assert');\nconst fs = require('fs-extra');\nconst os = require('node:os');\nconst path = require('node:path');\n\nconst getArgGroup = (name) => {\n  const group = [];\n  let inGroup = false;\n  for (const arg of process.argv) {\n    // At the next flag we stop being in the current group\n    if (arg.startsWith('--')) inGroup = false;\n    // Push all args in the group\n    if (inGroup) group.push(arg);\n    // If we find the start flag, start pushing\n    if (arg === `--${name}`) inGroup = true;\n  }\n\n  return group;\n};\n\nconst base = getArgGroup('base');\nconst files = getArgGroup('files');\nconst out = getArgGroup('out');\n\nassert(base.length === 1, 'should have a single base dir');\nassert(files.length >= 1, 'should have at least one input file');\nassert(out.length === 1, 'should have a single out path');\n\n// Ensure all files are inside the base dir\nfor (const file of files) {\n  if (!file.startsWith(base[0])) {\n    console.error(`Expected all files to be inside the base dir but \"${file}\" was not in \"${base[0]}\"`);\n    process.exit(1);\n  }\n}\n\nconst tmpPath = fs.mkdtempSync(path.resolve(os.tmpdir(), 'electron-gn-asar-'));\n\ntry {\n  // Copy all files to a tmp dir to avoid including scrap files in the ASAR\n  for (const file of files) {\n    const newLocation = path.resolve(tmpPath, path.relative(base[0], file));\n    fs.mkdirsSync(path.dirname(newLocation));\n    fs.writeFileSync(newLocation, fs.readFileSync(file));\n  }\n} catch (err) {\n  console.error('Unexpected error while generating ASAR', err);\n  fs.remove(tmpPath)\n    .then(() => process.exit(1))\n    .catch(() => process.exit(1));\n  return;\n}\n\n// Create the ASAR archive\nasar.createPackageWithOptions(tmpPath, out[0], {})\n  .catch(err => {\n    const exit = () => {\n      console.error('Unexpected error while generating ASAR', err);\n      process.exit(1);\n    };\n    fs.remove(tmpPath).then(exit).catch(exit);\n  }).then(() => fs.remove(tmpPath));\n",
      "  const releaseNotes = await releaseNotesGenerator(currentBranch, newVersion);\n  if (releaseNotes.warning) {\n    console.warn(releaseNotes.warning);\n  }\n  return releaseNotes;\n}\n\nasync function createRelease (branchToTarget, isBeta) {\n  const newVersion = await getNewVersion();\n  const releaseNotes = await getReleaseNotes(branchToTarget, newVersion);\n  await tagRelease(newVersion);\n\n  console.log('Checking for existing draft release.');\n  const releases = await octokit.repos.listReleases({\n    owner: 'electron',\n    repo: targetRepo\n  }).catch(err => {\n    console.log(`${fail} Could not get releases. Error was: `, err);\n  });\n\n  const drafts = releases.data.filter(release => release.draft &&\n    release.tag_name === newVersion);\n  if (drafts.length > 0) {\n    console.log(`${fail} Aborting because draft release for\n      ${drafts[0].tag_name} already exists.`);\n    process.exit(1);\n  }\n  console.log(`${pass} A draft release does not exist; creating one.`);\n\n  let releaseBody;\n  let releaseIsPrelease = false;\n  if (isBeta) {\n    if (newVersion.indexOf('nightly') > 0) {\n      releaseBody = 'Note: This is a nightly release.  Please file new issues ' +\n        'for any bugs you find in it.\\n \\n This release is published to npm ' +\n        'under the electron-nightly package and can be installed via `npm install electron-nightly`, ' +\n        `or \\`npm install electron-nightly@${newVersion.substr(1)}\\`.\\n \\n ${releaseNotes.text}`;\n    } else if (newVersion.indexOf('alpha') > 0) {\n      releaseBody = 'Note: This is an alpha release.  Please file new issues ' +\n        'for any bugs you find in it.\\n \\n This release is published to npm ' +\n        'under the alpha tag and can be installed via `npm install electron@alpha`, ' +\n        `or \\`npm install electron@${newVersion.substr(1)}\\`.\\n \\n ${releaseNotes.text}`;\n    } else {\n      releaseBody = 'Note: This is a beta release.  Please file new issues ' +\n        'for any bugs you find in it.\\n \\n This release is published to npm ' +\n        'under the beta tag and can be installed via `npm install electron@beta`, ' +\n        `or \\`npm install electron@${newVersion.substr(1)}\\`.\\n \\n ${releaseNotes.text}`;\n    }\n    releaseIsPrelease = true;\n  } else {\n    releaseBody = releaseNotes.text;\n  }\n\n  const release = await octokit.repos.createRelease({\n    owner: 'electron',\n    repo: targetRepo,\n    tag_name: newVersion,\n    draft: true,\n    name: `electron ${newVersion}`,\n    body: releaseBody,\n    prerelease: releaseIsPrelease,\n    target_commitish: newVersion.includes('nightly') ? 'main' : branchToTarget\n  }).catch(err => {\n    console.log(`${fail} Error creating new release: `, err);",
      "  plistContents = plistContents.replace(`$\\{${key}}`, keyPairs[key]);\n}\n\nfs.writeFileSync(outputPath, plistContents);\n"
    ]
  },
  {
    "id": "vuejs/vue",
    "org": "vuejs",
    "avatarURL": "https://avatars.githubusercontent.com/u/6128107?v=4",
    "name": "vuejs/vue",
    "url": "https://github.com/vuejs/vue",
    "lang": "JavaScript",
    "desc": "A progressive, incrementally-adoptable JavaScript framework for building UI on the web.",
    "star_num": 205167,
    "fork_num": 34344,
    "snippets": [
      "const path = require('path')\n\nconst resolve = p => path.resolve(__dirname, '../', p)\n\nmodule.exports = {\n  vue: resolve('src/platforms/web/entry-runtime-with-compiler'),\n  compiler: resolve('src/compiler'),\n  core: resolve('src/core'),\n  shared: resolve('src/shared'),\n  web: resolve('src/platforms/web'),\n  server: resolve('packages/server-renderer/src'),\n  sfc: resolve('packages/compiler-sfc/src')\n}\n",
      "    })\n\n    // simulate router.onReady\n    Foo().then(comp => {\n      // resolve now to make the render sync\n      Foo.resolved = Vue.extend(comp.default)\n      resolve(vm)\n    })\n  })\n}\n",
      "// localStorage persistence\nvar STORAGE_KEY = 'todos-vuejs-2.0'\nvar todoStorage = {\n  fetch: function () {\n    var todos = JSON.parse(localStorage.getItem(STORAGE_KEY) || '[]')\n    todos.forEach(function (todo, index) {\n      todo.id = index\n    })\n    todoStorage.uid = todos.length\n    return todos\n  },\n  save: function (todos) {\n    localStorage.setItem(STORAGE_KEY, JSON.stringify(todos))\n  }\n}\n\n// visibility filters\nvar filters = {\n  all: function (todos) {\n    return todos\n  },\n  active: function (todos) {\n    return todos.filter(function (todo) {\n      return !todo.completed\n    })\n  },\n  completed: function (todos) {\n    return todos.filter(function (todo) {\n      return todo.completed\n    })\n  }\n}\n\n// app Vue instance\nvar app = new Vue({\n  // app initial state\n  data: {\n    todos: todoStorage.fetch(),\n    newTodo: '',\n    editedTodo: null,\n    visibility: 'all'\n  },\n\n  // watch todos change for localStorage persistence\n  watch: {\n    todos: {\n      handler: function (todos) {\n        todoStorage.save(todos)\n      },\n      deep: true\n    }\n  },\n\n  // computed properties\n  // https://v2.vuejs.org/v2/guide/computed.html\n  computed: {\n    filteredTodos: function () {\n      return filters[this.visibility](this.todos)\n    },\n    remaining: function () {\n      return filters.active(this.todos).length\n    },\n    allDone: {\n      get: function () {",
      "const version = process.argv[2] || process.env.VERSION\nconst cc = require('conventional-changelog')\nconst file = `./RELEASE_NOTE${version ? `_${version}` : ``}.md`\nconst fileStream = require('fs').createWriteStream(file)\n\ncc({\n  preset: 'angular',\n  pkg: {\n    transform (pkg) {\n      pkg.version = `v${version}`\n      return pkg\n    }\n  }\n}).pipe(fileStream).on('close', () => {\n  console.log(`Generated release note at ${file}`)\n})\n",
      "import Vue from '../../../../dist/vue.runtime.common.js'\n\nconst app = {\n  name: 'app',\n  props: ['id'],\n  serverCacheKey: props => (props.id === 1 ? false : props.id),\n  render(h) {\n    return h('div', '/test')\n  }\n}\n\nexport default () => {\n  return Promise.resolve(\n    new Vue({\n      render: h => h(app, { props: { id: 1 } })\n    })\n  )\n}\n",
      "  }\n\n  function cleanQuery(value) {\n    if (value) {\n      value.formatElapsed = \"\";\n      value.elapsedClassName = \"\";\n      value.query = \"\";\n      value.elapsed = null;\n      value.waiting = null;\n    } else {\n      return {\n        query: \"***\",\n        formatElapsed: \"\",\n        elapsedClassName: \"\"\n      };\n    }\n  }\n\n  function generateRow(object, keepIdentity, counter) {\n    var nbQueries = Math.floor((Math.random() * 10) + 1);\n    if (!object) {\n      object = {};\n    }\n    object.lastMutationId = counter;\n    object.nbQueries = nbQueries;\n    if (!object.lastSample) {\n      object.lastSample = {};\n    }\n    if (!object.lastSample.topFiveQueries) {\n      object.lastSample.topFiveQueries = [];\n    }\n    if (keepIdentity) {\n      // for Angular optimization\n      if (!object.lastSample.queries) {\n        object.lastSample.queries = [];\n        for (var l = 0; l < 12; l++) {\n          object.lastSample.queries[l] = cleanQuery();\n        }\n      }\n      for (var j in object.lastSample.queries) {\n        var value = object.lastSample.queries[j];\n        if (j <= nbQueries) {\n          updateQuery(value);\n        } else {\n          cleanQuery(value);\n        }\n      }\n    } else {\n      object.lastSample.queries = [];\n      for (var j = 0; j < 12; j++) {\n        if (j < nbQueries) {\n          var value = updateQuery(cleanQuery());\n          object.lastSample.queries.push(value);\n        } else {\n          object.lastSample.queries.push(cleanQuery());\n        }\n      }\n    }\n    for (var i = 0; i < 5; i++) {\n      var source = object.lastSample.queries[i];\n      object.lastSample.topFiveQueries[i] = source;\n    }\n    object.lastSample.nbQueries = nbQueries;\n    object.lastSample.countClassName = countClassName(nbQueries);",
      "    console.log(`(skipped)`)\n  }\n\n  // generate changelog\n  step('\\nGenerating changelog...')\n  await run(`pnpm`, ['run', 'changelog'])\n\n  // update pnpm-lock.yaml\n  step('\\nUpdating lockfile...')\n  await run(`pnpm`, ['install', '--prefer-offline'])\n\n  const { stdout } = await run('git', ['diff'], { stdio: 'pipe' })\n  if (stdout) {\n    step('\\nCommitting changes...')\n    await runIfNotDry('git', ['add', '-A'])\n    await runIfNotDry('git', ['commit', '-m', `release: v${targetVersion}`])\n  } else {\n    console.log('No changes to commit.')\n  }\n\n  // publish packages\n  step('\\nPublishing packages...')\n  for (const pkg of packages) {\n    await publishPackage(pkg, targetVersion, runIfNotDry)\n  }\n\n  // push to GitHub\n  step('\\nPushing to GitHub...')\n  await runIfNotDry('git', ['tag', `v${targetVersion}`])\n  await runIfNotDry('git', ['push', 'origin', `refs/tags/v${targetVersion}`])\n  await runIfNotDry('git', ['push'])\n\n  if (isDryRun) {\n    console.log(`\\nDry run finished - run git diff to see package changes.`)\n  }\n  console.log()\n}\n\nfunction updatePackage(pkgRoot, version) {\n  const pkgPath = path.resolve(pkgRoot, 'package.json')\n  const pkg = JSON.parse(fs.readFileSync(pkgPath, 'utf-8'))\n  pkg.version = version\n  fs.writeFileSync(pkgPath, JSON.stringify(pkg, null, 2) + '\\n')\n}\n\nconst getPkgRoot = pkg =>\n  pkg === 'vue'\n    ? path.resolve(__dirname, '../')\n    : path.resolve(__dirname, '../packages/' + pkg)\n\nasync function publishPackage(pkgName, version, runIfNotDry) {\n  const pkgRoot = getPkgRoot(pkgName)\n  const pkgPath = path.resolve(pkgRoot, 'package.json')\n  const pkg = JSON.parse(fs.readFileSync(pkgPath, 'utf-8'))\n  const publishedName = pkg.name\n  if (pkg.private) {\n    return\n  }\n\n  let releaseTag = null\n  if (args.tag) {\n    releaseTag = args.tag\n  } else if (version.includes('alpha')) {\n    releaseTag = 'alpha'",
      "      pkg.version = `v${version}`\n      return pkg\n    }\n  }\n}).pipe(fileStream).on('close', () => {\n  console.log(`Generated release note at ${file}`)\n})\n",
      "'use strict'\n\nprocess.env.NODE_ENV = 'production'\n\nconst Vue = require('../../dist/vue.runtime.common.js')\nconst createRenderer = require('../../packages/server-renderer').createRenderer\nconst renderToString = createRenderer().renderToString\nconst gridComponent = require('./common.js')\n\nconsole.log('--- renderToString --- ')\nconst self = (global || root)\nself.s = self.performance.now()\n\nrenderToString(new Vue(gridComponent), (err, res) => {\n  if (err) throw err\n  // console.log(res)\n  console.log('Complete time: ' + (self.performance.now() - self.s).toFixed(2) + 'ms')\n  console.log()\n})\n"
    ]
  },
  {
    "id": "atom/atom",
    "org": "atom",
    "avatarURL": "https://avatars.githubusercontent.com/u/1089146?v=4",
    "name": "atom/atom",
    "url": "https://github.com/atom/atom",
    "lang": "JavaScript",
    "desc": "The hackable text editor built on Electron.",
    "star_num": 59582,
    "fork_num": 18356,
    "snippets": [
      "  // Private: Insert a new {RegistryWatcherNode} into the tree, creating new intermediate {RegistryNode} instances as\n  // needed. Any existing children of the watched directory are removed.\n  //\n  // * `pathSegments` filesystem path of the new {Watcher}, already split into an Array of directory names.\n  // * `leaf` initialized {RegistryWatcherNode} to insert\n  //\n  // Returns: The root of a new tree with the {RegistryWatcherNode} inserted at the correct location. Callers should\n  // replace their node references with the returned value.\n  insert(pathSegments, leaf) {\n    if (pathSegments.length === 0) {\n      return leaf;\n    }\n\n    const pathKey = pathSegments[0];\n    let child = this.children[pathKey];\n    if (child === undefined) {\n      child = new RegistryNode();\n    }\n    this.children[pathKey] = child.insert(pathSegments.slice(1), leaf);\n    return this;\n  }\n\n  // Private: Remove a {RegistryWatcherNode} by its exact watched directory.\n  //\n  // * `pathSegments` absolute pre-split filesystem path of the node to remove.\n  // * `createSplitNative` callback to be invoked with each child path segment {Array} if the {RegistryWatcherNode}\n  //   is split into child watchers rather than removed outright. See {RegistryWatcherNode.remove}.\n  //\n  // Returns: The root of a new tree with the {RegistryWatcherNode} removed. Callers should replace their node\n  // references with the returned value.\n  remove(pathSegments, createSplitNative) {\n    if (pathSegments.length === 0) {\n      // Attempt to remove a path with child watchers. Do nothing.\n      return this;\n    }\n\n    const pathKey = pathSegments[0];\n    const child = this.children[pathKey];\n    if (child === undefined) {\n      // Attempt to remove a path that isn't watched. Do nothing.\n      return this;\n    }\n\n    // Recurse\n    const newChild = child.remove(pathSegments.slice(1), createSplitNative);\n    if (newChild === null) {\n      delete this.children[pathKey];\n    } else {\n      this.children[pathKey] = newChild;\n    }\n\n    // Remove this node if all of its children have been removed\n    return Object.keys(this.children).length === 0 ? null : this;\n  }\n\n  // Private: Discover all {RegistryWatcherNode} instances beneath this tree node and the child paths\n  //  that they are watching.\n  //\n  // * `prefix` {Array} of intermediate path segments to prepend to the resulting child paths.\n  //\n  // Returns: A possibly empty {Array} of `{node, path}` objects describing {RegistryWatcherNode}\n  //  instances beneath this node.\n  leaves(prefix) {\n    const results = [];",
      "        return new FakeRemoteDirectory(uri)\n      }\n    }\n  }\n}\n",
      "      }\n    );\n  });\n};\n",
      "    return this.emitter.on('should-detach', callback);\n  }\n\n  // Private: Register a callback to be invoked when a {NativeWatcher} is about to be stopped.\n  //\n  // Returns: A {Disposable} to revoke the subscription.\n  onWillStop(callback) {\n    return this.emitter.on('will-stop', callback);\n  }\n\n  // Private: Register a callback to be invoked when the filesystem watcher has been stopped.\n  //\n  // Returns: A {Disposable} to revoke the subscription.\n  onDidStop(callback) {\n    return this.emitter.on('did-stop', callback);\n  }\n\n  // Private: Register a callback to be invoked with any errors reported from the watcher.\n  //\n  // Returns: A {Disposable} to revoke the subscription.\n  onDidError(callback) {\n    return this.emitter.on('did-error', callback);\n  }\n\n  // Private: Broadcast an `onShouldDetach` event to prompt any {Watcher} instances bound here to attach to a new\n  // {NativeWatcher} instead.\n  //\n  // * `replacement` the new {NativeWatcher} instance that a live {Watcher} instance should reattach to instead.\n  // * `watchedPath` absolute path watched by the new {NativeWatcher}.\n  reattachTo(replacement, watchedPath, options) {\n    this.emitter.emit('should-detach', { replacement, watchedPath, options });\n  }\n\n  // Private: Stop the native watcher and release any operating system resources associated with it.\n  //\n  // Has no effect if the watcher is not running.\n  async stop() {\n    if (this.state !== WATCHER_STATE.RUNNING) {\n      return;\n    }\n    this.state = WATCHER_STATE.STOPPING;\n    this.emitter.emit('will-stop');\n\n    await this.doStop();\n\n    this.state = WATCHER_STATE.STOPPED;\n\n    this.emitter.emit('did-stop');\n  }\n\n  doStop() {\n    return Promise.resolve();\n  }\n\n  // Private: Detach any event subscribers.\n  dispose() {\n    this.emitter.dispose();\n  }\n\n  // Private: Callback function invoked by the native watcher when a debounced group of filesystem events arrive.\n  // Normalize and re-broadcast them to any subscribers.\n  //\n  // * `events` An Array of filesystem events.\n  onEvents(events) {",
      "    decorationsForMarker.add(decoration);\n    if (decoration.isType('overlay')) this.overlayDecorations.add(decoration);\n    this.observeDecoratedLayer(marker.layer, true);\n    this.editor.didAddDecoration(decoration);\n    this.emitDidUpdateDecorations();\n    this.emitter.emit('did-add-decoration', decoration);\n    return decoration;\n  }\n\n  decorateMarkerLayer(markerLayer, decorationParams) {\n    if (markerLayer.isDestroyed()) {\n      throw new Error('Cannot decorate a destroyed marker layer');\n    }\n    markerLayer = this.displayLayer.getMarkerLayer(markerLayer.id);\n    const decoration = new LayerDecoration(markerLayer, this, decorationParams);\n    let layerDecorations = this.layerDecorationsByMarkerLayer.get(markerLayer);\n    if (layerDecorations == null) {\n      layerDecorations = new Set();\n      this.layerDecorationsByMarkerLayer.set(markerLayer, layerDecorations);\n    }\n    layerDecorations.add(decoration);\n    this.observeDecoratedLayer(markerLayer, false);\n    this.emitDidUpdateDecorations();\n    return decoration;\n  }\n\n  emitDidUpdateDecorations() {\n    this.editor.scheduleComponentUpdate();\n    this.emitter.emit('did-update-decorations');\n  }\n\n  decorationDidChangeType(decoration) {\n    if (decoration.isType('overlay')) {\n      this.overlayDecorations.add(decoration);\n    } else {\n      this.overlayDecorations.delete(decoration);\n    }\n  }\n\n  didDestroyMarkerDecoration(decoration) {\n    const { marker } = decoration;\n    const decorations = this.decorationsByMarker.get(marker);\n    if (decorations && decorations.has(decoration)) {\n      decorations.delete(decoration);\n      if (decorations.size === 0) this.decorationsByMarker.delete(marker);\n      this.overlayDecorations.delete(decoration);\n      this.unobserveDecoratedLayer(marker.layer, true);\n      this.emitter.emit('did-remove-decoration', decoration);\n      this.emitDidUpdateDecorations();\n    }\n  }\n\n  didDestroyLayerDecoration(decoration) {\n    const { markerLayer } = decoration;\n    const decorations = this.layerDecorationsByMarkerLayer.get(markerLayer);\n\n    if (decorations && decorations.has(decoration)) {\n      decorations.delete(decoration);\n      if (decorations.size === 0) {\n        this.layerDecorationsByMarkerLayer.delete(markerLayer);\n      }\n      this.unobserveDecoratedLayer(markerLayer, true);\n      this.emitDidUpdateDecorations();\n    }",
      "\nfunction transpilePegJsPath(pegJsPath) {\n  const inputCode = fs.readFileSync(pegJsPath, 'utf8');\n  const jsPath = pegJsPath.replace(/pegjs$/g, 'js');\n  const outputCode =\n    'module.exports = ' + peg.buildParser(inputCode, { output: 'source' });\n  fs.writeFileSync(jsPath, outputCode);\n  fs.unlinkSync(pegJsPath);\n}\n",
      "      },\n      customFileTypes: {\n        type: 'object',\n        default: {},\n        description:\n          'Associates scope names (e.g. `\"source.js\"`) with arrays of file extensions and file names (e.g. `[\"Somefile\", \".js2\"]`)',\n        additionalProperties: {\n          type: 'array',\n          items: {\n            type: 'string'\n          }\n        }\n      },\n      uriHandlerRegistration: {\n        type: 'string',\n        default: 'prompt',\n        description:\n          'When should Atom register itself as the default handler for atom:// URIs',\n        enum: [\n          {\n            value: 'prompt',\n            description:\n              'Prompt to register Atom as the default atom:// URI handler'\n          },\n          {\n            value: 'always',\n            description:\n              'Always become the default atom:// URI handler automatically'\n          },\n          {\n            value: 'never',\n            description: 'Never become the default atom:// URI handler'\n          }\n        ]\n      },\n      themes: {\n        type: 'array',\n        default: ['one-dark-ui', 'one-dark-syntax'],\n        items: {\n          type: 'string'\n        },\n        description:\n          'Names of UI and syntax themes which will be used when Atom starts.'\n      },\n      audioBeep: {\n        type: 'boolean',\n        default: true,\n        description:\n          \"Trigger the system's beep sound when certain actions cannot be executed or there are no results.\"\n      },\n      closeDeletedFileTabs: {\n        type: 'boolean',\n        default: false,\n        title: 'Close Deleted File Tabs',\n        description:\n          'Close corresponding editors when a file is deleted outside Atom.'\n      },\n      destroyEmptyPanes: {\n        type: 'boolean',\n        default: true,\n        title: 'Remove Empty Panes',\n        description:\n          'When the last tab of a pane is closed, remove that pane as well.'\n      },",
      "  }\n\n  addTranspilerConfigForPath(packagePath, packageName, packageMeta, config) {\n    this.configByPackagePath[packagePath] = {\n      name: packageName,\n      meta: packageMeta,\n      path: packagePath,\n      specs: config.map(spec => Object.assign({}, spec))\n    };\n  }\n\n  removeTranspilerConfigForPath(packagePath) {\n    delete this.configByPackagePath[packagePath];\n    const packagePathWithSep = packagePath.endsWith(path.sep)\n      ? path.join(packagePath)\n      : path.join(packagePath) + path.sep;\n    Object.keys(this.specByFilePath).forEach(filePath => {\n      if (path.join(filePath).startsWith(packagePathWithSep)) {\n        delete this.specByFilePath[filePath];\n      }\n    });\n  }\n\n  // Wraps the transpiler in an object with the same interface\n  // that falls back to the original transpiler implementation if and\n  // only if a package hasn't registered its desire to transpile its own source.\n  wrapTranspiler(transpiler) {\n    return {\n      getCachePath: (sourceCode, filePath) => {\n        const spec = this.getPackageTranspilerSpecForFilePath(filePath);\n        if (spec) {\n          return this.getCachePath(sourceCode, filePath, spec);\n        }\n\n        return transpiler.getCachePath(sourceCode, filePath);\n      },\n\n      compile: (sourceCode, filePath) => {\n        const spec = this.getPackageTranspilerSpecForFilePath(filePath);\n        if (spec) {\n          return this.transpileWithPackageTranspiler(\n            sourceCode,\n            filePath,\n            spec\n          );\n        }\n\n        return transpiler.compile(sourceCode, filePath);\n      },\n\n      shouldCompile: (sourceCode, filePath) => {\n        if (this.transpilerPaths[filePath]) {\n          return false;\n        }\n        const spec = this.getPackageTranspilerSpecForFilePath(filePath);\n        if (spec) {\n          return true;\n        }\n\n        return transpiler.shouldCompile(sourceCode, filePath);\n      }\n    };\n  }\n",
      "  beforeEach(() => {\n    const directory = atom.project.getDirectories()[0];\n    const paths = directory ? [directory.resolve('dir')] : [null];\n    atom.project.setPaths(paths);\n\n    // Wait for project's service consumers to be asynchronously added\n    waits(1);\n  });\n\n  describe('serialization', () => {\n    let deserializedProject = null;\n    let notQuittingProject = null;\n    let quittingProject = null;\n\n    afterEach(() => {\n      if (deserializedProject != null) {\n        deserializedProject.destroy();\n      }\n      if (notQuittingProject != null) {\n        notQuittingProject.destroy();\n      }\n      if (quittingProject != null) {\n        quittingProject.destroy();\n      }\n    });\n\n    it(\"does not deserialize paths to directories that don't exist\", () => {\n      deserializedProject = new Project({\n        notificationManager: atom.notifications,\n        packageManager: atom.packages,\n        confirm: atom.confirm,\n        grammarRegistry: atom.grammars\n      });\n      const state = atom.project.serialize();\n      state.paths.push('/directory/that/does/not/exist');\n\n      let err = null;\n      waitsForPromise(() =>\n        deserializedProject.deserialize(state, atom.deserializers).catch(e => {\n          err = e;\n        })\n      );\n\n      runs(() => {\n        expect(deserializedProject.getPaths()).toEqual(atom.project.getPaths());\n        expect(err.missingProjectPaths).toEqual([\n          '/directory/that/does/not/exist'\n        ]);\n      });\n    });\n\n    it('does not deserialize paths that are now files', () => {\n      const childPath = path.join(temp.mkdirSync('atom-spec-project'), 'child');\n      fs.mkdirSync(childPath);\n\n      deserializedProject = new Project({\n        notificationManager: atom.notifications,\n        packageManager: atom.packages,\n        confirm: atom.confirm,\n        grammarRegistry: atom.grammars\n      });\n      atom.project.setPaths([childPath]);\n      const state = atom.project.serialize();\n"
    ]
  },
  {
    "id": "golang/go",
    "org": "golang",
    "avatarURL": "https://avatars.githubusercontent.com/u/4314092?v=4",
    "name": "golang/go",
    "url": "https://github.com/golang/go",
    "lang": "Go",
    "desc": "The Go programming language.",
    "star_num": 114131,
    "fork_num": 17097,
    "snippets": [
      "// compile\n\n// Copyright 2020 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\n// Gccgo mishandles a couple of alias cases.\n\npackage p\n\ntype S struct{}\n\nfunc (*S) M() {}\n\ntype I interface {\n\tM()\n}\n\ntype A = *S\n\nvar V1 I\nvar _ = V1.(*S)\nvar _ = V1.(A)\n\nfunc F() {\n\tvar v I\n\tv = (*S)(nil)\n\tv = A(nil)\n\t_ = v\n}\n",
      "// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\n// TODO(rsc): Delete this file once Go 1.17 comes out and we can retire Go 1.15 support.\n\n//go:build !go1.16\n// +build !go1.16\n\n// Package buildtag defines an Analyzer that checks build tags.\npackage buildtag\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"go/ast\"\n\t\"go/parser\"\n\t\"strings\"\n\t\"unicode\"\n\n\t\"golang.org/x/tools/go/analysis\"\n\t\"golang.org/x/tools/go/analysis/passes/internal/analysisutil\"\n)\n\nconst Doc = \"check // +build directives\"\n\nvar Analyzer = &analysis.Analyzer{\n\tName: \"buildtag\",\n\tDoc:  Doc,\n\tRun:  runBuildTag,\n}\n\nfunc runBuildTag(pass *analysis.Pass) (interface{}, error) {\n\tfor _, f := range pass.Files {\n\t\tcheckGoFile(pass, f)\n\t}\n\tfor _, name := range pass.OtherFiles {\n\t\tif err := checkOtherFile(pass, name); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tfor _, name := range pass.IgnoredFiles {\n\t\tif strings.HasSuffix(name, \".go\") {\n\t\t\tf, err := parser.ParseFile(pass.Fset, name, nil, parser.ParseComments)\n\t\t\tif err != nil {\n\t\t\t\t// Not valid Go source code - not our job to diagnose, so ignore.\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\tcheckGoFile(pass, f)\n\t\t} else {\n\t\t\tif err := checkOtherFile(pass, name); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\treturn nil, nil\n}\n\nfunc checkGoFile(pass *analysis.Pass, f *ast.File) {\n\tpastCutoff := false\n\tfor _, group := range f.Comments {\n\t\t// A +build comment is ignored after or adjoining the package declaration.\n\t\tif group.End()+1 >= f.Package {\n\t\t\tpastCutoff = true",
      "func main() {\n\tvar a, b, c, d uint8 = 1, 1, 1, 1\n\tx := F(a, b, c, d)\n\tif x != 1 {\n\t\tprintln(x)\n\t\tpanic(\"x != 1\")\n\t}\n}\n",
      "import . \"os\"\n\nfunc f(e *os.File)\n\nfunc main() {\n\tvar _e_ *_os_.File\n\tvar dot *File\n\n\tf(_e_)\n\tf(dot)\n}\n",
      "\tcallee(p0exp)\n}\n",
      "\tdata, err := syscall.Mmap(int(f.Fd()), 0, mmapLength, syscall.PROT_READ, syscall.MAP_SHARED)\n\tif err != nil {\n\t\treturn Data{}, &fs.PathError{Op: \"mmap\", Path: f.Name(), Err: err}\n\t}\n\treturn Data{f, data[:n]}, nil\n}\n",
      "\t\tsync <- struct{}{}\n\t}()\n\t<-sync\n\n\twantLabels = map[string]string{\"key\": \"value\"}\n\tctx := WithLabels(context.Background(), Labels(\"key\", \"value\"))\n\tSetGoroutineLabels(ctx)\n\tif gotLabels := getProfLabel(); !reflect.DeepEqual(gotLabels, wantLabels) {\n\t\tt.Errorf(\"parent goroutine's profile labels: got %v, want %v\", gotLabels, wantLabels)\n\t}\n\tgo func() {\n\t\tif gotLabels := getProfLabel(); !reflect.DeepEqual(gotLabels, wantLabels) {\n\t\t\tt.Errorf(\"child goroutine's profile labels: got %v, want %v\", gotLabels, wantLabels)\n\t\t}\n\t\tsync <- struct{}{}\n\t}()\n\t<-sync\n\n\twantLabels = map[string]string{}\n\tctx = context.Background()\n\tSetGoroutineLabels(ctx)\n\tif gotLabels := getProfLabel(); !reflect.DeepEqual(gotLabels, wantLabels) {\n\t\tt.Errorf(\"Expected parent goroutine's profile labels to be empty, got %v\", gotLabels)\n\t}\n\tgo func() {\n\t\tif gotLabels := getProfLabel(); !reflect.DeepEqual(gotLabels, wantLabels) {\n\t\t\tt.Errorf(\"Expected child goroutine's profile labels to be empty, got %v\", gotLabels)\n\t\t}\n\t\tsync <- struct{}{}\n\t}()\n\t<-sync\n}\n\nfunc TestDo(t *testing.T) {\n\twantLabels := map[string]string{}\n\tif gotLabels := getProfLabel(); !reflect.DeepEqual(gotLabels, wantLabels) {\n\t\tt.Errorf(\"Expected parent goroutine's profile labels to be empty before Do, got %v\", gotLabels)\n\t}\n\n\tDo(context.Background(), Labels(\"key1\", \"value1\", \"key2\", \"value2\"), func(ctx context.Context) {\n\t\twantLabels := map[string]string{\"key1\": \"value1\", \"key2\": \"value2\"}\n\t\tif gotLabels := getProfLabel(); !reflect.DeepEqual(gotLabels, wantLabels) {\n\t\t\tt.Errorf(\"parent goroutine's profile labels: got %v, want %v\", gotLabels, wantLabels)\n\t\t}\n\n\t\tsync := make(chan struct{})\n\t\tgo func() {\n\t\t\twantLabels := map[string]string{\"key1\": \"value1\", \"key2\": \"value2\"}\n\t\t\tif gotLabels := getProfLabel(); !reflect.DeepEqual(gotLabels, wantLabels) {\n\t\t\t\tt.Errorf(\"child goroutine's profile labels: got %v, want %v\", gotLabels, wantLabels)\n\t\t\t}\n\t\t\tsync <- struct{}{}\n\t\t}()\n\t\t<-sync\n\n\t})\n\n\twantLabels = map[string]string{}\n\tif gotLabels := getProfLabel(); !reflect.DeepEqual(gotLabels, wantLabels) {\n\t\tfmt.Printf(\"%#v\", gotLabels)\n\t\tfmt.Printf(\"%#v\", wantLabels)\n\t\tt.Errorf(\"Expected parent goroutine's profile labels to be empty after Do, got %v\", gotLabels)\n\t}\n}",
      "// Copyright 2019 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\n//go:build windows\n\npackage modload\n\nimport \"io/fs\"\n\n// hasWritePerm reports whether the current user has permission to write to the\n// file with the given info.\nfunc hasWritePerm(_ string, fi fs.FileInfo) bool {\n\t// Windows has a read-only attribute independent of ACLs, so use that to\n\t// determine whether the file is intended to be overwritten.\n\t//\n\t// Per https://golang.org/pkg/os/#Chmod:\n\t// “On Windows, only the 0200 bit (owner writable) of mode is used; it\n\t// controls whether the file's read-only attribute is set or cleared.”\n\treturn fi.Mode()&0200 != 0\n}\n",
      "\n\trecv2 := make(chan<- int)\n\ta2 := _Append([]chan<- int{recv2}, recv2)\n\tif len(a2) != 2 || a2[0] != recv2 || a2[1] != recv2 {\n\t\tpanic(a)\n\t}\n}\n",
      "\n\thttp.Redirect(w, req, u.String(), http.StatusFound)\n}\n"
    ]
  },
  {
    "id": "facebook/jest",
    "org": "facebook",
    "avatarURL": "https://avatars.githubusercontent.com/u/103283236?v=4",
    "name": "facebook/jest",
    "url": "https://github.com/jestjs/jest",
    "lang": "JavaScript",
    "desc": "Delightful JavaScript testing.",
    "star_num": 42635,
    "fork_num": 6463,
    "snippets": [
      "/**\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n *\n */\n\n'use strict';\n\nexports.isGlobalImageStub = true;\n",
      "/**\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n\nit('should add two numbers', () => {\n  expect(1 + 1).toBe(2);\n});\n",
      "});\n",
      "  expect(true).toBe(true);\n\n  setImmediate(() => {\n    throw new Error('Scheduled Error');\n  });\n});\n",
      "  });\n\n  it.only.failing.each([\n    {a: 1, b: 1, expected: 2},\n    {a: 1, b: 2, expected: 3},\n    {a: 2, b: 1, expected: 3},\n  ])('.add($a, $b)', ({a, b, expected}) => {\n    expect(a + b).toBe(expected);\n  });\n\n  it('failing test but skipped', () => {\n    expect(10).toBe(101);\n  });\n\n  it('passing test but skipped', () => {\n    expect(10).toBe(10);\n  });\n});\n\ndescribe('block with only, should fail', () => {\n  it.only.failing('failing passes = fails, should fail', () => {\n    expect(10).toBe(10);\n  });\n\n  it('failing test but skipped', () => {\n    expect(10).toBe(101);\n  });\n\n  it('passing test but skipped', () => {\n    expect(10).toBe(10);\n  });\n});\n\ndescribe('block with only in other it, should skip', () => {\n  it.failing('failing passes = fails, should fail but skipped', () => {\n    expect(10).toBe(10);\n  });\n\n  it.only('failing test', () => {\n    expect(10).toBe(101);\n  });\n\n  it('passing test but skipped', () => {\n    expect(10).toBe(10);\n  });\n});\n\ndescribe('block with only with different syntax, should fail', () => {\n  fit.failing('failing passes = fails, should fail 1', () => {\n    expect(10).toBe(10);\n  });\n\n  test.only.failing('failing passes = fails, should fail 2', () => {\n    expect(10).toBe(10);\n  });\n\n  it('failing test but skipped', () => {\n    expect(10).toBe(101);\n  });\n\n  it('passing test but skipped', () => {\n    expect(10).toBe(10);\n  });\n});",
      "    await createTestScheduler(\n      makeGlobalConfig({\n        reporters: [['summary', {}]],\n      }),\n      {},\n      {},\n    );\n\n    expect(DefaultReporter).toHaveBeenCalledTimes(0);\n    expect(VerboseReporter).toHaveBeenCalledTimes(0);\n    expect(GitHubActionsReporter).toHaveBeenCalledTimes(0);\n    expect(NotifyReporter).toHaveBeenCalledTimes(0);\n    expect(CoverageReporter).toHaveBeenCalledTimes(0);\n    expect(SummaryReporter).toHaveBeenCalledTimes(1);\n  });\n\n  test('sets up custom reporter', async () => {\n    await createTestScheduler(\n      makeGlobalConfig({\n        reporters: [\n          ['default', {}],\n          ['/custom-reporter.js', {}],\n        ],\n      }),\n      {},\n      {},\n    );\n\n    expect(DefaultReporter).toHaveBeenCalledTimes(1);\n    expect(VerboseReporter).toHaveBeenCalledTimes(0);\n    expect(GitHubActionsReporter).toHaveBeenCalledTimes(0);\n    expect(NotifyReporter).toHaveBeenCalledTimes(0);\n    expect(CoverageReporter).toHaveBeenCalledTimes(0);\n    expect(SummaryReporter).toHaveBeenCalledTimes(1);\n    expect(CustomReporter).toHaveBeenCalledTimes(1);\n  });\n});\n\ntest('.addReporter() .removeReporter()', async () => {\n  const scheduler = await createTestScheduler(makeGlobalConfig(), {}, {});\n  const reporter = new SummaryReporter();\n  scheduler.addReporter(reporter);\n  expect(scheduler._dispatcher._reporters).toContain(reporter);\n  scheduler.removeReporter(SummaryReporter);\n  expect(scheduler._dispatcher._reporters).not.toContain(reporter);\n});\n\ndescribe('scheduleTests should always dispatch runStart and runComplete events', () => {\n  const mockReporter = {\n    onRunComplete: jest.fn(),\n    onRunStart: jest.fn(),\n  };\n\n  const errorMsg = 'runtime-error';\n  let scheduler, t;\n\n  beforeEach(async () => {\n    mockReporter.onRunStart.mockClear();\n    mockReporter.onRunComplete.mockClear();\n\n    t = {\n      context: {\n        config: makeProjectConfig({\n          moduleFileExtensions: ['.js'],",
      "/**\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n\nconst {value} = require('../module');\n\ntest('dummy', () => {\n  expect(value).toBe('abc');\n});\n",
      "\nlet cacheKey;\n\nexport function getCacheKey() {\n  return cacheKey;\n}\n\nexport function setCacheKey(key) {\n  cacheKey = key;\n}\n",
      "/**\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n\n'use strict';\n\n/**\n * TestReporter\n * Reporter for testing the outputs, without any extra\n * hassle. Uses a JSON like syntax for testing the reporters\n * instead of outputting the text to stdout and using match functions\n * to get the output.\n */\nclass TestReporter {\n  constructor(globalConfig, reporterOptions, reporterContext) {\n    this._context = reporterContext;\n    this._options = reporterOptions;\n\n    /**\n     * statsCollected property\n     * contains most of the statistics\n     * related to the object to be called,\n     * This here helps us in avoiding the string match\n     * statements nothing else\n     */\n    this._statsCollected = {\n      onRunComplete: {},\n      onRunStart: {},\n      onTestResult: {times: 0},\n      onTestStart: {},\n      reporterContext,\n      reporterOptions,\n    };\n  }\n\n  /**\n   * clearLine\n   * clears the line for easier JSON parsing\n   */\n  clearLine() {\n    if (process.stdout.isTTY) {\n      process.stderr.write('\\x1b[999D\\x1b[K');\n    }\n  }\n\n  onTestStart(path) {\n    const onTestStart = this._statsCollected.onTestStart;\n\n    onTestStart.called = true;\n    onTestStart.path = typeof path === 'string';\n  }\n\n  onTestResult(test, testResult, results) {\n    const onTestResult = this._statsCollected.onTestResult;\n\n    onTestResult.called = true;\n    onTestResult.times++;\n  }\n\n  onRunStart(results, options) {\n    this.clearLine();",
      "/**\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n\n'use strict';\n\nthrow new Error('throwing');\n"
    ]
  },
  {
    "id": "reduxjs/redux",
    "org": "reduxjs",
    "avatarURL": "https://avatars.githubusercontent.com/u/13142323?v=4",
    "name": "reduxjs/redux",
    "url": "https://github.com/reduxjs/redux",
    "lang": "JavaScript",
    "desc": "Predictable state container for JavaScript apps.",
    "star_num": 59882,
    "fork_num": 15444,
    "snippets": [
      "              title: 'Product 1',\n              inventory: 2\n            },\n            {\n              id: 2,\n              title: 'Product 2',\n              inventory: 1\n            }\n          ]\n        })\n      })\n\n      it('contains the products from the action', () => {\n        expect(products.getProduct(state, 1)).toEqual({\n          id: 1,\n          title: 'Product 1',\n            inventory: 2\n        })\n        expect(products.getProduct(state, 2)).toEqual({\n          id: 2,\n          title: 'Product 2',\n            inventory: 1\n        })\n      })\n\n      it ('contains no other products', () => {\n        expect(products.getProduct(state, 3)).toEqual(undefined)\n      })\n\n      it('lists all of the products as visible', () => {\n        expect(products.getVisibleProducts(state)).toEqual([\n          {\n            id: 1,\n            title: 'Product 1',\n            inventory: 2\n          }, {\n            id: 2,\n            title: 'Product 2',\n            inventory: 1\n          }\n        ])\n      })\n\n      describe('when an item is added to the cart', () => {\n\n        beforeEach(() => {\n          state = reducer(state, { type: 'ADD_TO_CART', productId: 1 })\n        })\n\n        it('the inventory is reduced', () => {\n          expect(products.getVisibleProducts(state)).toEqual([\n            {\n              id: 1,\n              title: 'Product 1',\n              inventory: 1\n            }, {\n              id: 2,\n              title: 'Product 2',\n              inventory: 1\n            }\n          ])\n        })\n\n      })",
      "import React from 'react'\nimport { render } from 'react-dom'\nimport { createStore, applyMiddleware } from 'redux'\nimport { Provider } from 'react-redux'\nimport thunk from 'redux-thunk'\nimport { createLogger } from 'redux-logger'\nimport reducer from './reducers'\nimport App from './containers/App'\n\nconst middleware = [ thunk ]\nif (process.env.NODE_ENV !== 'production') {\n  middleware.push(createLogger())\n}\n\nconst store = createStore(\n  reducer,\n  applyMiddleware(...middleware)\n)\n\nrender(\n  <Provider store={store}>\n    <App />\n  </Provider>,\n  document.getElementById('root')\n)\n",
      "\nconst mapStateToProps = state => ({\n  filteredTodos: getVisibleTodos(state)\n})\n\nconst mapDispatchToProps = dispatch => ({\n  actions: bindActionCreators(TodoActions, dispatch)\n})\n\n\nconst VisibleTodoList = connect(\n  mapStateToProps,\n  mapDispatchToProps\n)(TodoList)\n\nexport default VisibleTodoList\n",
      "  id: nextTodoId++,\n  text\n})\n\nexport const setVisibilityFilter = (filter) => ({\n  type: 'SET_VISIBILITY_FILTER',\n  filter\n})\n\nexport const toggleTodo = (id) => ({\n  type: 'TOGGLE_TODO',\n  id\n})\n",
      "export default Picker\n",
      "    })\n  })\n})\n",
      "  const output = renderer.getRenderOutput()\n  return output\n}\n\ndescribe('components', () => {\n  describe('Header', () => {\n    it('should render', () => {\n      const output = setup()\n      const [header] = output.props.children\n      expect(header.type).toBe(Header)\n    })\n  })\n\n  describe('Mainsection', () => {\n    it('should render', () => {\n      const output = setup()\n      const [, mainSection] = output.props.children\n      expect(mainSection.type).toBe(MainSection)\n    })\n  })\n})\n",
      "      expect(props.addTodo).not.toBeCalled()\n      input.props.onSave('Use Redux')\n      expect(props.addTodo).toBeCalled()\n    })\n  })\n})\n"
    ]
  },
  {
    "id": "moby/moby",
    "org": "moby",
    "avatarURL": "https://avatars.githubusercontent.com/u/27259197?v=4",
    "name": "moby/moby",
    "url": "https://github.com/moby/moby",
    "lang": "Go",
    "desc": "Moby Project - a collaborative project for the container ecosystem to assemble container-based systems.",
    "star_num": 66621,
    "fork_num": 18836,
    "snippets": [
      ")\n\nfunc recvmmsg(s uintptr, hs []mmsghdr, flags int) (int, error) {\n\tn, _, errno := syscall.Syscall6(sysRECVMMSG, s, uintptr(unsafe.Pointer(&hs[0])), uintptr(len(hs)), uintptr(flags), 0, 0)\n\treturn int(n), errnoErr(errno)\n}\n\nfunc sendmmsg(s uintptr, hs []mmsghdr, flags int) (int, error) {\n\tn, _, errno := syscall.Syscall6(sysSENDMMSG, s, uintptr(unsafe.Pointer(&hs[0])), uintptr(len(hs)), uintptr(flags), 0, 0)\n\treturn int(n), errnoErr(errno)\n}\n",
      "func rwFloat64Bytes(w jsWriter, msg []byte, scratch []byte) ([]byte, []byte, error) {\n\tvar f float64\n\tvar err error\n\tf, msg, err = ReadFloat64Bytes(msg)\n\tif err != nil {\n\t\treturn msg, scratch, err\n\t}\n\tscratch = strconv.AppendFloat(scratch[:0], f, 'f', -1, 64)\n\t_, err = w.Write(scratch)\n\treturn msg, scratch, err\n}\n\nfunc rwTimeBytes(w jsWriter, msg []byte, scratch []byte) ([]byte, []byte, error) {\n\tvar t time.Time\n\tvar err error\n\tt, msg, err = ReadTimeBytes(msg)\n\tif err != nil {\n\t\treturn msg, scratch, err\n\t}\n\tbts, err := t.MarshalJSON()\n\tif err != nil {\n\t\treturn msg, scratch, err\n\t}\n\t_, err = w.Write(bts)\n\treturn msg, scratch, err\n}\n\nfunc rwExtensionBytes(w jsWriter, msg []byte, scratch []byte) ([]byte, []byte, error) {\n\tvar err error\n\tvar et int8\n\tet, err = peekExtension(msg)\n\tif err != nil {\n\t\treturn msg, scratch, err\n\t}\n\n\t// if it's time.Time\n\tif et == TimeExtension {\n\t\tvar tm time.Time\n\t\ttm, msg, err = ReadTimeBytes(msg)\n\t\tif err != nil {\n\t\t\treturn msg, scratch, err\n\t\t}\n\t\tbts, err := tm.MarshalJSON()\n\t\tif err != nil {\n\t\t\treturn msg, scratch, err\n\t\t}\n\t\t_, err = w.Write(bts)\n\t\treturn msg, scratch, err\n\t}\n\n\t// if the extension is registered,\n\t// use its canonical JSON form\n\tif f, ok := extensionReg[et]; ok {\n\t\te := f()\n\t\tmsg, err = ReadExtensionBytes(msg, e)\n\t\tif err != nil {\n\t\t\treturn msg, scratch, err\n\t\t}\n\t\tbts, err := json.Marshal(e)\n\t\tif err != nil {\n\t\t\treturn msg, scratch, err\n\t\t}\n\t\t_, err = w.Write(bts)\n\t\treturn msg, scratch, err",
      "\t}\n\n\tsource, target := u.Hostname(), strings.TrimPrefix(u.Path, \"/\")\n\trepoLabel, ok := info.Labels[\"containerd.io/distribution.source.\"+source]\n\tif !ok || repoLabel == \"\" {\n\t\treturn false, nil\n\t}\n\n\tfor _, repo := range strings.Split(repoLabel, \",\") {\n\t\t// the target repo is not a candidate\n\t\tif repo == target {\n\t\t\treturn true, nil\n\t\t}\n\t}\n\treturn false, nil\n}\n",
      "package registry // import \"github.com/docker/docker/registry\"\n\nimport (\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n)\n\n// defaultCertsDir is the platform-specific default directory where certificates\n// are stored. On Linux, it may be overridden through certsDir, for example, when\n// running in rootless mode.\nvar defaultCertsDir = os.Getenv(\"programdata\") + `\\docker\\certs.d`\n\n// cleanPath is used to ensure that a directory name is valid on the target\n// platform. It will be passed in something *similar* to a URL such as\n// https:\\index.docker.io\\v1. Not all platforms support directory names\n// which contain those characters (such as : on Windows)\nfunc cleanPath(s string) string {\n\treturn filepath.FromSlash(strings.ReplaceAll(s, \":\", \"\"))\n}\n",
      "\t_, _, e1 := RawSyscall(SYS_GETSOCKNAME, uintptr(fd), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)))\n\tif e1 != 0 {\n\t\terr = errnoErr(e1)\n\t}\n\treturn\n}\n\n// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT\n\nfunc recvfrom(fd int, p []byte, flags int, from *RawSockaddrAny, fromlen *_Socklen) (n int, err error) {\n\tvar _p0 unsafe.Pointer\n\tif len(p) > 0 {\n\t\t_p0 = unsafe.Pointer(&p[0])\n\t} else {\n\t\t_p0 = unsafe.Pointer(&_zero)\n\t}\n\tr0, _, e1 := Syscall6(SYS_RECVFROM, uintptr(fd), uintptr(_p0), uintptr(len(p)), uintptr(flags), uintptr(unsafe.Pointer(from)), uintptr(unsafe.Pointer(fromlen)))\n\tn = int(r0)\n\tif e1 != 0 {\n\t\terr = errnoErr(e1)\n\t}\n\treturn\n}\n\n// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT\n\nfunc sendto(s int, buf []byte, flags int, to unsafe.Pointer, addrlen _Socklen) (err error) {\n\tvar _p0 unsafe.Pointer\n\tif len(buf) > 0 {\n\t\t_p0 = unsafe.Pointer(&buf[0])\n\t} else {\n\t\t_p0 = unsafe.Pointer(&_zero)\n\t}\n\t_, _, e1 := Syscall6(SYS_SENDTO, uintptr(s), uintptr(_p0), uintptr(len(buf)), uintptr(flags), uintptr(to), uintptr(addrlen))\n\tif e1 != 0 {\n\t\terr = errnoErr(e1)\n\t}\n\treturn\n}\n\n// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT\n\nfunc recvmsg(s int, msg *Msghdr, flags int) (n int, err error) {\n\tr0, _, e1 := Syscall(SYS_RECVMSG, uintptr(s), uintptr(unsafe.Pointer(msg)), uintptr(flags))\n\tn = int(r0)\n\tif e1 != 0 {\n\t\terr = errnoErr(e1)\n\t}\n\treturn\n}\n\n// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT\n\nfunc sendmsg(s int, msg *Msghdr, flags int) (n int, err error) {\n\tr0, _, e1 := Syscall(SYS_SENDMSG, uintptr(s), uintptr(unsafe.Pointer(msg)), uintptr(flags))\n\tn = int(r0)\n\tif e1 != 0 {\n\t\terr = errnoErr(e1)\n\t}\n\treturn\n}\n\n// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT\n",
      "\t\t}\n\n\t\tif forwardedOrg != org {\n\t\t\treturn \"\", status.Errorf(codes.PermissionDenied, \"Permission denied: organization mismatch, expecting: %s\", org)\n\t\t}\n\n\t\treturn forwardedID, nil\n\t}\n\n\t// There wasn't any node being forwarded, check if this is a direct call by the expected role\n\tnodeID, err := AuthorizeOrgAndRole(ctx, org, blacklistedCerts, authorizedRoles...)\n\tif err == nil {\n\t\treturn nodeID, nil\n\t}\n\n\treturn \"\", status.Errorf(codes.PermissionDenied, \"Permission denied: unauthorized peer role: %v\", err)\n}\n\n// intersectArrays returns true when there is at least one element in common\n// between the two arrays\nfunc intersectArrays(orig, tgt []string) bool {\n\tfor _, i := range orig {\n\t\tfor _, x := range tgt {\n\t\t\tif i == x {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\n// RemoteNodeInfo describes a node sending an RPC request.\ntype RemoteNodeInfo struct {\n\t// Roles is a list of roles contained in the node's certificate\n\t// (or forwarded by a trusted node).\n\tRoles []string\n\n\t// Organization is the organization contained in the node's certificate\n\t// (or forwarded by a trusted node).\n\tOrganization string\n\n\t// NodeID is the node's ID, from the CN field in its certificate\n\t// (or forwarded by a trusted node).\n\tNodeID string\n\n\t// ForwardedBy contains information for the node that forwarded this\n\t// request. It is set to nil if the request was received directly.\n\tForwardedBy *RemoteNodeInfo\n\n\t// RemoteAddr is the address that this node is connecting to the cluster\n\t// from.\n\tRemoteAddr string\n}\n\n// RemoteNode returns the node ID and role from the client's TLS certificate.\n// If the RPC was forwarded, the original client's ID and role is returned, as\n// well as the forwarder's ID. This function does not do authorization checks -\n// it only looks up the node ID.\nfunc RemoteNode(ctx context.Context) (RemoteNodeInfo, error) {\n\t// If we have a value on the context that marks this as a local\n\t// request, we return the node info from the context.\n\tlocalNodeInfo := ctx.Value(LocalRequestKey)\n\n\tif localNodeInfo != nil {",
      "//go:build !linux\n\npackage system // import \"github.com/docker/docker/pkg/system\"\n\n// Lgetxattr is not supported on platforms other than linux.\nfunc Lgetxattr(path string, attr string) ([]byte, error) {\n\treturn nil, ErrNotSupportedPlatform\n}\n\n// Lsetxattr is not supported on platforms other than linux.\nfunc Lsetxattr(path string, attr string, data []byte, flags int) error {\n\treturn ErrNotSupportedPlatform\n}\n",
      "//\t    }\n//\t}\npackage sdk\n",
      "\tm.active[p] = b\n\treturn b, nil\n}\n\nfunc (m *mmapper) Munmap(data []byte) (err error) {\n\tif len(data) == 0 || len(data) != cap(data) {\n\t\treturn EINVAL\n\t}\n\n\t// Find the base of the mapping.\n\tp := &data[cap(data)-1]\n\tm.Lock()\n\tdefer m.Unlock()\n\tb := m.active[p]\n\tif b == nil || &b[0] != &data[0] {\n\t\treturn EINVAL\n\t}\n\n\t// Unmap the memory and update m.\n\tif errno := m.munmap(uintptr(unsafe.Pointer(&b[0])), uintptr(len(b))); errno != nil {\n\t\treturn errno\n\t}\n\tdelete(m.active, p)\n\treturn nil\n}\n\nfunc Read(fd int, p []byte) (n int, err error) {\n\tn, err = read(fd, p)\n\tif raceenabled {\n\t\tif n > 0 {\n\t\t\traceWriteRange(unsafe.Pointer(&p[0]), n)\n\t\t}\n\t\tif err == nil {\n\t\t\traceAcquire(unsafe.Pointer(&ioSync))\n\t\t}\n\t}\n\treturn\n}\n\nfunc Write(fd int, p []byte) (n int, err error) {\n\tif raceenabled {\n\t\traceReleaseMerge(unsafe.Pointer(&ioSync))\n\t}\n\tn, err = write(fd, p)\n\tif raceenabled && n > 0 {\n\t\traceReadRange(unsafe.Pointer(&p[0]), n)\n\t}\n\treturn\n}\n\nfunc Pread(fd int, p []byte, offset int64) (n int, err error) {\n\tn, err = pread(fd, p, offset)\n\tif raceenabled {\n\t\tif n > 0 {\n\t\t\traceWriteRange(unsafe.Pointer(&p[0]), n)\n\t\t}\n\t\tif err == nil {\n\t\t\traceAcquire(unsafe.Pointer(&ioSync))\n\t\t}\n\t}\n\treturn\n}\n\nfunc Pwrite(fd int, p []byte, offset int64) (n int, err error) {",
      "\n// optionFunc wraps a func so it satisfies the SamplerOption interface.\ntype optionFunc func(*sampler)\n\nfunc (f optionFunc) apply(s *sampler) {\n\tf(s)\n}\n\n// SamplerOption configures a Sampler.\ntype SamplerOption interface {\n\tapply(*sampler)\n}\n\n// nopSamplingHook is the default hook used by sampler.\nfunc nopSamplingHook(Entry, SamplingDecision) {}\n\n// SamplerHook registers a function  which will be called when Sampler makes a\n// decision.\n//\n// This hook may be used to get visibility into the performance of the sampler.\n// For example, use it to track metrics of dropped versus sampled logs.\n//\n//  var dropped atomic.Int64\n//  zapcore.SamplerHook(func(ent zapcore.Entry, dec zapcore.SamplingDecision) {\n//    if dec&zapcore.LogDropped > 0 {\n//      dropped.Inc()\n//    }\n//  })\nfunc SamplerHook(hook func(entry Entry, dec SamplingDecision)) SamplerOption {\n\treturn optionFunc(func(s *sampler) {\n\t\ts.hook = hook\n\t})\n}\n\n// NewSamplerWithOptions creates a Core that samples incoming entries, which\n// caps the CPU and I/O load of logging while attempting to preserve a\n// representative subset of your logs.\n//\n// Zap samples by logging the first N entries with a given level and message\n// each tick. If more Entries with the same level and message are seen during\n// the same interval, every Mth message is logged and the rest are dropped.\n//\n// For example,\n//\n//   core = NewSamplerWithOptions(core, time.Second, 10, 5)\n//\n// This will log the first 10 log entries with the same level and message\n// in a one second interval as-is. Following that, it will allow through\n// every 5th log entry with the same level and message in that interval.\n//\n// If thereafter is zero, the Core will drop all log entries after the first N\n// in that interval.\n//\n// Sampler can be configured to report sampling decisions with the SamplerHook\n// option.\n//\n// Keep in mind that Zap's sampling implementation is optimized for speed over\n// absolute precision; under load, each tick may be slightly over- or\n// under-sampled.\nfunc NewSamplerWithOptions(core Core, tick time.Duration, first, thereafter int, opts ...SamplerOption) Core {\n\ts := &sampler{\n\t\tCore:       core,\n\t\ttick:       tick,\n\t\tcounts:     newCounters(),"
    ]
  },
  {
    "id": "apple/swift",
    "org": "apple",
    "avatarURL": "https://avatars.githubusercontent.com/u/10639145?v=4",
    "name": "apple/swift",
    "url": "https://github.com/apple/swift",
    "lang": "Swift",
    "desc": "The Swift programming language.",
    "star_num": 63966,
    "fork_num": 10278,
    "snippets": [
      "// UNSUPPORTED: OS=windows-msvc\n// RUN: %empty-directory(%t)\n// RUN: mkdir -p %t/clang-module-cache\n// RUN: mkdir -p %t/inputs\n// RUN: echo \"public func anotherFuncA() {}\" > %t/A.swift\n// RUN: %target-swift-frontend -emit-module -emit-module-path %t/inputs/A.swiftmodule -emit-module-doc-path %t/inputs/A.swiftdoc -emit-module-source-info -emit-module-source-info-path %t/inputs/A.swiftsourceinfo -import-underlying-module -I%S/Inputs/CHeaders -module-cache-path %t.module-cache %t/A.swift -module-name A\n// RUN: %target-swift-emit-pcm -module-name A -o %t/inputs/A.pcm %S/Inputs/CHeaders/module.modulemap\n// RUN: %target-swift-emit-pcm -module-name SwiftShims %swift-lib-dir/swift/shims/module.modulemap -o %t/inputs/SwiftShims.pcm\n// RUN: %target-swift-emit-pcm -module-name _SwiftConcurrencyShims %swift-lib-dir/swift/shims/module.modulemap -o %t/inputs/_SwiftConcurrencyShims.pcm\n\n// RUN: echo \"[{\" > %/t/inputs/map.json\n// RUN: echo \"\\\"moduleName\\\": \\\"A\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"modulePath\\\": \\\"%/t/inputs/A.swiftmodule\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"docPath\\\": \\\"%/t/inputs/A.swiftdoc\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"sourceInfoPath\\\": \\\"%/t/inputs/A.swiftsourceinfo\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"isFramework\\\": false,\" >> %/t/inputs/map.json\n// RUN: echo \"},\" >> %/t/inputs/map.json\n// RUN: echo \"{\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"moduleName\\\": \\\"A\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"clangModulePath\\\": \\\"%/t/inputs/A.pcm\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"clangModuleMapPath\\\": \\\"%/S/Inputs/CHeaders/module.modulemap\\\"\" >> %/t/inputs/map.json\n// RUN: echo \"},\" >> %/t/inputs/map.json\n// RUN: echo \"{\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"moduleName\\\": \\\"Swift\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"modulePath\\\": \\\"%/stdlib_module\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"isFramework\\\": false\" >> %/t/inputs/map.json\n// RUN: echo \"},\" >> %/t/inputs/map.json\n// RUN: echo \"{\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"moduleName\\\": \\\"SwiftOnoneSupport\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"modulePath\\\": \\\"%/ononesupport_module\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"isFramework\\\": false\" >> %/t/inputs/map.json\n// RUN: echo \"},\" >> %/t/inputs/map.json\n// RUN: echo \"{\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"moduleName\\\": \\\"_Concurrency\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"modulePath\\\": \\\"%/concurrency_module\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"isFramework\\\": false\" >> %/t/inputs/map.json\n// RUN: echo \"},\" >> %/t/inputs/map.json\n// RUN: echo \"{\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"moduleName\\\": \\\"SwiftShims\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"isFramework\\\": false,\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"clangModuleMapPath\\\": \\\"%swift-lib-dir/swift/shims/module.modulemap\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"clangModulePath\\\": \\\"%t/inputs/SwiftShims.pcm\\\"\" >> %/t/inputs/map.json\n// RUN: echo \"},\" >> %/t/inputs/map.json\n// RUN: echo \"{\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"moduleName\\\": \\\"_SwiftConcurrencyShims\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"isFramework\\\": false,\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"clangModuleMapPath\\\": \\\"%swift-lib-dir/swift/shims/module.modulemap\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"clangModulePath\\\": \\\"%t/inputs/_SwiftConcurrencyShims.pcm\\\"\" >> %/t/inputs/map.json\n// RUN: echo \"},\" >> %/t/inputs/map.json\n// RUN: echo \"{\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"moduleName\\\": \\\"_StringProcessing\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"modulePath\\\": \\\"%/string_processing_module\\\",\" >> %/t/inputs/map.json\n// RUN: echo \"\\\"isFramework\\\": false\" >> %/t/inputs/map.json\n// RUN: echo \"}]\" >> %/t/inputs/map.json\n\n// RUN: %target-swift-frontend -emit-module -emit-module-path %t/Foo.swiftmodule -disable-implicit-swift-modules -module-cache-path %t.module-cache -explicit-swift-module-map-file %t/inputs/map.json -Rmodule-loading -Xcc -Rmodule-import %s 2>&1 | %FileCheck %s\n\n// CHECK: <unknown>:0: remark: importing module 'A' from {{.*}}{{/|\\\\}}explicit-module-map-clang-and-swift.swift.tmp{{/|\\\\}}inputs{{/|\\\\}}A.pcm'\n\nimport A\n\nfunc callA() {\n  funcA()\n  anotherFuncA()",
      "import Utils\n\n// Test accessing public and package decls\npublic func test() {\n  let x = PublicKlass()\n  x.publicFunc()\n  x.pkgFunc() // OK\n  x.publicGetPkg = 3 // OK\n  x.publicGetInternal = 4 // expected-error {{cannot assign to property: 'publicGetInternal' setter is inaccessible}}\n\n  let y = PackageKlass() // OK\n  y.pkgVar = 1.5  // expected-error {{cannot assign to property: 'pkgVar' setter is inaccessible}}\n  y.pkgFunc() // OK\n}\n\n// Test conformance to a package protocol\npackage struct LibStruct : PackageProto { // OK\n  package var pkgVar: Double = 1.0\n  package func pkgFunc() {}\n}\n\n// Test subclassing / overrides\nclass SubOpenKlass: OpenKlass {\n  override open func openFunc() {}\n  override public func publicFunc() {} // expected-error {{overriding non-open instance method outside of its defining module}}\n  override package func packageFunc() {} // expected-error {{overriding non-open instance method outside of its defining module}}\n}\nclass SubPublicKlass: PublicKlass {} // expected-error {{cannot inherit from non-open class 'PublicKlass' outside of its defining module}}\nclass SubPackageKlass: PackageKlass {} // expected-error {{cannot inherit from non-open class 'PackageKlass' outside of its defining module}}\n\n\n//--- LibOtherPackage.swift\nimport Utils\n\n// Test accessing package decls\npublic func test() {\n  let x = PublicKlass()\n  x.publicFunc() // OK\n  x.pkgFunc() // expected-error {{'pkgFunc' is inaccessible due to 'package' protection level}}\n  let y = PackageKlass() // expected-error {{cannot find 'PackageKlass' in scope}}\n}\n\npackage struct LibStruct : PackageProto {} // expected-error {{cannot find type 'PackageProto' in scope}}\n\n//--- LibGood.swift\nimport Utils\n\npublic func libFunc() {\n  _ = LibStruct()\n}\n\npublic struct LibStruct: PackageProto {\n  public init() {}\n  package var pkgVar: Double = 1.0\n  package func pkgFunc() {}\n  public var publicVar: String = \"\"\n  public func publicFunc() {}\n}\n\n//--- Client.swift\nimport LibGood\n\nfunc clientFunc() {\n  let lib = LibStruct()",
      "  associatedtype Element\n  associatedtype RawIterator: UnsafeCxxInputIterator\n    where RawIterator.Pointee == Element\n\n  /// Do not implement this function manually in Swift.\n  func __beginUnsafe() -> RawIterator\n\n  /// Do not implement this function manually in Swift.\n  func __endUnsafe() -> RawIterator\n}\n\nextension CxxConvertibleToCollection {\n  @inlinable\n  public func forEach(_ body: (RawIterator.Pointee) throws -> Void) rethrows {\n    var rawIterator = __beginUnsafe()\n    let endIterator = __endUnsafe()\n    while rawIterator != endIterator {\n      try body(rawIterator.pointee)\n      rawIterator = rawIterator.successor()\n    }\n  }\n}\n\n// Break the ambiguity between Sequence.forEach and CxxConvertibleToCollection.forEach.\nextension CxxConvertibleToCollection where Self: Sequence {\n  @inlinable\n  public func forEach(_ body: (Element) throws -> Void) rethrows {\n    for element in self {\n      try body(element)\n    }\n  }\n}\n\nextension RangeReplaceableCollection {\n  /// Creates a collection containing the elements of a C++ container.\n  ///\n  /// This initializes the collection by copying every element of the C++\n  /// container.\n  ///\n  /// - Complexity: O(*n*), where *n* is the number of elements in the C++\n  ///   container when each element is copied in O(1). Note that this might not\n  ///   be true for certain C++ types, e.g. those with a custom copy\n  ///   constructor that performs additional logic.\n  @inlinable\n  public init<C: CxxConvertibleToCollection>(_ elements: __shared C)\n    where C.RawIterator.Pointee == Element {\n\n    self.init()\n    elements.forEach { self.append($0) }\n  }\n}\n\nextension SetAlgebra {\n  /// Creates a set containing the elements of a C++ container.\n  ///\n  /// This initializes the set by copying every element of the C++ container.\n  ///\n  /// - Complexity: O(*n*), where *n* is the number of elements in the C++\n  ///   container when each element is copied in O(1). Note that this might not\n  ///   be true for certain C++ types, e.g. those with a custom copy\n  ///   constructor that performs additional logic.\n  @inlinable\n  public init<C: CxxConvertibleToCollection>(_ elements: __shared C)\n    where C.RawIterator.Pointee == Element {",
      "      LifetimeTracked(element.value, identity: element.identity)\n    },\n    extractValue: { (element: LifetimeTracked) in\n      OpaqueValue(element.value, identity: element.identity)\n    },\n    makeCollectionOfEquatable: { (elements: [MinimalEquatableValue]) in\n      // FIXME: use LifetimeTracked.\n      return MinimalCollection(elements: elements)\n    },\n    wrapValueIntoEquatable: identityEq,\n    extractValueFromEquatable: identityEq,\n    resiliencyChecks: resiliencyChecks\n  )\n}\n\nrunAllTests()\n",
      "// This source file is part of the Swift.org open source project\n// Copyright (c) 2014 - 2017 Apple Inc. and the Swift project authors\n// Licensed under Apache License v2.0 with Runtime Library Exception\n//\n// See https://swift.org/LICENSE.txt for license information\n// See https://swift.org/CONTRIBUTORS.txt for the list of Swift project authors\n\n// RUN: not %target-swift-frontend %s -typecheck\nclass d<T where g:d{\nstruct B<d{{\n}class B{\nlet e=Swift.e\nlet c:d\n",
      "// This source file is part of the Swift.org open source project\n// Copyright (c) 2014 - 2017 Apple Inc. and the Swift project authors\n// Licensed under Apache License v2.0 with Runtime Library Exception\n//\n// See https://swift.org/LICENSE.txt for license information\n// See https://swift.org/CONTRIBUTORS.txt for the list of Swift project authors\n\n// RUN: not %target-swift-frontend %s -typecheck\nextension(Int[a,Int\n",
      "\nprotocol ClassProtoBase: AnyObject {\n  var baseProp: Int { get set }\n}\nprotocol ClassProto: ClassProtoBase {\n  var prop: Int { get set }\n}\nfunc getClassExistential() -> ClassProto? {\n  fatalError()\n}\n",
      "// REQUIRES: OS=macosx\n// RUN: %target-swift-frontend -typecheck %s -import-objc-header %S/Inputs/frameworks/SPIContainer.framework/Headers/SPIContainer.h -verify -library-level api\n\n\n@_spi(a) public let a: SPIInterface1\n@_spi(a) public let b: SPIInterface2\n\npublic let c: SPIInterface1 // expected-error{{cannot use class 'SPIInterface1' here; it is an SPI imported from '__ObjC'}}\npublic let d: SPIInterface2 // expected-error{{cannot use class 'SPIInterface2' here; it is an SPI imported from '__ObjC'}}\n\n@inlinable\npublic func inlinableUsingSPI() { // expected-warning{{public declarations should have an availability attribute with an introduction version}}\n  SharedInterface.foo() // expected-error{{class method 'foo()' cannot be used in an '@inlinable' function because it is an SPI imported from '__ObjC'}}\n}\n",
      "  @usableFromInline\n  let base: Base\n\n  @usableFromInline\n  let transform: (Base.Element) async -> Transformed\n\n  @usableFromInline\n  init(\n    _ base: Base, \n    transform: @escaping (Base.Element) async -> Transformed\n  ) {\n    self.base = base\n    self.transform = transform\n  }\n}\n\n@available(SwiftStdlib 5.1, *)\nextension AsyncMapSequence: AsyncSequence {\n  /// The type of element produced by this asynchronous sequence.\n  ///\n  /// The map sequence produces whatever type of element its transforming\n  /// closure produces.\n  public typealias Element = Transformed\n  /// The type of iterator that produces elements of the sequence.\n  public typealias AsyncIterator = Iterator\n\n  /// The iterator that produces elements of the map sequence.\n  public struct Iterator: AsyncIteratorProtocol {\n    @usableFromInline\n    var baseIterator: Base.AsyncIterator\n\n    @usableFromInline\n    let transform: (Base.Element) async -> Transformed\n\n    @usableFromInline\n    init(\n      _ baseIterator: Base.AsyncIterator, \n      transform: @escaping (Base.Element) async -> Transformed\n    ) {\n      self.baseIterator = baseIterator\n      self.transform = transform\n    }\n\n    /// Produces the next element in the map sequence.\n    ///\n    /// This iterator calls `next()` on its base iterator; if this call returns\n    /// `nil`, `next()` returns `nil`. Otherwise, `next()` returns the result of\n    /// calling the transforming closure on the received element.\n    @inlinable\n    public mutating func next() async rethrows -> Transformed? {\n      guard let element = try await baseIterator.next() else {\n        return nil\n      }\n      return await transform(element)\n    }\n  }\n\n  @inlinable\n  public __consuming func makeAsyncIterator() -> Iterator {\n    return Iterator(base.makeAsyncIterator(), transform: transform)\n  }\n}\n\n@available(SwiftStdlib 5.1, *)",
      "\n// RUN: %empty-directory(%t)\n// RUN: %target-build-swift -O -module-name=test %s -o %t/O.out\n// RUN: %target-codesign %t/O.out\n// RUN: %target-run %t/O.out\n// RUN: %target-build-swift -Onone -module-name=test %s -o %t/Onone.out\n// RUN: %target-codesign %t/Onone.out\n// RUN: %target-run %t/Onone.out\n\n// REQUIRES: executable_test\n\n// Freestanding/minimal runtime does not support printing type names at runtime.\n// UNSUPPORTED: freestanding\n\nimport StdlibUnittest\n\nlet TypeNameTests = TestSuite(\"TypeName\")\n\nclass C {}\nstruct S {}\nenum E {}\n\n// Test unicode type names.\nstruct 🙂 { }\nstruct 🙃 { }\n\nprotocol P {}\nprotocol P2 {}\nprotocol P3 {}\nprotocol AssociatedTypes {\n  associatedtype A\n  associatedtype B\n  associatedtype C\n}\n\nclass Model : AssociatedTypes {\n  typealias A = C\n  typealias B = S\n  typealias C = E\n}\n\nstruct Model2 : AssociatedTypes {\n  typealias A = C\n  typealias B = S\n  typealias C = E\n}\n\nclass GC<T : AssociatedTypes> {}\nstruct GS<T : AssociatedTypes> {}\nenum GE<T : AssociatedTypes> {}\nclass GC2<T : AssociatedTypes, U : AssociatedTypes> {}\n\nclass SomeOuterClass {\n  struct SomeInnerStruct {}\n  struct SomeInnerGenericStruct<T> {}\n}\n\nclass SomeOuterGenericClass<T> {\n  struct SomeInnerStruct {}\n  struct SomeInnerGenericStruct<U> {}\n}\n\nextension SomeOuterGenericClass {\n  struct OtherInnerStruct {}"
    ]
  },
  {
    "id": "openssl/openssl",
    "org": "openssl",
    "avatarURL": "https://avatars.githubusercontent.com/u/3279138?v=4",
    "name": "openssl/openssl",
    "url": "https://github.com/openssl/openssl",
    "lang": "C",
    "desc": "Toolkit for the Transport Layer Security (TLS) and Secure Sockets Layer (SSL) protocols.",
    "star_num": 22590,
    "fork_num": 9972,
    "snippets": [
      "}\n\n/*\n * Add a STACK_OF extensions to a certificate request: allow alternative OIDs\n * in case we want to create a non standard one.\n */\nint X509_REQ_add_extensions_nid(X509_REQ *req,\n                                const STACK_OF(X509_EXTENSION) *exts, int nid)\n{\n    int extlen;\n    int rv = 0;\n    unsigned char *ext = NULL;\n\n    /* Generate encoding of extensions */\n    extlen = ASN1_item_i2d((const ASN1_VALUE *)exts, &ext,\n                           ASN1_ITEM_rptr(X509_EXTENSIONS));\n    if (extlen <= 0)\n        return 0;\n    rv = X509_REQ_add1_attr_by_NID(req, nid, V_ASN1_SEQUENCE, ext, extlen);\n    OPENSSL_free(ext);\n    return rv;\n}\n\n/* This is the normal usage: use the \"official\" OID */\nint X509_REQ_add_extensions(X509_REQ *req, const STACK_OF(X509_EXTENSION) *exts)\n{\n    return X509_REQ_add_extensions_nid(req, exts, NID_ext_req);\n}\n\n/* Request attribute functions */\n\nint X509_REQ_get_attr_count(const X509_REQ *req)\n{\n    return X509at_get_attr_count(req->req_info.attributes);\n}\n\nint X509_REQ_get_attr_by_NID(const X509_REQ *req, int nid, int lastpos)\n{\n    return X509at_get_attr_by_NID(req->req_info.attributes, nid, lastpos);\n}\n\nint X509_REQ_get_attr_by_OBJ(const X509_REQ *req, const ASN1_OBJECT *obj,\n                             int lastpos)\n{\n    return X509at_get_attr_by_OBJ(req->req_info.attributes, obj, lastpos);\n}\n\nX509_ATTRIBUTE *X509_REQ_get_attr(const X509_REQ *req, int loc)\n{\n    return X509at_get_attr(req->req_info.attributes, loc);\n}\n\nX509_ATTRIBUTE *X509_REQ_delete_attr(X509_REQ *req, int loc)\n{\n    X509_ATTRIBUTE *attr;\n\n    if (req == NULL) {\n        ERR_raise(ERR_LIB_X509, ERR_R_PASSED_NULL_PARAMETER);\n        return 0;\n    }\n    attr = X509at_delete_attr(req->req_info.attributes, loc);\n    if (attr != NULL)\n        req->req_info.enc.modified = 1;\n    return attr;",
      "{\n    int bits;\n    int num = 0;\n    int ext = 0;\n    long l;\n\n    bits = BN_num_bits(a);\n    num = (bits + 7) / 8;\n    if (bits > 0) {\n        ext = ((bits & 0x07) == 0);\n    }\n    if (d == NULL)\n        return (num + 4 + ext);\n\n    l = num + ext;\n    d[0] = (unsigned char)(l >> 24) & 0xff;\n    d[1] = (unsigned char)(l >> 16) & 0xff;\n    d[2] = (unsigned char)(l >> 8) & 0xff;\n    d[3] = (unsigned char)(l) & 0xff;\n    if (ext)\n        d[4] = 0;\n    num = BN_bn2bin(a, &(d[4 + ext]));\n    if (a->neg)\n        d[4] |= 0x80;\n    return (num + 4 + ext);\n}\n\nBIGNUM *BN_mpi2bn(const unsigned char *d, int n, BIGNUM *ain)\n{\n    long len;\n    int neg = 0;\n    BIGNUM *a = NULL;\n\n    if (n < 4 || (d[0] & 0x80) != 0) {\n        ERR_raise(ERR_LIB_BN, BN_R_INVALID_LENGTH);\n        return NULL;\n    }\n    len = ((long)d[0] << 24) | ((long)d[1] << 16) | ((int)d[2] << 8) | (int)\n        d[3];\n    if ((len + 4) != n) {\n        ERR_raise(ERR_LIB_BN, BN_R_ENCODING_ERROR);\n        return NULL;\n    }\n\n    if (ain == NULL)\n        a = BN_new();\n    else\n        a = ain;\n\n    if (a == NULL)\n        return NULL;\n\n    if (len == 0) {\n        a->neg = 0;\n        a->top = 0;\n        return a;\n    }\n    d += 4;\n    if ((*d) & 0x80)\n        neg = 1;\n    if (BN_bin2bn(d, (int)len, a) == NULL) {\n        if (ain == NULL)\n            BN_free(a);\n        return NULL;",
      " * https://www.openssl.org/source/license.html\n */\n\n#include <stdlib.h>\n#include \"internal/event_queue.h\"\n#include \"crypto/sparse_array.h\"\n#include \"ssl_local.h\"\n\nstruct ossl_event_queue_st {\n    PRIORITY_QUEUE_OF(OSSL_EVENT) *timed_events;\n    PRIORITY_QUEUE_OF(OSSL_EVENT) *now_events;\n};\n\nstatic int event_compare_times(const OSSL_EVENT *a, const OSSL_EVENT *b)\n{\n    return ossl_time_compare(a->when, b->when);\n}\n\nstatic int event_compare_priority(const OSSL_EVENT *a, const OSSL_EVENT *b)\n{\n    if (a->priority > b->priority)\n        return -1;\n    if (a->priority < b->priority)\n        return 1;\n    return 0;\n}\n\nOSSL_EVENT_QUEUE *ossl_event_queue_new(void)\n{\n    OSSL_EVENT_QUEUE *r = OPENSSL_malloc(sizeof(*r));\n\n    if (r != NULL) {\n        r->timed_events = ossl_pqueue_OSSL_EVENT_new(&event_compare_times);\n        r->now_events = ossl_pqueue_OSSL_EVENT_new(&event_compare_priority);\n        if (r->timed_events == NULL || r->now_events == NULL) {\n            ossl_event_queue_free(r);\n            return NULL;\n        }\n    }\n    return r;\n}\n\nvoid ossl_event_free(OSSL_EVENT *event)\n{\n    if (event != NULL) {\n        if (event->flag_dynamic)\n            OPENSSL_free(event);\n        else\n            event->queue = NULL;\n    }\n}\n\nstatic void event_queue_free(PRIORITY_QUEUE_OF(OSSL_EVENT) *queue)\n{\n    OSSL_EVENT *e;\n\n    if (queue != NULL) {\n        while ((e = ossl_pqueue_OSSL_EVENT_pop(queue)) != NULL)\n            ossl_event_free(e);\n        ossl_pqueue_OSSL_EVENT_free(queue);\n    }\n}\n\nvoid ossl_event_queue_free(OSSL_EVENT_QUEUE *queue)",
      "                sock = BIO_accept_ex(asock, ourpeer, 0);\n            } while (sock < 0 && BIO_sock_should_retry(sock));\n            if (sock < 0) {\n                ERR_print_errors(bio_err);\n                BIO_closesocket(asock);\n                break;\n            }\n            BIO_set_tcp_ndelay(sock, 1);\n            i = (*cb)(sock, type, protocol, context);\n\n            /*\n             * If we ended with an alert being sent, but still with data in the\n             * network buffer to be read, then calling BIO_closesocket() will\n             * result in a TCP-RST being sent. On some platforms (notably\n             * Windows) then this will result in the peer immediately abandoning\n             * the connection including any buffered alert data before it has\n             * had a chance to be read. Shutting down the sending side first,\n             * and then closing the socket sends TCP-FIN first followed by\n             * TCP-RST. This seems to allow the peer to read the alert data.\n             */\n            shutdown(sock, 1); /* SHUT_WR */\n            /*\n             * We just said we have nothing else to say, but it doesn't mean\n             * that the other side has nothing. It's even recommended to\n             * consume incoming data. [In testing context this ensures that\n             * alerts are passed on...]\n             */\n            timeout.tv_sec = 0;\n            timeout.tv_usec = 500000;  /* some extreme round-trip */\n            do {\n                FD_ZERO(&readfds);\n                openssl_fdset(sock, &readfds);\n            } while (select(sock + 1, &readfds, NULL, NULL, &timeout) > 0\n                     && readsocket(sock, sink, sizeof(sink)) > 0);\n\n            BIO_closesocket(sock);\n        } else {\n            i = (*cb)(asock, type, protocol, context);\n        }\n\n        if (naccept != -1)\n            naccept--;\n        if (i < 0 || naccept == 0) {\n            BIO_closesocket(asock);\n            ret = i;\n            break;\n        }\n    }\n end:\n# ifdef AF_UNIX\n    if (family == AF_UNIX)\n        unlink(host);\n# endif\n    BIO_ADDR_free(ourpeer);\n    ourpeer = NULL;\n    return ret;\n}\n\nvoid do_ssl_shutdown(SSL *ssl)\n{\n    int ret;\n\n    do {\n        /* We only do unidirectional shutdown */",
      " * in the file LICENSE in the source distribution or at\n * https://www.openssl.org/source/license.html\n */\n\n#include <openssl/crypto.h>\n#include \"internal/e_os.h\"\n\n/* system-specific variants defining OSSL_sleep() */\n#if defined(OPENSSL_SYS_UNIX) || defined(__DJGPP__)\n#include <unistd.h>\n\nvoid OSSL_sleep(uint64_t millis)\n{\n# ifdef OPENSSL_SYS_VXWORKS\n    struct timespec ts;\n\n    ts.tv_sec = (long int) (millis / 1000);\n    ts.tv_nsec = (long int) (millis % 1000) * 1000000ul;\n    nanosleep(&ts, NULL);\n# elif defined(__TANDEM)\n#  if !defined(_REENTRANT)\n#   include <cextdecs.h(PROCESS_DELAY_)>\n\n    /* HPNS does not support usleep for non threaded apps */\n    PROCESS_DELAY_(millis * 1000);\n#  elif defined(_SPT_MODEL_)\n#   include <spthread.h>\n#   include <spt_extensions.h>\n\n    usleep(millis * 1000);\n#  else\n    usleep(millis * 1000);\n#  endif\n# else\n    unsigned int s = (unsigned int)(millis / 1000);\n    unsigned int us = (unsigned int)((millis % 1000) * 1000);\n\n    sleep(s);\n    usleep(us);\n# endif\n}\n#elif defined(_WIN32) && !defined(OPENSSL_SYS_UEFI)\n# include <windows.h>\n\nvoid OSSL_sleep(uint64_t millis)\n{\n    /*\n     * Windows' Sleep() takes a DWORD argument, which is smaller than\n     * a uint64_t, so we need to limit it to 49 days, which should be enough.\n     */\n    DWORD limited_millis = (DWORD)-1;\n\n    if (millis < limited_millis)\n        limited_millis = (DWORD)millis;\n    Sleep(limited_millis);\n}\n\n#else\n/* Fallback to a busy wait */\n# include \"internal/time.h\"\n\nstatic void ossl_sleep_secs(uint64_t secs)\n{\n    /*",
      "    pbe = ASN1_TYPE_unpack_sequence(ASN1_ITEM_rptr(PBEPARAM), param);\n    if (pbe == NULL) {\n        ERR_raise(ERR_LIB_EVP, EVP_R_DECODE_ERROR);\n        return 0;\n    }\n\n    ivl = EVP_CIPHER_get_iv_length(cipher);\n    if (ivl < 0 || ivl > 16) {\n        ERR_raise(ERR_LIB_EVP, EVP_R_INVALID_IV_LENGTH);\n        goto err;\n    }\n    kl = EVP_CIPHER_get_key_length(cipher);\n    if (kl < 0 || kl > (int)sizeof(md_tmp)) {\n        ERR_raise(ERR_LIB_EVP, EVP_R_INVALID_KEY_LENGTH);\n        goto err;\n    }\n\n    if (pbe->iter == NULL)\n        iter = 1;\n    else\n        iter = ASN1_INTEGER_get(pbe->iter);\n    salt = pbe->salt->data;\n    saltlen = pbe->salt->length;\n\n    if (pass == NULL)\n        passlen = 0;\n    else if (passlen == -1)\n        passlen = strlen(pass);\n\n    mdsize = EVP_MD_get_size(md);\n    if (mdsize < 0)\n        goto err;\n\n    kdf = EVP_KDF_fetch(libctx, OSSL_KDF_NAME_PBKDF1, propq);\n    kctx = EVP_KDF_CTX_new(kdf);\n    EVP_KDF_free(kdf);\n    if (kctx == NULL)\n        goto err;\n    *p++ = OSSL_PARAM_construct_octet_string(OSSL_KDF_PARAM_PASSWORD,\n                                             (char *)pass, (size_t)passlen);\n    *p++ = OSSL_PARAM_construct_octet_string(OSSL_KDF_PARAM_SALT,\n                                             salt, saltlen);\n    *p++ = OSSL_PARAM_construct_int(OSSL_KDF_PARAM_ITER, &iter);\n    *p++ = OSSL_PARAM_construct_utf8_string(OSSL_KDF_PARAM_DIGEST,\n                                            (char *)mdname, 0);\n    *p = OSSL_PARAM_construct_end();\n    if (EVP_KDF_derive(kctx, md_tmp, mdsize, params) != 1)\n        goto err;\n    memcpy(key, md_tmp, kl);\n    memcpy(iv, md_tmp + (16 - ivl), ivl);\n    if (!EVP_CipherInit_ex(cctx, cipher, NULL, key, iv, en_de))\n        goto err;\n    OPENSSL_cleanse(md_tmp, EVP_MAX_MD_SIZE);\n    OPENSSL_cleanse(key, EVP_MAX_KEY_LENGTH);\n    OPENSSL_cleanse(iv, EVP_MAX_IV_LENGTH);\n    rv = 1;\n err:\n    EVP_KDF_CTX_free(kctx);\n    PBEPARAM_free(pbe);\n    return rv;\n}\n\nint PKCS5_PBE_keyivgen(EVP_CIPHER_CTX *cctx, const char *pass, int passlen,\n                       ASN1_TYPE *param, const EVP_CIPHER *cipher,",
      "#include \"prov/implementations.h\"\n#include \"prov/providercommon.h\"\n\nOSSL_provider_init_fn ossl_null_provider_init;\n\n/* Parameters we provide to the core */\nstatic const OSSL_PARAM null_param_types[] = {\n    OSSL_PARAM_DEFN(OSSL_PROV_PARAM_NAME, OSSL_PARAM_UTF8_PTR, NULL, 0),\n    OSSL_PARAM_DEFN(OSSL_PROV_PARAM_VERSION, OSSL_PARAM_UTF8_PTR, NULL, 0),\n    OSSL_PARAM_DEFN(OSSL_PROV_PARAM_BUILDINFO, OSSL_PARAM_UTF8_PTR, NULL, 0),\n    OSSL_PARAM_DEFN(OSSL_PROV_PARAM_STATUS, OSSL_PARAM_INTEGER, NULL, 0),\n    OSSL_PARAM_END\n};\n\nstatic const OSSL_PARAM *null_gettable_params(const OSSL_PROVIDER *prov)\n{\n    return null_param_types;\n}\n\nstatic int null_get_params(const OSSL_PROVIDER *provctx, OSSL_PARAM params[])\n{\n    OSSL_PARAM *p;\n\n    p = OSSL_PARAM_locate(params, OSSL_PROV_PARAM_NAME);\n    if (p != NULL && !OSSL_PARAM_set_utf8_ptr(p, \"OpenSSL Null Provider\"))\n        return 0;\n    p = OSSL_PARAM_locate(params, OSSL_PROV_PARAM_VERSION);\n    if (p != NULL && !OSSL_PARAM_set_utf8_ptr(p, OPENSSL_VERSION_STR))\n        return 0;\n    p = OSSL_PARAM_locate(params, OSSL_PROV_PARAM_BUILDINFO);\n    if (p != NULL && !OSSL_PARAM_set_utf8_ptr(p, OPENSSL_FULL_VERSION_STR))\n        return 0;\n    p = OSSL_PARAM_locate(params, OSSL_PROV_PARAM_STATUS);\n    if (p != NULL && !OSSL_PARAM_set_int(p, ossl_prov_is_running()))\n        return 0;\n    return 1;\n}\n\nstatic const OSSL_ALGORITHM *null_query(OSSL_PROVIDER *prov,\n                                          int operation_id,\n                                          int *no_cache)\n{\n    *no_cache = 0;\n    return NULL;\n}\n\n/* Functions we provide to the core */\nstatic const OSSL_DISPATCH null_dispatch_table[] = {\n    { OSSL_FUNC_PROVIDER_GETTABLE_PARAMS, (void (*)(void))null_gettable_params },\n    { OSSL_FUNC_PROVIDER_GET_PARAMS, (void (*)(void))null_get_params },\n    { OSSL_FUNC_PROVIDER_QUERY_OPERATION, (void (*)(void))null_query },\n    OSSL_DISPATCH_END\n};\n\nint ossl_null_provider_init(const OSSL_CORE_HANDLE *handle,\n                            const OSSL_DISPATCH *in,\n                            const OSSL_DISPATCH **out,\n                            void **provctx)\n{\n    *out = null_dispatch_table;\n\n    /* Could be anything - we don't use it */\n    *provctx = (void *)handle;\n    return 1;",
      "    }\n    (void)ERR_pop_to_mark();\n\n    md_size = EVP_MD_get_size(md);\n    md_nid = EVP_MD_get_type(md);\n    if (md_size < 0)\n        goto err;\n    if ((md_nid == NID_id_GostR3411_94\n         || md_nid == NID_id_GostR3411_2012_256\n         || md_nid == NID_id_GostR3411_2012_512)\n        && ossl_safe_getenv(\"LEGACY_GOST_PKCS12\") == NULL) {\n        md_size = TK26_MAC_KEY_LEN;\n        if (!pkcs12_gen_gost_mac_key(pass, passlen, salt, saltlen, iter,\n                                     md_size, key, md)) {\n            ERR_raise(ERR_LIB_PKCS12, PKCS12_R_KEY_GEN_ERROR);\n            goto err;\n        }\n    } else {\n        if (pkcs12_key_gen != NULL) {\n            if (!(*pkcs12_key_gen)(pass, passlen, salt, saltlen, PKCS12_MAC_ID,\n                                   iter, md_size, key, md)) {\n                ERR_raise(ERR_LIB_PKCS12, PKCS12_R_KEY_GEN_ERROR);\n                goto err;\n            }\n        } else {\n            /* Default to UTF-8 password */\n            if (!PKCS12_key_gen_utf8_ex(pass, passlen, salt, saltlen, PKCS12_MAC_ID,\n                                       iter, md_size, key, md,\n                                       p12->authsafes->ctx.libctx,\n                                       p12->authsafes->ctx.propq)) {\n                ERR_raise(ERR_LIB_PKCS12, PKCS12_R_KEY_GEN_ERROR);\n                goto err;\n            }\n        }\n    }\n    if ((hmac = HMAC_CTX_new()) == NULL\n        || !HMAC_Init_ex(hmac, key, md_size, md, NULL)\n        || !HMAC_Update(hmac, p12->authsafes->d.data->data,\n                        p12->authsafes->d.data->length)\n        || !HMAC_Final(hmac, mac, maclen)) {\n        goto err;\n    }\n    ret = 1;\n\nerr:\n    OPENSSL_cleanse(key, sizeof(key));\n    HMAC_CTX_free(hmac);\n    EVP_MD_free(md_fetch);\n    return ret;\n}\n\nint PKCS12_gen_mac(PKCS12 *p12, const char *pass, int passlen,\n                   unsigned char *mac, unsigned int *maclen)\n{\n    return pkcs12_gen_mac(p12, pass, passlen, mac, maclen, NULL);\n}\n\n/* Verify the mac */\nint PKCS12_verify_mac(PKCS12 *p12, const char *pass, int passlen)\n{\n    unsigned char mac[EVP_MAX_MD_SIZE];\n    unsigned int maclen;\n    const ASN1_OCTET_STRING *macoct;\n",
      " *\n * Licensed under the Apache License 2.0 (the \"License\").  You may not use\n * this file except in compliance with the License.  You can obtain a copy\n * in the file LICENSE in the source distribution or at\n * https://www.openssl.org/source/license.html\n */\n\n/*\n * S/MIME detached data encrypt example: rarely done but should the need\n * arise this is an example....\n */\n#include <openssl/pem.h>\n#include <openssl/cms.h>\n#include <openssl/err.h>\n\nint main(int argc, char **argv)\n{\n    BIO *in = NULL, *out = NULL, *tbio = NULL, *dout = NULL;\n    X509 *rcert = NULL;\n    STACK_OF(X509) *recips = NULL;\n    CMS_ContentInfo *cms = NULL;\n    int ret = EXIT_FAILURE;\n\n    int flags = CMS_STREAM | CMS_DETACHED;\n\n    OpenSSL_add_all_algorithms();\n    ERR_load_crypto_strings();\n\n    /* Read in recipient certificate */\n    tbio = BIO_new_file(\"signer.pem\", \"r\");\n\n    if (!tbio)\n        goto err;\n\n    rcert = PEM_read_bio_X509(tbio, NULL, 0, NULL);\n\n    if (!rcert)\n        goto err;\n\n    /* Create recipient STACK and add recipient cert to it */\n    recips = sk_X509_new_null();\n\n    if (!recips || !sk_X509_push(recips, rcert))\n        goto err;\n\n    /*\n     * OSSL_STACK_OF_X509_free() free up recipient STACK and its contents\n     * so set rcert to NULL so it isn't freed up twice.\n     */\n    rcert = NULL;\n\n    /* Open content being encrypted */\n\n    in = BIO_new_file(\"encr.txt\", \"r\");\n\n    dout = BIO_new_file(\"smencr.out\", \"wb\");\n\n    if (!in)\n        goto err;\n\n    /* encrypt content */\n    cms = CMS_encrypt(recips, in, EVP_des_ede3_cbc(), flags);\n\n    if (!cms)",
      "#include \"err_local.h\"\n\nvoid ERR_new(void)\n{\n    ERR_STATE *es;\n\n    es = ossl_err_get_state_int();\n    if (es == NULL)\n        return;\n\n    /* Allocate a slot */\n    err_get_slot(es);\n    err_clear(es, es->top, 0);\n}\n\nvoid ERR_set_debug(const char *file, int line, const char *func)\n{\n    ERR_STATE *es;\n\n    es = ossl_err_get_state_int();\n    if (es == NULL)\n        return;\n\n    err_set_debug(es, es->top, file, line, func);\n}\n\nvoid ERR_set_error(int lib, int reason, const char *fmt, ...)\n{\n    va_list args;\n\n    va_start(args, fmt);\n    ERR_vset_error(lib, reason, fmt, args);\n    va_end(args);\n}\n\nvoid ERR_vset_error(int lib, int reason, const char *fmt, va_list args)\n{\n    ERR_STATE *es;\n    char *buf = NULL;\n    size_t buf_size = 0;\n    unsigned long flags = 0;\n    size_t i;\n\n    es = ossl_err_get_state_int();\n    if (es == NULL)\n        return;\n    i = es->top;\n\n    if (fmt != NULL) {\n        int printed_len = 0;\n        char *rbuf = NULL;\n\n        buf = es->err_data[i];\n        buf_size = es->err_data_size[i];\n\n        /*\n         * To protect the string we just grabbed from tampering by other\n         * functions we may call, or to protect them from freeing a pointer\n         * that may no longer be valid at that point, we clear away the\n         * data pointer and the flags.  We will set them again at the end\n         * of this function.\n         */\n        es->err_data[i] = NULL;\n        es->err_data_flags[i] = 0;"
    ]
  },
  {
    "id": "apache/spark",
    "org": "apache",
    "avatarURL": "https://avatars.githubusercontent.com/u/47359?v=4",
    "name": "apache/spark",
    "url": "https://github.com/apache/spark",
    "lang": "Scala",
    "desc": "Unified analytics engine for big data and machine learning.",
    "star_num": 36637,
    "fork_num": 27594,
    "snippets": [
      "\nimport org.apache.commons.io.FileUtils\nimport org.apache.hadoop.fs.Path\n\nimport org.apache.spark._\nimport org.apache.spark.{SparkContext, SparkFunSuite}\nimport org.apache.spark.internal.config._\nimport org.apache.spark.launcher.SparkLauncher\nimport org.apache.spark.network.util.JavaUtils\nimport org.apache.spark.util.Utils\n\nclass DriverLoggerSuite extends SparkFunSuite with LocalSparkContext {\n\n  private var rootDfsDir : File = _\n\n  override def beforeAll(): Unit = {\n    super.beforeAll()\n    rootDfsDir = Utils.createTempDir(namePrefix = \"dfs_logs\")\n  }\n\n  override def afterAll(): Unit = {\n    super.afterAll()\n    JavaUtils.deleteRecursively(rootDfsDir)\n  }\n\n  test(\"driver logs are persisted locally and synced to dfs\") {\n    val sc = getSparkContext()\n\n    val app_id = sc.applicationId\n    // Run a simple spark application\n    sc.parallelize(1 to 1000).count()\n\n    // Assert driver log file exists\n    val rootDir = Utils.getLocalDir(sc.getConf)\n    val driverLogsDir = FileUtils.getFile(rootDir, DriverLogger.DRIVER_LOG_DIR)\n    assert(driverLogsDir.exists())\n    val files = driverLogsDir.listFiles()\n    assert(files.length === 1)\n    assert(files(0).getName.equals(DriverLogger.DRIVER_LOG_FILE))\n\n    sc.stop()\n    assert(!driverLogsDir.exists())\n    val dfsFile = FileUtils.getFile(sc.getConf.get(DRIVER_LOG_DFS_DIR).get,\n      app_id + DriverLogger.DRIVER_LOG_FILE_SUFFIX)\n    assert(dfsFile.exists())\n    assert(dfsFile.length() > 0)\n  }\n\n  test(\"SPARK-40901: driver logs are persisted locally and synced to dfs when log \" +\n    \"dir is absolute URI\") {\n    val sparkConf = new SparkConf()\n    sparkConf.set(DRIVER_LOG_DFS_DIR, \"file://\" + rootDfsDir.getAbsolutePath())\n    val sc = getSparkContext(sparkConf)\n    val app_id = sc.applicationId\n    // Run a simple spark application\n    sc.parallelize(1 to 1000).count()\n\n    // Assert driver log file exists\n    val rootDir = Utils.getLocalDir(sc.getConf)\n    val driverLogsDir = FileUtils.getFile(rootDir, DriverLogger.DRIVER_LOG_DIR)\n    assert(driverLogsDir.exists())\n    val files = driverLogsDir.listFiles()\n    assert(files.length === 1)\n    assert(files(0).getName.equals(DriverLogger.DRIVER_LOG_FILE))",
      "    Seq.empty\n  }\n\n  override def output: Seq[Attribute] = Seq.empty\n}\n",
      "  type ReturnType = (Seq[NamedExpression], Seq[Expression], LogicalPlan)\n  override protected def collectAllFilters: Boolean = false\n\n  def unapply(plan: LogicalPlan): Option[ReturnType] = {\n    val alwaysInline = SQLConf.get.getConf(SQLConf.COLLAPSE_PROJECT_ALWAYS_INLINE)\n    val (fields, filters, child, _) = collectProjectsAndFilters(plan, alwaysInline)\n    // If more than 2 filters are collected, they must all be deterministic.\n    if (filters.length > 1) assert(filters.forall(_.deterministic))\n    Some((\n      fields.getOrElse(child.output),\n      filters.flatMap(splitConjunctivePredicates),\n      child))\n  }\n}\n\n/**\n * A variant of [[PhysicalOperation]] which can match multiple Filters that are not combinable due\n * to non-deterministic predicates. This is useful for scan operations as we need to match a bunch\n * of adjacent Projects/Filters to apply column pruning, even if the Filters can't be combined,\n * such as `Project(a, Filter(rand() > 0.5, Filter(rand() < 0.8, TableScan)))`, which we should\n * only read column `a` from the relation.\n */\nobject ScanOperation extends OperationHelper {\n  // Returns: (the final project list, filters to stay up, filters to push down, relation)\n  type ReturnType = (Seq[NamedExpression], Seq[Expression], Seq[Expression], LogicalPlan)\n  override protected def collectAllFilters: Boolean = true\n\n  def unapply(plan: LogicalPlan): Option[ReturnType] = {\n    val alwaysInline = SQLConf.get.getConf(SQLConf.COLLAPSE_PROJECT_ALWAYS_INLINE)\n    val (fields, filters, child, _) = collectProjectsAndFilters(plan, alwaysInline)\n    // `collectProjectsAndFilters` transforms the plan bottom-up, so the bottom-most filter are\n    // placed at the beginning of `filters` list. According to the SQL semantic, we cannot merge\n    // Filters if one or more of them are nondeterministic. This means we can only push down the\n    // bottom-most Filter, or more following deterministic Filters if the bottom-most Filter is\n    // also deterministic.\n    if (filters.isEmpty) {\n      Some((fields.getOrElse(child.output), Nil, Nil, child))\n    } else if (filters.head.deterministic) {\n      val filtersCanPushDown = filters.takeWhile(_.deterministic)\n        .flatMap(splitConjunctivePredicates)\n      val filtersStayUp = filters.dropWhile(_.deterministic)\n      Some((fields.getOrElse(child.output), filtersStayUp, filtersCanPushDown, child))\n    } else {\n      val filtersCanPushDown = splitConjunctivePredicates(filters.head)\n      val filtersStayUp = filters.drop(1)\n      Some((fields.getOrElse(child.output), filtersStayUp, filtersCanPushDown, child))\n    }\n  }\n}\n\nobject NodeWithOnlyDeterministicProjectAndFilter {\n  @scala.annotation.tailrec\n  def unapply(plan: LogicalPlan): Option[LogicalPlan] = plan match {\n    case Project(projectList, child) if projectList.forall(_.deterministic) => unapply(child)\n    case Filter(cond, child) if cond.deterministic => unapply(child)\n    case _ => Some(plan)\n  }\n}\n\n/**\n * A pattern that finds joins with equality conditions that can be evaluated using equi-join.\n *\n * Null-safe equality will be transformed into equality as joining key (replace null with default\n * value).",
      "    assert(df1.logicalPlan.asInstanceOf[Project].projectList.forall(!_.deterministic))\n    assert(df1.head().getDouble(0) >= 0.0)\n\n    withSQLConf(SQLConf.LEGACY_ALLOW_UNTYPED_SCALA_UDF.key -> \"true\") {\n      val bar = udf(() => Math.random(), DataTypes.DoubleType).asNondeterministic()\n      val df2 = testData.select(bar())\n      assert(df2.logicalPlan.asInstanceOf[Project].projectList.forall(!_.deterministic))\n      assert(df2.head().getDouble(0) >= 0.0)\n    }\n\n    val javaUdf = udf(new UDF0[Double] {\n      override def call(): Double = Math.random()\n    }, DoubleType).asNondeterministic()\n    val df3 = testData.select(javaUdf())\n    assert(df3.logicalPlan.asInstanceOf[Project].projectList.forall(!_.deterministic))\n    assert(df3.head().getDouble(0) >= 0.0)\n  }\n\n  test(\"TwoArgument UDF\") {\n    spark.udf.register(\"strLenScala\", (_: String).length + (_: Int))\n    assert(sql(\"SELECT strLenScala('test', 1)\").head().getInt(0) === 5)\n  }\n\n  test(\"UDF in a WHERE\") {\n    withTempView(\"integerData\") {\n      spark.udf.register(\"oneArgFilter\", (n: Int) => { n > 80 })\n\n      val df = sparkContext.parallelize(\n        (1 to 100).map(i => TestData(i, i.toString))).toDF()\n      df.createOrReplaceTempView(\"integerData\")\n\n      val result =\n        sql(\"SELECT * FROM integerData WHERE oneArgFilter(key)\")\n      assert(result.count() === 20)\n    }\n  }\n\n  test(\"UDF in a HAVING\") {\n    withTempView(\"groupData\") {\n      spark.udf.register(\"havingFilter\", (n: Long) => { n > 5 })\n\n      val df = Seq((\"red\", 1), (\"red\", 2), (\"blue\", 10),\n        (\"green\", 100), (\"green\", 200)).toDF(\"g\", \"v\")\n      df.createOrReplaceTempView(\"groupData\")\n\n      val result =\n        sql(\n          \"\"\"\n           | SELECT g, SUM(v) as s\n           | FROM groupData\n           | GROUP BY g\n           | HAVING havingFilter(s)\n          \"\"\".stripMargin)\n\n      assert(result.count() === 2)\n    }\n  }\n\n  test(\"UDF in a GROUP BY\") {\n    withTempView(\"groupData\") {\n      spark.udf.register(\"groupFunction\", (n: Int) => { n > 10 })\n\n      val df = Seq((\"red\", 1), (\"red\", 2), (\"blue\", 10),\n        (\"green\", 100), (\"green\", 200)).toDF(\"g\", \"v\")",
      "\nprivate[r] object BisectingKMeansWrapper extends MLReadable[BisectingKMeansWrapper] {\n\n  def fit(\n      data: DataFrame,\n      formula: String,\n      k: Int,\n      maxIter: Int,\n      seed: String,\n      minDivisibleClusterSize: Double\n      ): BisectingKMeansWrapper = {\n\n    val rFormula = new RFormula()\n      .setFormula(formula)\n      .setFeaturesCol(\"features\")\n    RWrapperUtils.checkDataColumns(rFormula, data)\n    val rFormulaModel = rFormula.fit(data)\n\n    // get feature names from output schema\n    val schema = rFormulaModel.transform(data).schema\n    val featureAttrs = AttributeGroup.fromStructField(schema(rFormulaModel.getFeaturesCol))\n      .attributes.get\n    val features = featureAttrs.map(_.name.get)\n\n    val bisectingKmeans = new BisectingKMeans()\n      .setK(k)\n      .setMaxIter(maxIter)\n      .setMinDivisibleClusterSize(minDivisibleClusterSize)\n      .setFeaturesCol(rFormula.getFeaturesCol)\n\n    if (seed != null && seed.length > 0) bisectingKmeans.setSeed(seed.toInt)\n\n    val pipeline = new Pipeline()\n      .setStages(Array(rFormulaModel, bisectingKmeans))\n      .fit(data)\n\n    val bisectingKmeansModel: BisectingKMeansModel =\n      pipeline.stages.last.asInstanceOf[BisectingKMeansModel]\n    val size: Array[Long] = bisectingKmeansModel.summary.clusterSizes\n\n    new BisectingKMeansWrapper(pipeline, features, size)\n  }\n\n  override def read: MLReader[BisectingKMeansWrapper] = new BisectingKMeansWrapperReader\n\n  override def load(path: String): BisectingKMeansWrapper = super.load(path)\n\n  class BisectingKMeansWrapperWriter(instance: BisectingKMeansWrapper) extends MLWriter {\n\n    override protected def saveImpl(path: String): Unit = {\n      val rMetadataPath = new Path(path, \"rMetadata\").toString\n      val pipelinePath = new Path(path, \"pipeline\").toString\n\n      val rMetadata = (\"class\" -> instance.getClass.getName) ~\n        (\"features\" -> instance.features.toSeq) ~\n        (\"size\" -> instance.size.toSeq)\n      val rMetadataJson: String = compact(render(rMetadata))\n\n      sc.parallelize(Seq(rMetadataJson), 1).saveAsTextFile(rMetadataPath)\n      instance.pipeline.save(pipelinePath)\n    }\n  }\n\n  class BisectingKMeansWrapperReader extends MLReader[BisectingKMeansWrapper] {",
      "  }\n}\n",
      " * the License.  You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.spark.sql.catalyst.analysis\n\nimport org.apache.spark.sql.catalyst.catalog.CatalogTable\nimport org.apache.spark.sql.catalyst.plans.logical.{AddColumns, AlterColumn, CreateTable, LogicalPlan, ReplaceColumns, ReplaceTable}\nimport org.apache.spark.sql.catalyst.rules.Rule\nimport org.apache.spark.sql.catalyst.util.CharVarcharUtils\nimport org.apache.spark.sql.execution.command.{AlterTableAddColumnsCommand, AlterTableChangeColumnCommand, CreateDataSourceTableCommand, CreateTableCommand}\nimport org.apache.spark.sql.internal.SQLConf\nimport org.apache.spark.sql.types.StructType\n\nobject ReplaceCharWithVarchar extends Rule[LogicalPlan] {\n  override def apply(plan: LogicalPlan): LogicalPlan = {\n    if (!conf.getConf(SQLConf.CHAR_AS_VARCHAR)) return plan\n\n    plan.resolveOperators {\n      // V2 commands\n      case cmd: CreateTable =>\n        cmd.copy(tableSchema = replaceCharWithVarcharInSchema(cmd.tableSchema))\n      case cmd: ReplaceTable =>\n        cmd.copy(tableSchema = replaceCharWithVarcharInSchema(cmd.tableSchema))\n      case cmd: AddColumns =>\n        cmd.copy(columnsToAdd = cmd.columnsToAdd.map { col =>\n          col.copy(dataType = CharVarcharUtils.replaceCharWithVarchar(col.dataType))\n        })\n      case cmd: AlterColumn =>\n        cmd.copy(dataType = cmd.dataType.map(CharVarcharUtils.replaceCharWithVarchar))\n      case cmd: ReplaceColumns =>\n        cmd.copy(columnsToAdd = cmd.columnsToAdd.map { col =>\n          col.copy(dataType = CharVarcharUtils.replaceCharWithVarchar(col.dataType))\n        })\n\n      // V1 commands\n      case cmd: CreateTableCommand =>\n        cmd.copy(table = replaceCharWithVarcharInTableMeta(cmd.table))\n      case cmd: CreateDataSourceTableCommand =>\n        cmd.copy(table = replaceCharWithVarcharInTableMeta(cmd.table))\n      case cmd: AlterTableAddColumnsCommand =>\n        cmd.copy(colsToAdd = cmd.colsToAdd.map { col =>\n          col.copy(dataType = CharVarcharUtils.replaceCharWithVarchar(col.dataType))\n        })\n      case cmd: AlterTableChangeColumnCommand =>\n        cmd.copy(newColumn = cmd.newColumn.copy(\n          dataType = CharVarcharUtils.replaceCharWithVarchar(cmd.newColumn.dataType)))\n    }\n  }\n\n  private def replaceCharWithVarcharInSchema(schema: StructType): StructType = {\n    CharVarcharUtils.replaceCharWithVarchar(schema).asInstanceOf[StructType]\n  }\n\n  private def replaceCharWithVarcharInTableMeta(tbl: CatalogTable): CatalogTable = {\n    tbl.copy(schema = replaceCharWithVarcharInSchema(tbl.schema))\n  }",
      "        }\n      } catch {\n        case _: Throwable => Thread.sleep(10)\n          // Do nothing. We might see exceptions because block manager\n          // is racing this thread to remove entries from the driver.\n      }\n    }\n    assert(sc.getRDDStorageInfo.isEmpty)\n  }\n}\n",
      "  metricRegistry.register(MetricRegistry.name(\"waitingDrivers\"), new Gauge[Int] {\n    override def getValue: Int = scheduler.getQueuedDriversSize\n  })\n\n  metricRegistry.register(MetricRegistry.name(\"launchedDrivers\"), new Gauge[Int] {\n    override def getValue: Int = scheduler.getLaunchedDriversSize\n  })\n\n  metricRegistry.register(MetricRegistry.name(\"retryDrivers\"), new Gauge[Int] {\n    override def getValue: Int = scheduler.getPendingRetryDriversSize\n  })\n}\n",
      "}\n"
    ]
  },
  {
    "id": "microsoft/TypeScript",
    "org": "microsoft",
    "avatarURL": "https://avatars.githubusercontent.com/u/6154722?v=4",
    "name": "microsoft/TypeScript",
    "url": "https://github.com/microsoft/TypeScript",
    "lang": "TypeScript",
    "desc": "Superset of JavaScript that compiles to clean JavaScript output.",
    "star_num": 93895,
    "fork_num": 12163,
    "snippets": [
      "﻿// @removeComments: true\r\n\r\nclass C {\r\n    /*! remove pinned comment anywhere else */\r\n    public foo(x: string, y: any)\r\n    public foo(x: string, y: number) { }\r\n}\r\n\r\n/*! remove pinned comment anywhere else */\r\ndeclare var OData: any;",
      "class TestFile {\r\n    name: string;\r\n    foo(message: string): () => string {\r\n        return (...x: string[]) =>\r\n            /// <summary>Test summary</summary>\r\n            /// <param name=\"message\" type=\"String\" />\r\n            /// <returns type=\"Function\" />\r\n            message + this.name;\r\n    }\r\n}",
      "\r\nexport {};\r\n",
      "// @target: es5\r\n// @module: commonjs\r\n// @declaration: true\r\n\r\n// @filename: server.ts\r\nexport class c {\r\n}\r\nexport interface i {\r\n}\r\nexport module m {\r\n    export var x = 10;\r\n}\r\nexport var x = 10;\r\nexport module uninstantiated {\r\n}\r\n\r\n// @filename: client.ts\r\nexport * from \"./server\";",
      "declare var m1_a1: number;\r\ndeclare class m1_c1 {\r\n    m1_c1_p1: number;\r\n}\r\ndeclare var m1_instance1: m1_c1;\r\ndeclare function m1_f1(): m1_c1;\r\ndeclare var m2_a1: number;\r\ndeclare class m2_c1 {\r\n    m2_c1_p1: number;\r\n}\r\ndeclare var m2_instance1: m2_c1;\r\ndeclare function m2_f1(): m2_c1;\r\ndeclare var a1: number;\r\ndeclare class c1 {\r\n    p1: number;\r\n}\r\ndeclare var instance1: c1;\r\ndeclare function f1(): c1;\r\n",
      "// @allowJs: true\r\n// @checkJs: true\r\n// @outDir: ./out\r\n// @target: es2018\r\n// @filename: file.js\r\n/**\r\n * Adds\r\n * @param {number} 𝑚\r\n * @param {number} 𝑀\r\n */\r\nfunction foo(𝑚, 𝑀) {\r\n    console.log(𝑀 + 𝑚);\r\n}",
      "// @module: amd\r\nconst foo = foo; // compile error\r\nexport const bar = bar; // should be compile error\r\nfunction f() {\r\n  const bar = bar; // compile error\r\n}\r\nnamespace NS {\r\n  export const bar = bar; // should be compile error\r\n}\r\n\r\nlet foo1 = foo1; // compile error\r\nexport let bar1 = bar1; // should be compile error\r\nfunction f1() {\r\n  let bar1 = bar1; // compile error\r\n}\r\nnamespace NS1 {\r\n  export let bar1 = bar1; // should be compile error\r\n}",
      "// @allowJs: true\n// @checkJs: true\n// @outDir: ./out\n// @filename: checkJsdocTypeTagOnExportAssignment1.js\n\n// @Filename: a.js\n/**\n * @typedef {Object} Foo\n * @property {boolean} a\n * @property {boolean} b\n */\n\n/** @type {Foo} */\nexport default { c: false };\n\n// @Filename: b.js\nimport a from \"./a\";\na;\n",
      "import {foo} from \"./utils\";\r\nexport = foo;",
      "// @target: ES5\r\n// @module: amd\r\nexport class A\r\n{\r\n    constructor ()\r\n    {\r\n    }\r\n\r\n    public B()\r\n    {\r\n        return 42;\r\n    }\r\n}"
    ]
  },
  {
    "id": "spring-projects/spring-boot",
    "org": "spring-projects",
    "avatarURL": "https://avatars.githubusercontent.com/u/317776?v=4",
    "name": "spring-projects/spring-boot",
    "url": "https://github.com/spring-projects/spring-boot",
    "lang": "Java",
    "desc": "Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications.",
    "star_num": 69217,
    "fork_num": 39398,
    "snippets": [
      " *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.boot.loader.tools;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.Arrays;\n\nimport org.springframework.util.Assert;\nimport org.springframework.util.StringUtils;\n\n/**\n * Provides access to the java binary executable, regardless of OS.\n *\n * @author Phillip Webb\n * @since 1.1.0\n */\npublic class JavaExecutable {\n\n\tprivate final File file;\n\n\tpublic JavaExecutable() {\n\t\tString javaHome = System.getProperty(\"java.home\");\n\t\tAssert.state(StringUtils.hasLength(javaHome), \"Unable to find java executable due to missing 'java.home'\");\n\t\tthis.file = findInJavaHome(javaHome);\n\t}\n\n\tprivate File findInJavaHome(String javaHome) {\n\t\tFile bin = new File(new File(javaHome), \"bin\");\n\t\tFile command = new File(bin, \"java.exe\");\n\t\tcommand = command.exists() ? command : new File(bin, \"java\");\n\t\tAssert.state(command.exists(), () -> \"Unable to find java in \" + javaHome);\n\t\treturn command;\n\t}\n\n\t/**\n\t * Create a new {@link ProcessBuilder} that will run with the Java executable.\n\t * @param arguments the command arguments\n\t * @return a {@link ProcessBuilder}\n\t */\n\tpublic ProcessBuilder processBuilder(String... arguments) {\n\t\tProcessBuilder processBuilder = new ProcessBuilder(toString());\n\t\tprocessBuilder.command().addAll(Arrays.asList(arguments));\n\t\treturn processBuilder;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\ttry {\n\t\t\treturn this.file.getCanonicalPath();\n\t\t}\n\t\tcatch (IOException ex) {\n\t\t\tthrow new IllegalStateException(ex);\n\t\t}\n\t}\n",
      "\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\treturn null;\n\t}\n\n\tstatic Double toDouble(Object value) {\n\t\tif (value instanceof Double) {\n\t\t\treturn (Double) value;\n\t\t}\n\t\tif (value instanceof Number) {\n\t\t\treturn ((Number) value).doubleValue();\n\t\t}\n\t\tif (value instanceof String) {\n\t\t\ttry {\n\t\t\t\treturn Double.valueOf((String) value);\n\t\t\t}\n\t\t\tcatch (NumberFormatException ignored) {\n\t\t\t}\n\t\t}\n\t\treturn null;\n\t}\n\n\tstatic Integer toInteger(Object value) {\n\t\tif (value instanceof Integer) {\n\t\t\treturn (Integer) value;\n\t\t}\n\t\tif (value instanceof Number) {\n\t\t\treturn ((Number) value).intValue();\n\t\t}\n\t\tif (value instanceof String) {\n\t\t\ttry {\n\t\t\t\treturn (int) Double.parseDouble((String) value);\n\t\t\t}\n\t\t\tcatch (NumberFormatException ignored) {\n\t\t\t}\n\t\t}\n\t\treturn null;\n\t}\n\n\tstatic Long toLong(Object value) {\n\t\tif (value instanceof Long) {\n\t\t\treturn (Long) value;\n\t\t}\n\t\tif (value instanceof Number) {\n\t\t\treturn ((Number) value).longValue();\n\t\t}\n\t\tif (value instanceof String) {\n\t\t\ttry {\n\t\t\t\treturn (long) Double.parseDouble((String) value);\n\t\t\t}\n\t\t\tcatch (NumberFormatException ignored) {\n\t\t\t}\n\t\t}\n\t\treturn null;\n\t}\n\n\tstatic String toString(Object value) {\n\t\tif (value instanceof String) {\n\t\t\treturn (String) value;\n\t\t}\n\t\tif (value != null) {\n\t\t\treturn String.valueOf(value);\n\t\t}",
      "\t\t}\n\n\t\tpublic void setInitQueryTimeout(Duration initQueryTimeout) {\n\t\t\tthis.initQueryTimeout = initQueryTimeout;\n\t\t}\n\n\t}\n\n\tpublic static class Request {\n\n\t\t/**\n\t\t * How long the driver waits for a request to complete.\n\t\t */\n\t\tprivate Duration timeout;\n\n\t\t/**\n\t\t * Queries consistency level.\n\t\t */\n\t\tprivate DefaultConsistencyLevel consistency;\n\n\t\t/**\n\t\t * Queries serial consistency level.\n\t\t */\n\t\tprivate DefaultConsistencyLevel serialConsistency;\n\n\t\t/**\n\t\t * How many rows will be retrieved simultaneously in a single network round-trip.\n\t\t */\n\t\tprivate Integer pageSize;\n\n\t\tprivate final Throttler throttler = new Throttler();\n\n\t\tpublic Duration getTimeout() {\n\t\t\treturn this.timeout;\n\t\t}\n\n\t\tpublic void setTimeout(Duration timeout) {\n\t\t\tthis.timeout = timeout;\n\t\t}\n\n\t\tpublic DefaultConsistencyLevel getConsistency() {\n\t\t\treturn this.consistency;\n\t\t}\n\n\t\tpublic void setConsistency(DefaultConsistencyLevel consistency) {\n\t\t\tthis.consistency = consistency;\n\t\t}\n\n\t\tpublic DefaultConsistencyLevel getSerialConsistency() {\n\t\t\treturn this.serialConsistency;\n\t\t}\n\n\t\tpublic void setSerialConsistency(DefaultConsistencyLevel serialConsistency) {\n\t\t\tthis.serialConsistency = serialConsistency;\n\t\t}\n\n\t\tpublic Integer getPageSize() {\n\t\t\treturn this.pageSize;\n\t\t}\n\n\t\tpublic void setPageSize(int pageSize) {\n\t\t\tthis.pageSize = pageSize;\n\t\t}\n",
      " * Auto-configuration for Jetty actuator metrics.\n */\npackage org.springframework.boot.actuate.autoconfigure.metrics.web.jetty;\n",
      " * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.boot.actuate.autoconfigure.tracing;\n\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\n\nimport brave.internal.propagation.StringPropagationAdapter;\nimport brave.propagation.Propagation;\nimport brave.propagation.TraceContext;\nimport brave.propagation.TraceContextOrSamplingFlags;\nimport org.junit.jupiter.api.Nested;\nimport org.junit.jupiter.api.Test;\nimport org.mockito.Mockito;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.entry;\nimport static org.mockito.BDDMockito.given;\n\n/**\n * Tests for {@link CompositePropagationFactory}.\n *\n * @author Moritz Halbritter\n */\nclass CompositePropagationFactoryTests {\n\n\t@Test\n\tvoid supportsJoin() {\n\t\tPropagation.Factory supported = Mockito.mock(Propagation.Factory.class);\n\t\tgiven(supported.supportsJoin()).willReturn(true);\n\t\tgiven(supported.get()).willReturn(new DummyPropagation(\"a\"));\n\t\tPropagation.Factory unsupported = Mockito.mock(Propagation.Factory.class);\n\t\tgiven(unsupported.supportsJoin()).willReturn(false);\n\t\tgiven(unsupported.get()).willReturn(new DummyPropagation(\"a\"));\n\t\tCompositePropagationFactory factory = new CompositePropagationFactory(List.of(supported), List.of(unsupported));\n\t\tassertThat(factory.supportsJoin()).isFalse();\n\t}\n\n\t@Test\n\tvoid requires128BitTraceId() {\n\t\tPropagation.Factory required = Mockito.mock(Propagation.Factory.class);\n\t\tgiven(required.requires128BitTraceId()).willReturn(true);\n\t\tgiven(required.get()).willReturn(new DummyPropagation(\"a\"));\n\t\tPropagation.Factory notRequired = Mockito.mock(Propagation.Factory.class);\n\t\tgiven(notRequired.requires128BitTraceId()).willReturn(false);\n\t\tgiven(notRequired.get()).willReturn(new DummyPropagation(\"a\"));\n\t\tCompositePropagationFactory factory = new CompositePropagationFactory(List.of(required), List.of(notRequired));\n\t\tassertThat(factory.requires128BitTraceId()).isTrue();\n\t}\n\n\t@Nested\n\tstatic class CompostePropagationTests {",
      "\t\t\t\t\t\tif (value != null) {\n\t\t\t\t\t\t\tClass<?> fieldType = value.getClass();\n\t\t\t\t\t\t\tif (Serializable.class.isAssignableFrom(fieldType)) {\n\t\t\t\t\t\t\t\tmodelClasses.add((Class<? extends Serializable>) fieldType);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t});\n\t\t\t\t\tcandidate = candidate.getSuperclass();\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor (Model submodel : model.getSubModels()) {\n\t\t\t\tmodelClasses.addAll(serializationTypes(submodel));\n\t\t\t}\n\t\t\treturn modelClasses;\n\t\t}\n\n\t\tprivate Set<String> reflectionTypes(Model model) {\n\t\t\treturn reflectionTypes(model, () -> null);\n\t\t}\n\n\t\tprivate Set<String> reflectionTypes(Model model, Supplier<Object> parent) {\n\t\t\tSet<String> reflectionTypes = new HashSet<>();\n\t\t\tClass<?> componentType = determineType(model, parent);\n\t\t\tif (componentType != null) {\n\t\t\t\tprocessComponent(componentType, reflectionTypes);\n\t\t\t}\n\t\t\tSupplier<Object> componentSupplier = SingletonSupplier.ofNullable(() -> instantiate(componentType));\n\t\t\tfor (Model submodel : model.getSubModels()) {\n\t\t\t\treflectionTypes.addAll(reflectionTypes(submodel, componentSupplier));\n\t\t\t}\n\t\t\treturn reflectionTypes;\n\t\t}\n\n\t\tprivate Class<?> determineType(Model model, Supplier<Object> parentSupplier) {\n\t\t\tString className = (model instanceof ComponentModel componentModel) ? componentModel.getClassName() : null;\n\t\t\tif (className != null) {\n\t\t\t\treturn loadImportType(className);\n\t\t\t}\n\t\t\tString tag = model.getTag();\n\t\t\tif (tag != null) {\n\t\t\t\tclassName = this.modelInterpretationContext.getDefaultNestedComponentRegistry()\n\t\t\t\t\t.findDefaultComponentTypeByTag(tag);\n\t\t\t\tif (className != null) {\n\t\t\t\t\treturn loadImportType(className);\n\t\t\t\t}\n\t\t\t\treturn inferTypeFromParent(parentSupplier, tag);\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\n\t\tprivate Class<?> loadImportType(String className) {\n\t\t\treturn loadComponentType(this.modelInterpretationContext.getImport(className));\n\t\t}\n\n\t\tprivate Class<?> inferTypeFromParent(Supplier<Object> parentSupplier, String tag) {\n\t\t\tObject parent = parentSupplier.get();\n\t\t\tif (parent != null) {\n\t\t\t\ttry {\n\t\t\t\t\tPropertySetter propertySetter = new PropertySetter(\n\t\t\t\t\t\t\tthis.modelInterpretationContext.getBeanDescriptionCache(), parent);\n\t\t\t\t\tClass<?> typeFromPropertySetter = propertySetter.getClassNameViaImplicitRules(tag,\n\t\t\t\t\t\t\tAggregationType.AS_COMPLEX_PROPERTY,\n\t\t\t\t\t\t\tthis.modelInterpretationContext.getDefaultNestedComponentRegistry());\n\t\t\t\t\treturn typeFromPropertySetter;",
      "import java.util.Iterator;\nimport java.util.LinkedHashMap;\nimport java.util.Map;\nimport java.util.Map.Entry;\nimport java.util.function.Function;\n\nimport org.springframework.util.Assert;\n\n/**\n * {@link NamedContributors} backed by a map with values adapted as necessary.\n *\n * @param <V> the value type\n * @param <C> the contributor type\n * @author Phillip Webb\n * @author Guirong Hu\n * @see CompositeHealthContributorMapAdapter\n * @see CompositeReactiveHealthContributorMapAdapter\n */\nabstract class NamedContributorsMapAdapter<V, C> implements NamedContributors<C> {\n\n\tprivate final Map<String, C> map;\n\n\tNamedContributorsMapAdapter(Map<String, V> map, Function<V, ? extends C> valueAdapter) {\n\t\tAssert.notNull(map, \"Map must not be null\");\n\t\tAssert.notNull(valueAdapter, \"ValueAdapter must not be null\");\n\t\tmap.keySet().forEach(this::validateKey);\n\t\tthis.map = Collections.unmodifiableMap(map.entrySet()\n\t\t\t.stream()\n\t\t\t.collect(LinkedHashMap::new,\n\t\t\t\t\t(result, entry) -> result.put(entry.getKey(), adapt(entry.getValue(), valueAdapter)), Map::putAll));\n\n\t}\n\n\tprivate void validateKey(String value) {\n\t\tAssert.notNull(value, \"Map must not contain null keys\");\n\t\tAssert.isTrue(!value.contains(\"/\"), \"Map keys must not contain a '/'\");\n\t}\n\n\tprivate C adapt(V value, Function<V, ? extends C> valueAdapter) {\n\t\tC contributor = (value != null) ? valueAdapter.apply(value) : null;\n\t\tAssert.notNull(contributor, \"Map must not contain null values\");\n\t\treturn contributor;\n\t}\n\n\t@Override\n\tpublic Iterator<NamedContributor<C>> iterator() {\n\t\tIterator<Entry<String, C>> iterator = this.map.entrySet().iterator();\n\t\treturn new Iterator<>() {\n\n\t\t\t@Override\n\t\t\tpublic boolean hasNext() {\n\t\t\t\treturn iterator.hasNext();\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic NamedContributor<C> next() {\n\t\t\t\tEntry<String, C> entry = iterator.next();\n\t\t\t\treturn NamedContributor.of(entry.getKey(), entry.getValue());\n\t\t\t}\n\n\t\t};\n\t}\n\n\t@Override",
      "\nimport org.flywaydb.core.api.configuration.FluentConfiguration;\n\n/**\n * A Flyway customizer which gets replaced with\n * {@link NativeImageResourceProviderCustomizer} when running in a native image.\n *\n * @author Moritz Halbritter\n */\nclass ResourceProviderCustomizer {\n\n\tvoid customize(FluentConfiguration configuration) {\n\t}\n\n}\n",
      "\t\t.withConfiguration(AutoConfigurations.of(ValidationAutoConfiguration.class));\n\n\t@Test\n\tvoid validationIsDisabled() {\n\t\tthis.contextRunner.run((context) -> {\n\t\t\tassertThat(context).doesNotHaveBean(Validator.class);\n\t\t\tassertThat(context).doesNotHaveBean(MethodValidationPostProcessor.class);\n\t\t});\n\t}\n\n}\n",
      " * Copyright 2012-2019 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.boot.context.properties.bind;\n\nimport org.springframework.boot.context.properties.source.ConfigurationProperty;\nimport org.springframework.boot.context.properties.source.ConfigurationPropertyName;\nimport org.springframework.boot.origin.Origin;\nimport org.springframework.boot.origin.OriginProvider;\n\n/**\n * Exception thrown when binding fails.\n *\n * @author Phillip Webb\n * @author Madhura Bhave\n * @since 2.0.0\n */\npublic class BindException extends RuntimeException implements OriginProvider {\n\n\tprivate final Bindable<?> target;\n\n\tprivate final ConfigurationProperty property;\n\n\tprivate final ConfigurationPropertyName name;\n\n\tBindException(ConfigurationPropertyName name, Bindable<?> target, ConfigurationProperty property, Throwable cause) {\n\t\tsuper(buildMessage(name, target), cause);\n\t\tthis.name = name;\n\t\tthis.target = target;\n\t\tthis.property = property;\n\t}\n\n\t/**\n\t * Return the name of the configuration property being bound.\n\t * @return the configuration property name\n\t */\n\tpublic ConfigurationPropertyName getName() {\n\t\treturn this.name;\n\t}\n\n\t/**\n\t * Return the target being bound.\n\t * @return the bind target\n\t */\n\tpublic Bindable<?> getTarget() {\n\t\treturn this.target;\n\t}\n\n\t/**\n\t * Return the configuration property name of the item that was being bound.\n\t * @return the configuration property name\n\t */"
    ]
  },
  {
    "id": "rails/rails",
    "org": "rails",
    "avatarURL": "https://avatars.githubusercontent.com/u/4223?v=4",
    "name": "rails/rails",
    "url": "https://github.com/rails/rails",
    "lang": "Ruby",
    "desc": "Ruby on Rails, the web-application framework.",
    "star_num": 53440,
    "fork_num": 21298,
    "snippets": [
      "      end\n\n      def unsubscribe(channel, message_callback)\n        raise NotImplementedError\n      end\n\n      def shutdown\n        raise NotImplementedError\n      end\n\n      def identifier\n        @server.config.cable[:id] ||= \"ActionCable-PID-#{$$}\"\n      end\n    end\n  end\nend\n",
      "    initializer \"action_mailbox.deprecator\", before: :load_environment_config do |app|\n      app.deprecators[:action_mailbox] = ActionMailbox.deprecator\n    end\n\n    initializer \"action_mailbox.config\" do\n      config.after_initialize do |app|\n        ActionMailbox.logger = app.config.action_mailbox.logger || Rails.logger\n        ActionMailbox.incinerate = app.config.action_mailbox.incinerate.nil? ? true : app.config.action_mailbox.incinerate\n        ActionMailbox.incinerate_after = app.config.action_mailbox.incinerate_after || 30.days\n        ActionMailbox.queues = app.config.action_mailbox.queues || {}\n        ActionMailbox.ingress = app.config.action_mailbox.ingress\n        ActionMailbox.storage_service = app.config.action_mailbox.storage_service\n      end\n    end\n  end\nend\n",
      "          bird.update! name: \"Robin\"\n        end\n\n        assert_match %r/\\AWrite query attempted while in readonly mode: UPDATE /, error.message\n      end\n    end\n\n    test \"deleting a record raises if preventing writes\" do\n      bird = Bird.create! name: \"Bluejay\"\n\n      ActiveRecord::Base.while_preventing_writes do\n        error = assert_raises ActiveRecord::ReadOnlyError do\n          bird.destroy!\n        end\n\n        assert_match %r/\\AWrite query attempted while in readonly mode: DELETE /, error.message\n      end\n    end\n\n    test \"selecting a record does not raise if preventing writes\" do\n      bird = Bird.create! name: \"Bluejay\"\n\n      ActiveRecord::Base.while_preventing_writes do\n        assert_equal bird, Bird.where(name: \"Bluejay\").last\n      end\n    end\n\n    test \"an explain query does not raise if preventing writes\" do\n      Bird.create!(name: \"Bluejay\")\n\n      ActiveRecord::Base.while_preventing_writes do\n        assert_queries(2) { Bird.where(name: \"Bluejay\").explain }\n      end\n    end\n\n    test \"an empty transaction does not raise if preventing writes\" do\n      ActiveRecord::Base.while_preventing_writes do\n        assert_queries(2, ignore_none: true) do\n          Bird.transaction do\n            ActiveRecord::Base.connection.materialize_transactions\n          end\n        end\n      end\n    end\n\n    test \"preventing writes applies to all connections in block\" do\n      ActiveRecord::Base.while_preventing_writes do\n        conn1_error = assert_raises ActiveRecord::ReadOnlyError do\n          assert_equal ActiveRecord::Base.connection, Bird.connection\n          assert_not_equal ARUnit2Model.connection, Bird.connection\n          Bird.create!(name: \"Bluejay\")\n        end\n\n        assert_match %r/\\AWrite query attempted while in readonly mode: INSERT /, conn1_error.message\n      end\n\n      ActiveRecord::Base.while_preventing_writes do\n        conn2_error = assert_raises ActiveRecord::ReadOnlyError do\n          assert_not_equal ActiveRecord::Base.connection, Professor.connection\n          assert_equal ARUnit2Model.connection, Professor.connection\n          Professor.create!(name: \"Professor Bluejay\")\n        end\n\n        assert_match %r/\\AWrite query attempted while in readonly mode: INSERT /, conn2_error.message",
      "\n    silence_stream($stdout) do\n      TestHelperMailer.with(mail_params).test_parameter_args.deliver_later\n    end\n\n    matcher_params = nil\n\n    assert_nothing_raised do\n      assert_enqueued_email_with TestHelperMailer, :test_parameter_args, params: ->(params) { matcher_params = params }\n    end\n\n    assert_equal mail_params, matcher_params\n\n    assert_raises ActiveSupport::TestCase::Assertion do\n      assert_enqueued_email_with TestHelperMailer, :test_parameter_args, params: ->(_) { false }\n    end\n  end\n\n  def test_assert_enqueued_email_with_supports_args_matcher_proc\n    mail_args = [\"some_email\", \"some_name\"]\n\n    silence_stream($stdout) do\n      TestHelperMailer.test_args(*mail_args).deliver_later\n    end\n\n    matcher_args = nil\n\n    assert_nothing_raised do\n      assert_enqueued_email_with TestHelperMailer, :test_args, args: ->(args) { matcher_args = args }\n    end\n\n    assert_equal mail_args, matcher_args\n\n    assert_raises ActiveSupport::TestCase::Assertion do\n      assert_enqueued_email_with TestHelperMailer, :test_args, args: ->(_) { false }\n    end\n  end\n\n  def test_assert_enqueued_email_with_supports_named_args_matcher_proc\n    mail_args = [{ email: \"some_email\", name: \"some_name\" }]\n\n    silence_stream($stdout) do\n      TestHelperMailer.test_named_args(**mail_args[0]).deliver_later\n    end\n\n    matcher_args = nil\n\n    assert_nothing_raised do\n      assert_enqueued_email_with TestHelperMailer, :test_named_args, args: ->(args) { matcher_args = args }\n    end\n\n    assert_equal mail_args, matcher_args\n  end\n\n  def test_deliver_enqueued_emails_with_no_block\n    assert_nothing_raised do\n      silence_stream($stdout) do\n        TestHelperMailer.test.deliver_later\n        deliver_enqueued_emails\n      end\n    end\n\n    assert_emails(1)\n  end",
      "class ActiveRecord::Encryption::EncryptorTest < ActiveRecord::EncryptionTestCase\n  setup do\n    @secret_key = \"This is my secret 256 bits key!!\"\n    @encryptor = ActiveRecord::Encryption::Encryptor.new\n  end\n\n  test \"encrypt and decrypt a string\" do\n    assert_encrypt_text(\"my secret text\")\n  end\n\n  test \"decrypt and invalid string will raise a Decryption error\" do\n    assert_raises(ActiveRecord::Encryption::Errors::Decryption) do\n      @encryptor.decrypt(\"some test that does not make sense\")\n    end\n  end\n\n  test \"decrypt an encrypted text with an invalid key will raise a Decryption error\" do\n    assert_raises(ActiveRecord::Encryption::Errors::Decryption) do\n      encrypted_text = @encryptor.encrypt(\"Some text to encrypt\")\n      @encryptor.decrypt(encrypted_text, key_provider: ActiveRecord::Encryption::DerivedSecretKeyProvider.new(\"some invalid key\"))\n    end\n  end\n\n  test \"if an encryption error happens when encrypting an encrypted text it should raise\" do\n    assert_raises(ActiveRecord::Encryption::Errors::Encryption) do\n      key_provider_that_raises_an_encryption_error = ActiveRecord::Encryption::DerivedSecretKeyProvider.new(\"some key\")\n      key_provider_that_raises_an_encryption_error.stub :encryption_key, -> { raise ActiveRecord::Encryption::Errors::Encryption } do\n        @encryptor.encrypt(\"Some text to encrypt\", key_provider: key_provider_that_raises_an_encryption_error)\n      end\n    end\n  end\n\n  test \"content is compressed\" do\n    content = SecureRandom.hex(5.kilobytes)\n    cipher_text = @encryptor.encrypt(content)\n\n    assert_encrypt_text content\n    assert cipher_text.bytesize < content.bytesize\n  end\n\n  test \"trying to encrypt custom classes raises a ForbiddenClass exception\" do\n    assert_raises ActiveRecord::Encryption::Errors::ForbiddenClass do\n      @encryptor.encrypt(Struct.new(:name).new(\"Jorge\"))\n    end\n  end\n\n  test \"store custom metadata with the encrypted data, accessible by the key provider\" do\n    key = ActiveRecord::Encryption::Key.new(@secret_key)\n    key.public_tags[:key] = \"my tag\"\n    key_provider = ActiveRecord::Encryption::KeyProvider.new(key)\n    encryptor = ActiveRecord::Encryption::Encryptor.new\n\n    key_provider.stub :decryption_keys, ->(message) { [key] } do\n      encryptor.decrypt encryptor.encrypt(\"some text\", key_provider: key_provider), key_provider: key_provider\n    end\n  end\n\n  test \"encrypted? returns whether the passed text is encrypted\" do\n    assert @encryptor.encrypted?(@encryptor.encrypt(\"clean text\"))\n    assert_not @encryptor.encrypted?(\"clean text\")\n  end\n\n  test \"decrypt respects encoding even when compression is used\" do\n    text = \"The Starfleet is here #{'OMG! ' * 50}!\".dup.force_encoding(Encoding::ISO_8859_1)",
      "  end\n\n  test \"<ol> tags are separated by two new lines\" do\n    assert_converted_to(\n      \"Hello world!\\n\\n1. list1\\n\\n1. list2\\n\\nHow are you?\",\n      \"<p>Hello world!</p><ol><li>list1</li></ol><ol><li>list2</li></ol><p>How are you?</p>\"\n    )\n  end\n\n  test \"<ul> tags are separated by two new lines\" do\n    assert_converted_to(\n      \"Hello world!\\n\\n• list1\\n\\n• list2\\n\\nHow are you?\",\n      \"<p>Hello world!</p><ul><li>list1</li></ul><ul><li>list2</li></ul><p>How are you?</p>\"\n    )\n  end\n\n  test \"<h1> tags are separated by two new lines\" do\n    assert_converted_to(\n      \"Hello world!\\n\\nHow are you?\",\n      \"<h1>Hello world!</h1><div>How are you?</div>\"\n    )\n  end\n\n  test \"<li> tags are separated by one new line\" do\n    assert_converted_to(\n      \"• one\\n• two\\n• three\",\n      \"<ul><li>one</li><li>two</li><li>three</li></ul>\"\n    )\n  end\n\n  test \"<li> tags without a parent list\" do\n    assert_converted_to(\n      \"• one\\n• two\\n• three\",\n      \"<li>one</li><li>two</li><li>three</li>\"\n    )\n  end\n\n  test \"basic nested <ul> tags are indented\" do\n    assert_converted_to(\n      \"• Item 1\\n  • Item 2\",\n      \"<ul><li>Item 1<ul><li>Item 2</li></ul></li></ul>\"\n    )\n  end\n\n  test \"basic nested <ol> tags are indented\" do\n    assert_converted_to(\n      \"1. Item 1\\n  1. Item 2\",\n      \"<ol><li>Item 1<ol><li>Item 2</li></ol></li></ol>\"\n    )\n  end\n\n  test \"complex nested / mixed list tags are indented\" do\n    assert_converted_to(\n      \"• Item 0\\n• Item 1\\n  • Item A\\n    1. Item i\\n    2. Item ii\\n  • Item B\\n    • Item i\\n• Item 2\",\n      \"<ul><li>Item 0</li><li>Item 1<ul><li>Item A<ol><li>Item i</li><li>Item ii</li></ol></li><li>Item B<ul><li>Item i</li></ul></li></ul></li><li>Item 2</li></ul>\"\n    )\n  end\n\n  test \"<br> tags are separated by one new line\" do\n    assert_converted_to(\n      \"Hello world!\\none\\ntwo\\nthree\",\n      \"<p>Hello world!<br>one<br>two<br>three</p>\"\n    )\n  end",
      "  before_destroy do\n    @destroy_count += 1\n    if @destroy_count == 1\n      throw :abort\n    end\n  end\nend\n\nclass ContentPosition < ActiveRecord::Base\n  belongs_to :content, dependent: :destroy\n\n  def self.destroyed_ids\n    @destroyed_ids ||= []\n  end\n\n  before_destroy do |object|\n    ContentPosition.destroyed_ids << object.id\n  end\nend\n",
      "      end\n    end\n  end\nend\n",
      "      #\n      #   simple_format(\"<blink>Blinkable!</blink> It's true.\", {}, sanitize: false)\n      #   # => \"<p><blink>Blinkable!</blink> It's true.</p>\"\n      #\n      #   simple_format(\"<a target=\\\"_blank\\\" href=\\\"http://example.com\\\">Continue</a>\", {}, { sanitize_options: { attributes: %w[target href] } })\n      #   # => \"<p><a target=\\\"_blank\\\" href=\\\"http://example.com\\\">Continue</a></p>\"\n      def simple_format(text, html_options = {}, options = {})\n        wrapper_tag = options[:wrapper_tag] || \"p\"\n\n        text = sanitize(text, options.fetch(:sanitize_options, {})) if options.fetch(:sanitize, true)\n        paragraphs = split_paragraphs(text)\n\n        if paragraphs.empty?\n          content_tag(wrapper_tag, nil, html_options)\n        else\n          paragraphs.map! { |paragraph|\n            content_tag(wrapper_tag, raw(paragraph), html_options)\n          }.join(\"\\n\\n\").html_safe\n        end\n      end\n\n      # Creates a Cycle object whose _to_s_ method cycles through elements of an\n      # array every time it is called. This can be used for example, to alternate\n      # classes for table rows. You can use named cycles to allow nesting in loops.\n      # Passing a Hash as the last parameter with a <tt>:name</tt> key will create a\n      # named cycle. The default name for a cycle without a +:name+ key is\n      # <tt>\"default\"</tt>. You can manually reset a cycle by calling reset_cycle\n      # and passing the name of the cycle. The current cycle string can be obtained\n      # anytime using the current_cycle method.\n      #\n      #   # Alternate CSS classes for even and odd numbers...\n      #   @items = [1,2,3,4]\n      #   <table>\n      #   <% @items.each do |item| %>\n      #     <tr class=\"<%= cycle(\"odd\", \"even\") -%>\">\n      #       <td><%= item %></td>\n      #     </tr>\n      #   <% end %>\n      #   </table>\n      #\n      #\n      #   # Cycle CSS classes for rows, and text colors for values within each row\n      #   @items = x = [{first: 'Robert', middle: 'Daniel', last: 'James'},\n      #                {first: 'Emily', middle: 'Shannon', maiden: 'Pike', last: 'Hicks'},\n      #               {first: 'June', middle: 'Dae', last: 'Jones'}]\n      #   <% @items.each do |item| %>\n      #     <tr class=\"<%= cycle(\"odd\", \"even\", name: \"row_class\") -%>\">\n      #       <td>\n      #         <% item.values.each do |value| %>\n      #           <%# Create a named cycle \"colors\" %>\n      #           <span style=\"color:<%= cycle(\"red\", \"green\", \"blue\", name: \"colors\") -%>\">\n      #             <%= value %>\n      #           </span>\n      #         <% end %>\n      #         <% reset_cycle(\"colors\") %>\n      #       </td>\n      #    </tr>\n      #  <% end %>\n      def cycle(first_value, *values)\n        options = values.extract_options!\n        name = options.fetch(:name, \"default\")\n\n        values.unshift(*first_value)\n",
      "# frozen_string_literal: true\n\nrequire \"cases/helper\"\nrequire \"active_record/connection_adapters/postgresql/utils\"\n\nclass PostgreSQLUtilsTest < ActiveRecord::PostgreSQLTestCase\n  Name = ActiveRecord::ConnectionAdapters::PostgreSQL::Name\n  include ActiveRecord::ConnectionAdapters::PostgreSQL::Utils\n\n  def test_extract_schema_qualified_name\n    {\n      %(table_name)            => [nil, \"table_name\"],\n      %(\"table.name\")          => [nil, \"table.name\"],\n      %(schema.table_name)     => %w{schema table_name},\n      %(\"schema\".table_name)   => %w{schema table_name},\n      %(schema.\"table_name\")   => %w{schema table_name},\n      %(\"schema\".\"table_name\") => %w{schema table_name},\n      %(\"even spaces\".table)   => [\"even spaces\", \"table\"],\n      %(schema.\"table.name\")   => [\"schema\", \"table.name\"]\n    }.each do |given, expect|\n      assert_equal Name.new(*expect), extract_schema_qualified_name(given)\n    end\n  end\nend\n\nclass PostgreSQLNameTest < ActiveRecord::PostgreSQLTestCase\n  Name = ActiveRecord::ConnectionAdapters::PostgreSQL::Name\n\n  test \"represents itself as schema.name\" do\n    obj = Name.new(\"public\", \"articles\")\n    assert_equal \"public.articles\", obj.to_s\n  end\n\n  test \"without schema, represents itself as name only\" do\n    obj = Name.new(nil, \"articles\")\n    assert_equal \"articles\", obj.to_s\n  end\n\n  test \"quoted returns a string representation usable in a query\" do\n    assert_equal %(\"articles\"), Name.new(nil, \"articles\").quoted\n    assert_equal %(\"public\".\"articles\"), Name.new(\"public\", \"articles\").quoted\n  end\n\n  test \"prevents double quoting\" do\n    name = Name.new('\"quoted_schema\"', '\"quoted_table\"')\n    assert_equal \"quoted_schema.quoted_table\", name.to_s\n    assert_equal %(\"quoted_schema\".\"quoted_table\"), name.quoted\n  end\n\n  test \"equality based on state\" do\n    assert_equal Name.new(\"access\", \"users\"), Name.new(\"access\", \"users\")\n    assert_equal Name.new(nil, \"users\"), Name.new(nil, \"users\")\n    assert_not_equal Name.new(nil, \"users\"), Name.new(\"access\", \"users\")\n    assert_not_equal Name.new(\"access\", \"users\"), Name.new(\"public\", \"users\")\n    assert_not_equal Name.new(\"public\", \"users\"), Name.new(\"public\", \"articles\")\n  end\n\n  test \"can be used as hash key\" do\n    hash = { Name.new(\"schema\", \"article_seq\") => \"success\" }\n    assert_equal \"success\", hash[Name.new(\"schema\", \"article_seq\")]\n    assert_nil hash[Name.new(\"schema\", \"articles\")]\n    assert_nil hash[Name.new(\"public\", \"article_seq\")]\n  end\nend"
    ]
  },
  {
    "id": "django/django",
    "org": "django",
    "avatarURL": "https://avatars.githubusercontent.com/u/27804?v=4",
    "name": "django/django",
    "url": "https://github.com/django/django",
    "lang": "Python",
    "desc": "The web framework for perfectionists with deadlines.",
    "star_num": 72889,
    "fork_num": 29866,
    "snippets": [
      "\n\nclass FieldOperation(Operation):\n    def __init__(self, model_name, name, field=None):\n        self.model_name = model_name\n        self.name = name\n        self.field = field\n\n    @cached_property\n    def model_name_lower(self):\n        return self.model_name.lower()\n\n    @cached_property\n    def name_lower(self):\n        return self.name.lower()\n\n    def is_same_model_operation(self, operation):\n        return self.model_name_lower == operation.model_name_lower\n\n    def is_same_field_operation(self, operation):\n        return (\n            self.is_same_model_operation(operation)\n            and self.name_lower == operation.name_lower\n        )\n\n    def references_model(self, name, app_label):\n        name_lower = name.lower()\n        if name_lower == self.model_name_lower:\n            return True\n        if self.field:\n            return bool(\n                field_references(\n                    (app_label, self.model_name_lower),\n                    self.field,\n                    (app_label, name_lower),\n                )\n            )\n        return False\n\n    def references_field(self, model_name, name, app_label):\n        model_name_lower = model_name.lower()\n        # Check if this operation locally references the field.\n        if model_name_lower == self.model_name_lower:\n            if name == self.name:\n                return True\n            elif (\n                self.field\n                and hasattr(self.field, \"from_fields\")\n                and name in self.field.from_fields\n            ):\n                return True\n        # Check if this operation remotely references the field.\n        if self.field is None:\n            return False\n        return bool(\n            field_references(\n                (app_label, self.model_name_lower),\n                self.field,\n                (app_label, model_name_lower),\n                name,\n            )\n        )\n\n    def reduce(self, operation, app_label):",
      "# Getting the semi_major, semi_minor, and flattening functions.\nsemi_major = srs_double(lgdal.OSRGetSemiMajor)\nsemi_minor = srs_double(lgdal.OSRGetSemiMinor)\ninvflattening = srs_double(lgdal.OSRGetInvFlattening)\n\n# WKT, PROJ, EPSG, XML importation routines.\nfrom_wkt = void_output(lgdal.OSRImportFromWkt, [c_void_p, POINTER(c_char_p)])\nfrom_proj = void_output(lgdal.OSRImportFromProj4, [c_void_p, c_char_p])\nfrom_epsg = void_output(std_call(\"OSRImportFromEPSG\"), [c_void_p, c_int])\nfrom_xml = void_output(lgdal.OSRImportFromXML, [c_void_p, c_char_p])\nfrom_user_input = void_output(std_call(\"OSRSetFromUserInput\"), [c_void_p, c_char_p])\n\n# Morphing to/from ESRI WKT.\nmorph_to_esri = void_output(lgdal.OSRMorphToESRI, [c_void_p])\nmorph_from_esri = void_output(lgdal.OSRMorphFromESRI, [c_void_p])\n\n# Identifying the EPSG\nidentify_epsg = void_output(lgdal.OSRAutoIdentifyEPSG, [c_void_p])\n\n# Getting the angular_units, linear_units functions\nlinear_units = units_func(lgdal.OSRGetLinearUnits)\nangular_units = units_func(lgdal.OSRGetAngularUnits)\n\n# For exporting to WKT, PROJ, \"Pretty\" WKT, and XML.\nto_wkt = string_output(\n    std_call(\"OSRExportToWkt\"), [c_void_p, POINTER(c_char_p)], decoding=\"utf-8\"\n)\nto_proj = string_output(\n    std_call(\"OSRExportToProj4\"), [c_void_p, POINTER(c_char_p)], decoding=\"ascii\"\n)\nto_pretty_wkt = string_output(\n    std_call(\"OSRExportToPrettyWkt\"),\n    [c_void_p, POINTER(c_char_p), c_int],\n    offset=-2,\n    decoding=\"utf-8\",\n)\n\nto_xml = string_output(\n    lgdal.OSRExportToXML,\n    [c_void_p, POINTER(c_char_p), c_char_p],\n    offset=-2,\n    decoding=\"utf-8\",\n)\n\n# String attribute retrieval routines.\nget_attr_value = const_string_output(\n    std_call(\"OSRGetAttrValue\"), [c_void_p, c_char_p, c_int], decoding=\"utf-8\"\n)\nget_auth_name = const_string_output(\n    lgdal.OSRGetAuthorityName, [c_void_p, c_char_p], decoding=\"ascii\"\n)\nget_auth_code = const_string_output(\n    lgdal.OSRGetAuthorityCode, [c_void_p, c_char_p], decoding=\"ascii\"\n)\n\n# SRS Properties\nisgeographic = int_output(lgdal.OSRIsGeographic, [c_void_p])\nislocal = int_output(lgdal.OSRIsLocal, [c_void_p])\nisprojected = int_output(lgdal.OSRIsProjected, [c_void_p])\n\n# Coordinate transformation\nnew_ct = srs_output(std_call(\"OCTNewCoordinateTransformation\"), [c_void_p, c_void_p])\ndestroy_ct = void_output(\n    std_call(\"OCTDestroyCoordinateTransformation\"), [c_void_p], errcheck=False",
      "    RestrictedError,\n)\nfrom django.db.models.enums import *  # NOQA\nfrom django.db.models.enums import __all__ as enums_all\nfrom django.db.models.expressions import (\n    Case,\n    Exists,\n    Expression,\n    ExpressionList,\n    ExpressionWrapper,\n    F,\n    Func,\n    OrderBy,\n    OuterRef,\n    RowRange,\n    Subquery,\n    Value,\n    ValueRange,\n    When,\n    Window,\n    WindowFrame,\n)\nfrom django.db.models.fields import *  # NOQA\nfrom django.db.models.fields import __all__ as fields_all\nfrom django.db.models.fields.files import FileField, ImageField\nfrom django.db.models.fields.json import JSONField\nfrom django.db.models.fields.proxy import OrderWrt\nfrom django.db.models.indexes import *  # NOQA\nfrom django.db.models.indexes import __all__ as indexes_all\nfrom django.db.models.lookups import Lookup, Transform\nfrom django.db.models.manager import Manager\nfrom django.db.models.query import Prefetch, QuerySet, prefetch_related_objects\nfrom django.db.models.query_utils import FilteredRelation, Q\n\n# Imports that would create circular imports if sorted\nfrom django.db.models.base import DEFERRED, Model  # isort:skip\nfrom django.db.models.fields.related import (  # isort:skip\n    ForeignKey,\n    ForeignObject,\n    OneToOneField,\n    ManyToManyField,\n    ForeignObjectRel,\n    ManyToOneRel,\n    ManyToManyRel,\n    OneToOneRel,\n)\n\n\n__all__ = aggregates_all + constraints_all + enums_all + fields_all + indexes_all\n__all__ += [\n    \"ObjectDoesNotExist\",\n    \"signals\",\n    \"CASCADE\",\n    \"DO_NOTHING\",\n    \"PROTECT\",\n    \"RESTRICT\",\n    \"SET\",\n    \"SET_DEFAULT\",\n    \"SET_NULL\",\n    \"ProtectedError\",\n    \"RestrictedError\",\n    \"Case\",\n    \"Exists\",\n    \"Expression\",",
      "    def test_center02(self):\n        output = self.engine.render_to_string(\n            \"center02\", {\"a\": \"a&b\", \"b\": mark_safe(\"a&b\")}\n        )\n        self.assertEqual(output, \". a&amp;b . . a&b .\")\n\n\nclass FunctionTests(SimpleTestCase):\n    def test_center(self):\n        self.assertEqual(center(\"test\", 6), \" test \")\n\n    def test_non_string_input(self):\n        self.assertEqual(center(123, 5), \" 123 \")\n",
      "        404.\n        \"\"\"\n        for obj in Article.objects.all():\n            with self.subTest(obj=obj):\n                short_url = \"/shortcut/%s/%s/\" % (\n                    ContentType.objects.get_for_model(Article).id,\n                    obj.pk,\n                )\n                response = self.client.get(short_url)\n                self.assertEqual(response.status_code, 404)\n\n    def test_wrong_type_pk(self):\n        short_url = \"/shortcut/%s/%s/\" % (\n            ContentType.objects.get_for_model(Author).id,\n            \"nobody/expects\",\n        )\n        response = self.client.get(short_url)\n        self.assertEqual(response.status_code, 404)\n\n    def test_shortcut_bad_pk(self):\n        short_url = \"/shortcut/%s/%s/\" % (\n            ContentType.objects.get_for_model(Author).id,\n            \"42424242\",\n        )\n        response = self.client.get(short_url)\n        self.assertEqual(response.status_code, 404)\n\n    def test_nonint_content_type(self):\n        an_author = Author.objects.all()[0]\n        short_url = \"/shortcut/%s/%s/\" % (\"spam\", an_author.pk)\n        response = self.client.get(short_url)\n        self.assertEqual(response.status_code, 404)\n\n    def test_bad_content_type(self):\n        an_author = Author.objects.all()[0]\n        short_url = \"/shortcut/%s/%s/\" % (42424242, an_author.pk)\n        response = self.client.get(short_url)\n        self.assertEqual(response.status_code, 404)\n\n\n@override_settings(ROOT_URLCONF=\"contenttypes_tests.urls\")\nclass ContentTypesViewsSiteRelTests(TestCase):\n    def setUp(self):\n        Site.objects.clear_cache()\n\n    @classmethod\n    def setUpTestData(cls):\n        cls.site_2 = Site.objects.create(domain=\"example2.com\", name=\"example2.com\")\n        cls.site_3 = Site.objects.create(domain=\"example3.com\", name=\"example3.com\")\n\n    @mock.patch(\"django.apps.apps.get_model\")\n    def test_shortcut_view_with_null_site_fk(self, get_model):\n        \"\"\"\n        The shortcut view works if a model's ForeignKey to site is None.\n        \"\"\"\n        get_model.side_effect = (\n            lambda *args, **kwargs: MockSite\n            if args[0] == \"sites.Site\"\n            else ModelWithNullFKToSite\n        )\n\n        obj = ModelWithNullFKToSite.objects.create(title=\"title\")\n        url = \"/shortcut/%s/%s/\" % (\n            ContentType.objects.get_for_model(ModelWithNullFKToSite).id,",
      "            with self.subTest():\n                Person.objects.filter(first_name=\"subtest-error\").count()\n                raise Exception\n\n    def _test_output(self, verbosity):\n        runner = DiscoverRunner(debug_sql=True, verbosity=0)\n        suite = runner.test_suite()\n        suite.addTest(self.FailingTest())\n        suite.addTest(self.ErrorTest())\n        suite.addTest(self.PassingTest())\n        suite.addTest(self.PassingSubTest())\n        suite.addTest(self.FailingSubTest())\n        suite.addTest(self.ErrorSubTest())\n        old_config = runner.setup_databases()\n        stream = StringIO()\n        resultclass = runner.get_resultclass()\n        runner.test_runner(\n            verbosity=verbosity,\n            stream=stream,\n            resultclass=resultclass,\n        ).run(suite)\n        runner.teardown_databases(old_config)\n\n        return stream.getvalue()\n\n    def test_output_normal(self):\n        full_output = self._test_output(1)\n        for output in self.expected_outputs:\n            self.assertIn(output, full_output)\n        for output in self.verbose_expected_outputs:\n            self.assertNotIn(output, full_output)\n\n    def test_output_verbose(self):\n        full_output = self._test_output(2)\n        for output in self.expected_outputs:\n            self.assertIn(output, full_output)\n        for output in self.verbose_expected_outputs:\n            self.assertIn(output, full_output)\n\n    expected_outputs = [\n        (\n            \"\"\"SELECT COUNT(*) AS \"__count\"\\n\"\"\"\n            \"\"\"FROM \"test_runner_person\"\\n\"\"\"\n            \"\"\"WHERE \"test_runner_person\".\"first_name\" = 'error';\"\"\"\n        ),\n        (\n            \"\"\"SELECT COUNT(*) AS \"__count\"\\n\"\"\"\n            \"\"\"FROM \"test_runner_person\"\\n\"\"\"\n            \"\"\"WHERE \"test_runner_person\".\"first_name\" = 'fail';\"\"\"\n        ),\n        (\n            \"\"\"SELECT COUNT(*) AS \"__count\"\\n\"\"\"\n            \"\"\"FROM \"test_runner_person\"\\n\"\"\"\n            \"\"\"WHERE \"test_runner_person\".\"first_name\" = 'subtest-error';\"\"\"\n        ),\n        (\n            \"\"\"SELECT COUNT(*) AS \"__count\"\\n\"\"\"\n            \"\"\"FROM \"test_runner_person\"\\n\"\"\"\n            \"\"\"WHERE \"test_runner_person\".\"first_name\" = 'subtest-fail';\"\"\"\n        ),\n    ]\n\n    # Python 3.11 uses fully qualified test name in the output.\n    method_name = \".runTest\" if PY311 else \"\"",
      "            },\n            setting: [setting_path] if setting == \"STATICFILES_DIRS\" else setting_path,\n        }\n\n    def test_cache_path_matches_media_static_setting(self):\n        root = pathlib.Path.cwd()\n        for setting in (\"MEDIA_ROOT\", \"STATIC_ROOT\", \"STATICFILES_DIRS\"):\n            settings = self.get_settings(setting, root, root)\n            with self.subTest(setting=setting), self.settings(**settings):\n                msg = self.warning_message % (\"matches\", setting)\n                self.assertEqual(\n                    check_cache_location_not_exposed(None),\n                    [\n                        Warning(msg, id=\"caches.W002\"),\n                    ],\n                )\n\n    def test_cache_path_inside_media_static_setting(self):\n        root = pathlib.Path.cwd()\n        for setting in (\"MEDIA_ROOT\", \"STATIC_ROOT\", \"STATICFILES_DIRS\"):\n            settings = self.get_settings(setting, root / \"cache\", root)\n            with self.subTest(setting=setting), self.settings(**settings):\n                msg = self.warning_message % (\"is inside\", setting)\n                self.assertEqual(\n                    check_cache_location_not_exposed(None),\n                    [\n                        Warning(msg, id=\"caches.W002\"),\n                    ],\n                )\n\n    def test_cache_path_contains_media_static_setting(self):\n        root = pathlib.Path.cwd()\n        for setting in (\"MEDIA_ROOT\", \"STATIC_ROOT\", \"STATICFILES_DIRS\"):\n            settings = self.get_settings(setting, root, root / \"other\")\n            with self.subTest(setting=setting), self.settings(**settings):\n                msg = self.warning_message % (\"contains\", setting)\n                self.assertEqual(\n                    check_cache_location_not_exposed(None),\n                    [\n                        Warning(msg, id=\"caches.W002\"),\n                    ],\n                )\n\n    def test_cache_path_not_conflict(self):\n        root = pathlib.Path.cwd()\n        for setting in (\"MEDIA_ROOT\", \"STATIC_ROOT\", \"STATICFILES_DIRS\"):\n            settings = self.get_settings(setting, root / \"cache\", root / \"other\")\n            with self.subTest(setting=setting), self.settings(**settings):\n                self.assertEqual(check_cache_location_not_exposed(None), [])\n\n    def test_staticfiles_dirs_prefix(self):\n        root = pathlib.Path.cwd()\n        tests = [\n            (root, root, \"matches\"),\n            (root / \"cache\", root, \"is inside\"),\n            (root, root / \"other\", \"contains\"),\n        ]\n        for cache_path, setting_path, msg in tests:\n            settings = self.get_settings(\n                \"STATICFILES_DIRS\",\n                cache_path,\n                (\"prefix\", setting_path),\n            )\n            with self.subTest(path=setting_path), self.settings(**settings):",
      "        ),\n    ]\n",
      "    def __str__(self):\n        return self.common_name\n\n\nclass Vegetable(models.Model):\n    name = models.CharField(max_length=150)\n    is_yucky = models.BooleanField(default=True)\n\n    tags = GenericRelation(TaggedItem)\n\n    def __str__(self):\n        return self.name\n\n\nclass Carrot(Vegetable):\n    pass\n\n\nclass Mineral(models.Model):\n    name = models.CharField(max_length=150)\n    hardness = models.PositiveSmallIntegerField()\n\n    # note the lack of an explicit GenericRelation here...\n\n    def __str__(self):\n        return self.name\n\n\nclass GeckoManager(models.Manager):\n    def get_queryset(self):\n        return super().get_queryset().filter(has_tail=True)\n\n\nclass Gecko(models.Model):\n    has_tail = models.BooleanField(default=False)\n    objects = GeckoManager()\n\n\n# To test fix for #11263\nclass Rock(Mineral):\n    tags = GenericRelation(TaggedItem)\n\n\nclass ValuableRock(Mineral):\n    tags = GenericRelation(ValuableTaggedItem)\n\n\nclass ManualPK(models.Model):\n    id = models.IntegerField(primary_key=True)\n    tags = GenericRelation(TaggedItem, related_query_name=\"manualpk\")\n\n\nclass ForProxyModelModel(models.Model):\n    content_type = models.ForeignKey(ContentType, models.CASCADE)\n    object_id = models.PositiveIntegerField()\n    obj = GenericForeignKey(for_concrete_model=False)\n    title = models.CharField(max_length=255, null=True)\n\n\nclass ForConcreteModelModel(models.Model):\n    content_type = models.ForeignKey(ContentType, models.CASCADE)\n    object_id = models.PositiveIntegerField()\n    obj = GenericForeignKey()\n"
    ]
  },
  {
    "id": "flutter/flutter",
    "org": "flutter",
    "avatarURL": "https://avatars.githubusercontent.com/u/14101776?v=4",
    "name": "flutter/flutter",
    "url": "https://github.com/flutter/flutter",
    "lang": "Dart",
    "desc": "Flutter is Google's UI toolkit for building natively compiled applications for mobile, web, and desktop from a single codebase.",
    "star_num": 156480,
    "fork_num": 25949,
    "snippets": [
      "  /// there should be the following string resource:\n  ///\n  ///   <string name=\"component1Name\">component1</string>\n  ///\n  /// The string element's name attribute should be the component name with\n  /// `Name` as a suffix, and the text contents should be the component name.\n  bool checkAndroidResourcesStrings(List<DeferredComponent> components) {\n    final Directory androidDir = projectDir.childDirectory('android');\n    inputs.add(projectDir.childFile('pubspec.yaml'));\n\n    // Add component name mapping to strings.xml\n    final File stringRes = androidDir\n      .childDirectory('app')\n      .childDirectory('src')\n      .childDirectory('main')\n      .childDirectory('res')\n      .childDirectory('values')\n      .childFile('strings.xml');\n    inputs.add(stringRes);\n    final File stringResOutput = outputDir\n      .childDirectory('app')\n      .childDirectory('src')\n      .childDirectory('main')\n      .childDirectory('res')\n      .childDirectory('values')\n      .childFile('strings.xml');\n    ErrorHandlingFileSystem.deleteIfExists(stringResOutput);\n    if (components.isEmpty) {\n      return true;\n    }\n    final Map<String, String> requiredEntriesMap  = <String, String>{};\n    for (final DeferredComponent component in components) {\n      requiredEntriesMap['${component.name}Name'] = component.name;\n    }\n    if (stringRes.existsSync()) {\n      bool modified = false;\n      XmlDocument document;\n      try {\n        document = XmlDocument.parse(stringRes.readAsStringSync());\n      } on XmlException {\n        invalidFiles[stringRes.path] = 'Error parsing $stringRes '\n        'Please ensure that the strings.xml is a valid XML document and '\n        'try again.';\n        return false;\n      }\n      // Check if all required lines are present, and fix if name exists, but\n      // wrong string stored.\n      for (final XmlElement resources in document.findAllElements('resources')) {\n        for (final XmlElement element in resources.findElements('string')) {\n          final String? name = element.getAttribute('name');\n          if (requiredEntriesMap.containsKey(name)) {\n            if (element.innerText != requiredEntriesMap[name]) {\n              element.innerText = requiredEntriesMap[name]!;\n              modified = true;\n            }\n            requiredEntriesMap.remove(name);\n          }\n        }\n        requiredEntriesMap.forEach((String key, String value) {\n          modified = true;\n          final XmlElement newStringElement = XmlElement(\n            XmlName.fromString('string'),\n            <XmlAttribute>[\n              XmlAttribute(XmlName.fromString('name'), key),",
      "                      key: formState,\n                      autovalidateMode: AutovalidateMode.onUserInteraction,\n                      restorationId: 'text_form_field',\n                      initialValue: 'foo',\n                      validator: errorText,\n                    ),\n                  );\n                },\n              ),\n            ),\n          ),\n        ),\n      );\n    }\n\n    await tester.pumpWidget(builder());\n\n    // No error text is visible yet.\n    expect(find.text(errorText('foo')!), findsNothing);\n\n    await tester.enterText(find.byType(TextFormField), 'bar');\n    await tester.pumpAndSettle();\n    expect(find.text(errorText('bar')!), findsOneWidget);\n\n    final TestRestorationData data = await tester.getRestorationData();\n    await tester.restartAndRestore();\n    // Error text should be present after restart and restore.\n    expect(find.text(errorText('bar')!), findsOneWidget);\n\n    // Resetting the form state should remove the error text.\n    formState.currentState!.reset();\n    await tester.pumpAndSettle();\n    expect(find.text(errorText('bar')!), findsNothing);\n    await tester.restartAndRestore();\n    // Error text should still be removed after restart and restore.\n    expect(find.text(errorText('bar')!), findsNothing);\n\n    await tester.restoreFrom(data);\n    expect(find.text(errorText('bar')!), findsOneWidget);\n  });\n\n  testWidgets('State Restoration (No Form ancestor) - validator sets the error text only when validate is called', (WidgetTester tester) async {\n    String? errorText(String? value) => '$value/error';\n    late GlobalKey<FormFieldState<String>> formState;\n\n    Widget builder(AutovalidateMode mode) {\n      return MaterialApp(\n        restorationScopeId: 'app',\n        home: MediaQuery(\n          data: const MediaQueryData(),\n          child: Directionality(\n            textDirection: TextDirection.ltr,\n            child: Center(\n              child: StatefulBuilder(\n                builder: (BuildContext context, StateSetter state) {\n                  formState = GlobalKey<FormFieldState<String>>();\n                  return Material(\n                    child: TextFormField(\n                      key: formState,\n                      restorationId: 'form_field',\n                      autovalidateMode: mode,\n                      initialValue: 'foo',\n                      validator: errorText,\n                    ),",
      "/// This will not reduce the number of [Layer] objects created; the compositing\n/// strategy is unaffected. It merely causes the opacity layers to be skipped\n/// when building the scene.\nbool debugDisableOpacityLayers = false;\n\nvoid _debugDrawDoubleRect(Canvas canvas, Rect outerRect, Rect innerRect, Color color) {\n  final Path path = Path()\n    ..fillType = PathFillType.evenOdd\n    ..addRect(outerRect)\n    ..addRect(innerRect);\n  final Paint paint = Paint()\n    ..color = color;\n  canvas.drawPath(path, paint);\n}\n\n/// Paint a diagram showing the given area as padding.\n///\n/// The `innerRect` argument represents the position of the child, if any.\n///\n/// When `innerRect` is null, the method draws the entire `outerRect` in a\n/// grayish color representing _spacing_.\n///\n/// When `innerRect` is non-null, the method draws the padding region around the\n/// `innerRect` in a tealish color, with a solid outline around the inner\n/// region.\n///\n/// This method is used by [RenderPadding.debugPaintSize] when\n/// [debugPaintSizeEnabled] is true.\nvoid debugPaintPadding(Canvas canvas, Rect outerRect, Rect? innerRect, { double outlineWidth = 2.0 }) {\n  assert(() {\n    if (innerRect != null && !innerRect.isEmpty) {\n      _debugDrawDoubleRect(canvas, outerRect, innerRect, const Color(0x900090FF));\n      _debugDrawDoubleRect(canvas, innerRect.inflate(outlineWidth).intersect(outerRect), innerRect, const Color(0xFF0090FF));\n    } else {\n      final Paint paint = Paint()\n        ..color = const Color(0x90909090);\n      canvas.drawRect(outerRect, paint);\n    }\n    return true;\n  }());\n}\n\n/// Returns true if none of the rendering library debug variables have been changed.\n///\n/// This function is used by the test framework to ensure that debug variables\n/// haven't been inadvertently changed.\n///\n/// See [the rendering library](rendering/rendering-library.html) for a complete\n/// list.\n///\n/// The `debugCheckIntrinsicSizesOverride` argument can be provided to override\n/// the expected value for [debugCheckIntrinsicSizes]. (This exists because the\n/// test framework itself overrides this value in some cases.)\nbool debugAssertAllRenderVarsUnset(String reason, { bool debugCheckIntrinsicSizesOverride = false }) {\n  assert(() {\n    if (debugPaintSizeEnabled ||\n        debugPaintBaselinesEnabled ||\n        debugPaintLayerBordersEnabled ||\n        debugPaintPointersEnabled ||\n        debugRepaintRainbowEnabled ||\n        debugRepaintTextRainbowEnabled ||\n        debugCurrentRepaintColor != _kDebugDefaultRepaintColor ||\n        debugPrintMarkNeedsLayoutStacks ||\n        debugPrintMarkNeedsPaintStacks ||",
      "  });\n\n  testWidgets('Fullscreen dialogs do not create heroes', (WidgetTester tester) async {\n    await tester.pumpWidget(\n      const CupertinoApp(\n        home: Placeholder(),\n      ),\n    );\n\n    tester\n        .state<NavigatorState>(find.byType(Navigator))\n        .push(CupertinoPageRoute<void>(\n          title: 'Page 1',\n          builder: (BuildContext context) => scaffoldForNavBar(null)!,\n        ));\n\n    await tester.pump();\n    await tester.pump(const Duration(milliseconds: 500));\n\n    tester\n        .state<NavigatorState>(find.byType(Navigator))\n        .push(CupertinoPageRoute<void>(\n          title: 'Page 2',\n          fullscreenDialog: true,\n          builder: (BuildContext context) => scaffoldForNavBar(null)!,\n        ));\n\n    await tester.pump();\n    await tester.pump(const Duration(milliseconds: 100));\n\n    // Only the first (non-fullscreen-dialog) page has a Hero.\n    expect(find.byType(Hero), findsOneWidget);\n    // No Hero transition happened.\n    expect(() => flying(tester, find.text('Page 2')), throwsAssertionError);\n  });\n\n  testWidgets('Turning off transition works', (WidgetTester tester) async {\n    await startTransitionBetween(\n      tester,\n      from: const CupertinoNavigationBar(\n        transitionBetweenRoutes: false,\n        middle: Text('Page 1'),\n      ),\n      toTitle: 'Page 2',\n    );\n\n    await tester.pump(const Duration(milliseconds: 50));\n\n    // Only the second page that doesn't have the transitionBetweenRoutes\n    // override off has a Hero.\n    expect(find.byType(Hero), findsOneWidget);\n    expect(\n      find.descendant(of: find.byType(Hero), matching: find.text('Page 2')),\n      findsOneWidget,\n    );\n\n    // No Hero transition happened.\n    expect(() => flying(tester, find.text('Page 2')), throwsAssertionError);\n  });\n\n  testWidgets('Popping mid-transition is symmetrical', (WidgetTester tester) async {\n    await startTransitionBetween(tester, fromTitle: 'Page 1');\n\n    // Be mid-transition.",
      "  static final Path _path3 = _pathFromString(\n      'M10.0002 6.875C9.1714 6.875 8.37655 7.20424 7.7905 7.79029C7.20445 8.37635 6.87521 9.1712 6.87521 10C6.87521 10.6181 7.05848 11.2223 7.40186 11.7362C7.74524 12.2501 8.2333 12.6506 8.80432 12.8871C9.37534 13.1237 10.0037 13.1855 10.6099 13.065C11.2161 12.9444 11.7729 12.6467 12.2099 12.2097C12.647 11.7727 12.9446 11.2159 13.0652 10.6097C13.1857 10.0035 13.1239 9.37514 12.8873 8.80412C12.6508 8.2331 12.2503 7.74504 11.7364 7.40166C11.2225 7.05828 10.6183 6.875 10.0002 6.875ZM10.0002 8.125C9.50292 8.125 9.02601 8.32255 8.67438 8.67418C8.32275 9.02581 8.12521 9.50272 8.12521 10C8.12521 10.3708 8.23517 10.7334 8.4412 11.0417C8.64723 11.35 8.94006 11.5904 9.28267 11.7323C9.62529 11.8742 10.0023 11.9113 10.366 11.839C10.7297 11.7666 11.0638 11.5881 11.326 11.3258C11.5883 11.0636 11.7668 10.7295 11.8392 10.3658C11.9115 10.0021 11.8744 9.62508 11.7325 9.28247C11.5906 8.93986 11.3502 8.64703 11.0419 8.441C10.7336 8.23497 10.371 8.125 10.0002 8.125Z');\n\n  @override\n  bool shouldRepaint(covariant CustomPainter oldDelegate) {\n    return false;\n  }\n}\n\nclass _CameraIconPainter extends CustomPainter {\n  @override\n  void paint(Canvas canvas, Size size) {\n    final Matrix4 scale =\n        Matrix4.diagonal3Values(size.width / 20, size.height / 20, 1.0);\n\n    Path path;\n    path = _path1.transform(scale.storage)..fillType = PathFillType.evenOdd;\n    canvas.drawPath(path, Paint()..color = const Color(0xFFF84F39));\n\n    path = _path2.transform(scale.storage)..fillType = PathFillType.evenOdd;\n    canvas.drawPath(path, Paint()..color = const Color(0x60F84F39));\n  }\n\n  static final Path _path1 = _pathFromString(\n      'M3.26366 17H16.7363C17.4857 17 18.0503 16.8123 18.4302 16.4369C18.8101 16.0615 19 15.5035 19 14.7631V7.01229C19 6.27188 18.8101 5.71397 18.4302 5.33855C18.0503 4.96313 17.4857 4.77542 16.7363 4.77542H14.6132C14.4154 4.77542 14.2567 4.76238 14.137 4.73631C14.0173 4.70503 13.9107 4.65549 13.817 4.58771C13.7233 4.51471 13.6219 4.41825 13.5126 4.29832L12.9115 3.61788C12.7242 3.41453 12.5212 3.26071 12.3027 3.15642C12.0893 3.05214 11.7953 3 11.4206 3H8.50911C8.13443 3 7.83781 3.05214 7.61925 3.15642C7.4059 3.26071 7.20555 3.41453 7.01821 3.61788L6.41717 4.29832C6.24545 4.48082 6.09193 4.60596 5.95663 4.67374C5.82134 4.74153 5.61058 4.77542 5.32437 4.77542H3.26366C2.50911 4.77542 1.94189 4.96313 1.56201 5.33855C1.18734 5.71397 1 6.27188 1 7.01229V14.7631C1 15.5035 1.18734 16.0615 1.56201 16.4369C1.94189 16.8123 2.50911 17 3.26366 17ZM3.27927 15.9207C2.89419 15.9207 2.59757 15.819 2.38942 15.6156C2.18647 15.4123 2.085 15.1099 2.085 14.7084V7.07486C2.085 6.67337 2.18647 6.37095 2.38942 6.1676C2.59757 5.95903 2.89419 5.85475 3.27927 5.85475H5.57415C5.9072 5.85475 6.1752 5.81825 6.37814 5.74525C6.58109 5.67225 6.77624 5.53147 6.96357 5.32291L7.55681 4.6581C7.76496 4.42346 7.9497 4.26965 8.11101 4.19665C8.27233 4.12365 8.50911 4.08715 8.82134 4.08715H11.1084C11.4258 4.08715 11.6626 4.12365 11.8187 4.19665C11.9801 4.26965 12.1648 4.42346 12.3729 4.6581L12.9662 5.32291C13.1535 5.53147 13.3487 5.67225 13.5516 5.74525C13.7546 5.81825 14.0225 5.85475 14.3556 5.85475H16.7207C17.1058 5.85475 17.4024 5.95903 17.6106 6.1676C17.8187 6.37095 17.9228 6.67337 17.9228 7.07486V14.7084C17.9228 15.1099 17.8187 15.4123 17.6106 15.6156C17.4024 15.819 17.1058 15.9207 16.7207 15.9207H3.27927ZM10 14.8101C10.7493 14.8101 11.4284 14.6302 12.0373 14.2704C12.6461 13.9054 13.1301 13.4179 13.4892 12.8078C13.8534 12.1926 14.0356 11.5095 14.0356 10.7587C14.0356 10.0078 13.8534 9.32477 13.4892 8.7095C13.1301 8.09423 12.6461 7.6067 12.0373 7.24693C11.4284 6.88194 10.7493 6.69944 10 6.69944C9.25585 6.69944 8.57676 6.88194 7.96271 7.24693C7.35386 7.6067 6.8673 8.09423 6.50304 8.7095C6.14397 9.32477 5.96444 10.0078 5.96444 10.7587C5.96444 11.5095 6.14397 12.1926 6.50304 12.8078C6.8673 13.4179 7.35386 13.9054 7.96271 14.2704C8.57676 14.6302 9.25585 14.8101 10 14.8101ZM10 13.7855C9.4484 13.7855 8.94363 13.6499 8.48569 13.3788C8.03296 13.1076 7.66869 12.7426 7.39289 12.2838C7.12229 11.825 6.98699 11.3166 6.98699 10.7587C6.98699 10.1955 7.12229 9.68454 7.39289 9.2257C7.66349 8.76685 8.02775 8.40447 8.48569 8.13855C8.94363 7.86741 9.4484 7.73184 10 7.73184C10.5568 7.73184 11.0616 7.86741 11.5143 8.13855C11.9722 8.40447 12.3365 8.76685 12.6071 9.2257C12.8829 9.68454 13.0208 10.1955 13.0208 10.7587C13.0208 11.3166 12.8829 11.825 12.6071 12.2838C12.3365 12.7426 11.9722 13.1076 11.5143 13.3788C11.0616 13.6499 10.5568 13.7855 10 13.7855ZM14.3556 8.04469C14.3556 8.30019 14.4467 8.51657 14.6288 8.69385C14.8109 8.86592 15.0269 8.95196 15.2767 8.95196C15.516 8.94674 15.7242 8.8581 15.9011 8.68603C16.0833 8.50875 16.1743 8.29497 16.1743 8.04469C16.1743 7.79963 16.0833 7.58845 15.9011 7.41117C15.7242 7.22868 15.516 7.13743 15.2767 7.13743C15.0269 7.13743 14.8109 7.22868 14.6288 7.41117C14.4467 7.58845 14.3556 7.79963 14.3556 8.04469Z');\n  static final Path _path2 = _pathFromString(\n      'M2.30754 15.6907C2.51782 15.8969 2.81748 16 3.20651 16H16.7856C17.1746 16 17.4743 15.8969 17.6846 15.6907C17.8949 15.4845 18 15.1778 18 14.7707V7.02974C18 6.6226 17.8949 6.31593 17.6846 6.10972C17.4743 5.89822 17.1746 5.79247 16.7856 5.79247H14.3963C14.0598 5.79247 13.7891 5.75545 13.584 5.68143C13.379 5.6074 13.1819 5.46464 12.9926 5.25314L12.3933 4.57898C12.183 4.34104 11.9964 4.18506 11.8334 4.11104C11.6757 4.03701 11.4365 4 11.1158 4H8.80532C8.4899 4 8.2507 4.03701 8.08773 4.11104C7.92476 4.18506 7.73813 4.34104 7.52785 4.57898L6.92854 5.25314C6.73928 5.46464 6.54214 5.6074 6.33711 5.68143C6.13208 5.75545 5.86134 5.79247 5.52489 5.79247H3.20651C2.81748 5.79247 2.51782 5.89822 2.30754 6.10972C2.10251 6.31593 2 6.6226 2 7.02974V14.7707C2 15.1778 2.10251 15.4845 2.30754 15.6907ZM9.99547 14C9.99547 14 8.76994 13.8432 8.23868 13.5297C7.71345 13.2162 7.29086 12.7941 6.97089 12.2636C6.65696 11.733 6.5 11.1451 6.5 10.5C6.5 9.84884 6.65696 9.25797 6.97089 8.72739C7.28482 8.19681 7.70742 7.77778 8.23868 7.47028C8.76994 7.15676 9.35554 7 9.99547 7C10.6414 7 11.7523 7.47028 11.7523 7.47028C11.7523 7.47028 12.7061 8.19681 13.0201 8.72739C13.34 9.25797 13.5 9.84884 13.5 10.5C13.5 11.1451 13.34 11.733 13.0201 12.2636C12.7061 12.7941 12.2835 13.2162 11.7523 13.5297C11.227 13.8432 9.99547 14 9.99547 14Z');\n\n  @override\n  bool shouldRepaint(covariant CustomPainter oldDelegate) {\n    return false;\n  }\n}\n\nclass _CalendarIconPainter extends CustomPainter {\n  @override\n  void paint(Canvas canvas, Size size) {\n    final Matrix4 scale =\n        Matrix4.diagonal3Values(size.width / 20, size.height / 20, 1.0);\n\n    Path path;\n    path = _path1.transform(scale.storage)..fillType = PathFillType.evenOdd;\n    canvas.drawPath(path, Paint()..color = const Color(0x60F84F39));\n\n    path = _path2.transform(scale.storage)..fillType = PathFillType.evenOdd;\n    canvas.drawPath(path, Paint()..color = const Color(0xFFF84F39));\n  }\n\n  static final Path _path1 = _pathFromString(\n      'M16.7812 6.85938H3.28125V4.5L5 3H15L16.7812 4.5V6.85938Z');\n  static final Path _path2 = _pathFromString(\n      'M6.5606 11.2462C7.07732 11.2462 7.4962 10.8273 7.4962 10.3106C7.4962 9.79388 7.07732 9.375 6.5606 9.375C6.04388 9.375 5.625 9.79388 5.625 10.3106C5.625 10.8273 6.04388 11.2462 6.5606 11.2462ZM7.4962 13.4356C7.4962 13.9523 7.07732 14.3712 6.5606 14.3712C6.04388 14.3712 5.625 13.9523 5.625 13.4356C5.625 12.9189 6.04388 12.5 6.5606 12.5C7.07732 12.5 7.4962 12.9189 7.4962 13.4356ZM10.0005 11.2462C10.5173 11.2462 10.9361 10.8273 10.9361 10.3106C10.9361 9.79388 10.5173 9.375 10.0005 9.375C9.48382 9.375 9.06494 9.79388 9.06494 10.3106C9.06494 10.8273 9.48382 11.2462 10.0005 11.2462ZM10.9361 13.4356C10.9361 13.9523 10.5173 14.3712 10.0005 14.3712C9.48382 14.3712 9.06494 13.9523 9.06494 13.4356C9.06494 12.9189 9.48382 12.5 10.0005 12.5C10.5173 12.5 10.9361 12.9189 10.9361 13.4356ZM13.4356 11.2462C13.9523 11.2462 14.3712 10.8273 14.3712 10.3106C14.3712 9.79388 13.9523 9.375 13.4356 9.375C12.9189 9.375 12.5 9.79388 12.5 10.3106C12.5 10.8273 12.9189 11.2462 13.4356 11.2462ZM17.5 5.625C17.5 3.89911 16.1009 2.5 14.375 2.5H5.625C3.89911 2.5 2.5 3.89911 2.5 5.625V14.375C2.5 16.1009 3.89911 17.5 5.625 17.5H14.375C16.1009 17.5 17.5 16.1009 17.5 14.375V5.625ZM3.75 7.5H16.25V14.375C16.25 15.4105 15.4105 16.25 14.375 16.25H5.625C4.58947 16.25 3.75 15.4105 3.75 14.375V7.5ZM5.625 3.75H14.375C15.4105 3.75 16.25 4.58947 16.25 5.625V6.25H3.75V5.625C3.75 4.58947 4.58947 3.75 5.625 3.75Z');\n\n  @override\n  bool shouldRepaint(covariant CustomPainter oldDelegate) {\n    return false;\n  }\n}\n\nclass _ConversationIconPainter extends CustomPainter {\n  @override\n  void paint(Canvas canvas, Size size) {\n    final Matrix4 scale =\n        Matrix4.diagonal3Values(size.width / 20, size.height / 20, 1.0);",
      "\n    final TextStyle? effectiveTextStyle = widget.textStyle ?? theme.textStyle ?? defaults.textStyle;\n\n    MenuStyle? effectiveMenuStyle = widget.menuStyle\n      ?? theme.menuStyle\n      ?? defaults.menuStyle!;\n\n    final double? anchorWidth = getWidth(_anchorKey);\n    if (widget.width != null) {\n      effectiveMenuStyle = effectiveMenuStyle.copyWith(minimumSize: MaterialStatePropertyAll<Size?>(Size(widget.width!, 0.0)));\n    } else if (anchorWidth != null){\n      effectiveMenuStyle = effectiveMenuStyle.copyWith(minimumSize: MaterialStatePropertyAll<Size?>(Size(anchorWidth, 0.0)));\n    }\n\n    if (widget.menuHeight != null) {\n      effectiveMenuStyle = effectiveMenuStyle.copyWith(maximumSize: MaterialStatePropertyAll<Size>(Size(double.infinity, widget.menuHeight!)));\n    }\n    final InputDecorationTheme effectiveInputDecorationTheme = widget.inputDecorationTheme\n      ?? theme.inputDecorationTheme\n      ?? defaults.inputDecorationTheme!;\n\n    final MouseCursor effectiveMouseCursor = canRequestFocus() ? SystemMouseCursors.text : SystemMouseCursors.click;\n\n    Widget menuAnchor = MenuAnchor(\n      style: effectiveMenuStyle,\n      controller: _controller,\n      menuChildren: menu,\n      crossAxisUnconstrained: false,\n      builder: (BuildContext context, MenuController controller, Widget? child) {\n        assert(_initialMenu != null);\n        final Widget trailingButton = Padding(\n          padding: const EdgeInsets.all(4.0),\n          child: IconButton(\n            isSelected: controller.isOpen,\n            icon: widget.trailingIcon ?? const Icon(Icons.arrow_drop_down),\n            selectedIcon: widget.selectedTrailingIcon ?? const Icon(Icons.arrow_drop_up),\n            onPressed: () {\n              handlePressed(controller);\n            },\n          ),\n        );\n\n        final Widget leadingButton = Padding(\n            padding: const EdgeInsets.all(8.0),\n            child: widget.leadingIcon ?? const SizedBox()\n        );\n\n        final Widget textField = TextField(\n            key: _anchorKey,\n            mouseCursor: effectiveMouseCursor,\n            canRequestFocus: canRequestFocus(),\n            enableInteractiveSelection: canRequestFocus(),\n            textAlignVertical: TextAlignVertical.center,\n            style: effectiveTextStyle,\n            controller: _textEditingController,\n            onEditingComplete: () {\n              if (currentHighlight != null) {\n                final DropdownMenuEntry<T> entry = filteredEntries[currentHighlight!];\n                if (entry.enabled) {\n                  _textEditingController.text = entry.label;\n                  _textEditingController.selection =\n                      TextSelection.collapsed(offset: _textEditingController.text.length);\n                  widget.onSelected?.call(entry.value);\n                }",
      "// Copyright 2014 The Flutter Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be\n// found in the LICENSE file.\n\nimport 'package:flutter/material.dart';\nimport 'package:flutter_api_samples/material/input_decorator/input_decoration.label_style_error.0.dart' as example;\nimport 'package:flutter_test/flutter_test.dart';\n\nvoid main() {\n  testWidgets('InputDecorator label uses error color', (WidgetTester tester) async {\n    await tester.pumpWidget(\n      const example.LabelStyleErrorExampleApp(),\n    );\n    final Theme theme = tester.firstWidget(find.byType(Theme));\n\n    final AnimatedDefaultTextStyle label = tester.firstWidget(find.ancestor(of: find.text('Name'), matching: find.byType(AnimatedDefaultTextStyle)));\n    expect(label.style.color, theme.data.colorScheme.error);\n  });\n}\n",
      "          completer.complete();\n        });\n        await completer.future;\n      }\n    } else if (!setSemanticsCommand.enabled && _semantics != null) {\n      _semantics!.dispose();\n      _semantics = null;\n    }\n    return SetSemanticsResult(semanticsWasEnabled != _semanticsIsEnabled);\n  }\n\n  // This can be used to wait for the first frame being rasterized during app launch.\n  @Deprecated(\n    'This method has been deprecated in favor of _waitForCondition. '\n    'This feature was deprecated after v1.9.3.'\n  )\n  Future<Result> _waitUntilFirstFrameRasterized(Command command) async {\n    await WidgetsBinding.instance.waitUntilFirstFrameRasterized;\n    return Result.empty;\n  }\n\n  /// Runs `finder` repeatedly until it finds one or more [Element]s.\n  Future<Finder> waitForElement(Finder finder) async {\n    if (_frameSync) {\n      await _waitUntilFrame(() => SchedulerBinding.instance.transientCallbackCount == 0);\n    }\n\n    await _waitUntilFrame(() => finder.evaluate().isNotEmpty);\n\n    if (_frameSync) {\n      await _waitUntilFrame(() => SchedulerBinding.instance.transientCallbackCount == 0);\n    }\n\n    return finder;\n  }\n\n  /// Runs `finder` repeatedly until it finds zero [Element]s.\n  Future<Finder> waitForAbsentElement(Finder finder) async {\n    if (_frameSync) {\n      await _waitUntilFrame(() => SchedulerBinding.instance.transientCallbackCount == 0);\n    }\n\n    await _waitUntilFrame(() => finder.evaluate().isEmpty);\n\n    if (_frameSync) {\n      await _waitUntilFrame(() => SchedulerBinding.instance.transientCallbackCount == 0);\n    }\n\n    return finder;\n  }\n\n  // Waits until at the end of a frame the provided [condition] is [true].\n  Future<void> _waitUntilFrame(bool Function() condition, [ Completer<void>? completer ]) {\n    completer ??= Completer<void>();\n    if (!condition()) {\n      SchedulerBinding.instance.addPostFrameCallback((Duration timestamp) {\n        _waitUntilFrame(condition, completer);\n      });\n    } else {\n      completer.complete();\n    }\n    return completer.future;\n  }\n}",
      "// Copyright 2014 The Flutter Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be\n// found in the LICENSE file.\n\nimport 'package:flutter/services.dart';\nimport 'package:flutter_test/flutter_test.dart';\n\nclass MockRestorationManager extends TestRestorationManager {\n  MockRestorationManager({ this.enableChannels = false });\n\n  bool get updateScheduled => _updateScheduled;\n  bool _updateScheduled = false;\n\n  final List<RestorationBucket> _buckets = <RestorationBucket>[];\n\n  final bool enableChannels;\n\n  @override\n  void initChannels() {\n    if (enableChannels) {\n      super.initChannels();\n    }\n  }\n\n  @override\n  void scheduleSerializationFor(RestorationBucket bucket) {\n    _updateScheduled = true;\n    _buckets.add(bucket);\n  }\n\n  @override\n  bool unscheduleSerializationFor(RestorationBucket bucket) {\n    _updateScheduled = true;\n    return _buckets.remove(bucket);\n  }\n\n  void doSerialization() {\n    _updateScheduled = false;\n    for (final RestorationBucket bucket in _buckets) {\n      bucket.finalize();\n    }\n    _buckets.clear();\n  }\n\n  @override\n  void restoreFrom(TestRestorationData data) {\n    // Ignore in mock.\n  }\n\n  int rootBucketAccessed = 0;\n\n  @override\n  Future<RestorationBucket?> get rootBucket {\n    rootBucketAccessed++;\n    return _rootBucket;\n  }\n  late Future<RestorationBucket?> _rootBucket;\n  set rootBucket(Future<RestorationBucket?> value) {\n    _rootBucket = value;\n    _isRestoring = true;\n    ServicesBinding.instance.addPostFrameCallback((Duration _) {\n      _isRestoring = false;\n    });\n    notifyListeners();"
    ]
  },
  {
    "id": "pandas-dev/pandas",
    "org": "pandas-dev",
    "avatarURL": "https://avatars.githubusercontent.com/u/21206976?v=4",
    "name": "pandas-dev/pandas",
    "url": "https://github.com/pandas-dev/pandas",
    "lang": "Python",
    "desc": "Flexible and powerful data analysis / manipulation library for Python.",
    "star_num": 39638,
    "fork_num": 16662,
    "snippets": [
      "    is_complex_dtype,\n    is_datetime64_any_dtype,\n    is_datetime64_dtype,\n    is_datetime64_ns_dtype,\n    is_datetime64tz_dtype,\n    is_dict_like,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_file_like,\n    is_float,\n    is_float_dtype,\n    is_hashable,\n    is_int64_dtype,\n    is_integer,\n    is_integer_dtype,\n    is_interval,\n    is_interval_dtype,\n    is_iterator,\n    is_list_like,\n    is_named_tuple,\n    is_number,\n    is_numeric_dtype,\n    is_object_dtype,\n    is_period_dtype,\n    is_re,\n    is_re_compilable,\n    is_scalar,\n    is_signed_integer_dtype,\n    is_sparse,\n    is_string_dtype,\n    is_timedelta64_dtype,\n    is_timedelta64_ns_dtype,\n    is_unsigned_integer_dtype,\n    pandas_dtype,\n)\n\n__all__ = [\n    \"is_any_real_numeric_dtype\",\n    \"is_array_like\",\n    \"is_bool\",\n    \"is_bool_dtype\",\n    \"is_categorical_dtype\",\n    \"is_complex\",\n    \"is_complex_dtype\",\n    \"is_datetime64_any_dtype\",\n    \"is_datetime64_dtype\",\n    \"is_datetime64_ns_dtype\",\n    \"is_datetime64tz_dtype\",\n    \"is_dict_like\",\n    \"is_dtype_equal\",\n    \"is_extension_array_dtype\",\n    \"is_file_like\",\n    \"is_float\",\n    \"is_float_dtype\",\n    \"is_hashable\",\n    \"is_int64_dtype\",\n    \"is_integer\",\n    \"is_integer_dtype\",\n    \"is_interval\",\n    \"is_interval_dtype\",\n    \"is_iterator\",\n    \"is_list_like\",\n    \"is_named_tuple\",\n    \"is_number\",",
      "    s = Series(list(\"abcdef\"))\n\n    with pytest.raises(ValueError, match=msg(\"slice\")):\n        s[:-1] = list(range(27))\n\n    s[-3:-1] = list(range(2))\n    expected = Series([\"a\", \"b\", \"c\", 0, 1, \"f\"])\n    tm.assert_series_equal(s, expected)\n\n    # list\n    s = Series(list(\"abc\"))\n\n    with pytest.raises(ValueError, match=msg(\"list-like\")):\n        s[[0, 1, 2]] = list(range(27))\n\n    s = Series(list(\"abc\"))\n\n    with pytest.raises(ValueError, match=msg(\"list-like\")):\n        s[[0, 1, 2]] = list(range(2))\n\n    # scalar\n    s = Series(list(\"abc\"))\n    s[0] = list(range(10))\n    expected = Series([list(range(10)), \"b\", \"c\"])\n    tm.assert_series_equal(s, expected)\n\n\n@pytest.mark.parametrize(\"size\", range(2, 6))\n@pytest.mark.parametrize(\n    \"mask\", [[True, False, False, False, False], [True, False], [False]]\n)\n@pytest.mark.parametrize(\n    \"item\", [2.0, np.nan, np.finfo(float).max, np.finfo(float).min]\n)\n# Test numpy arrays, lists and tuples as the input to be\n# broadcast\n@pytest.mark.parametrize(\n    \"box\", [lambda x: np.array([x]), lambda x: [x], lambda x: (x,)]\n)\ndef test_broadcast(size, mask, item, box):\n    # GH#8801, GH#4195\n    selection = np.resize(mask, size)\n\n    data = np.arange(size, dtype=float)\n\n    # Construct the expected series by taking the source\n    # data or item based on the selection\n    expected = Series(\n        [item if use_item else data[i] for i, use_item in enumerate(selection)]\n    )\n\n    s = Series(data)\n\n    s[selection] = item\n    tm.assert_series_equal(s, expected)\n\n    s = Series(data)\n    result = s.where(~selection, box(item))\n    tm.assert_series_equal(result, expected)\n\n    s = Series(data)\n    result = s.mask(selection, box(item))\n    tm.assert_series_equal(result, expected)\n",
      "\n        return cache_readonly\n\n    raise AttributeError(f\"module 'pandas.util' has no attribute '{key}'\")\n\n\ndef capitalize_first_letter(s):\n    return s[:1].upper() + s[1:]\n",
      "        fig = mpl.pyplot.figure()\n        ax = fig.add_subplot(211)\n        ax = df.plot(secondary_y=[\"A\", \"B\"], ax=ax)\n        leg = ax.get_legend()\n        assert len(leg.get_lines()) == 4\n        assert ax.right_ax.get_legend() is None\n        colors = set()\n        for line in leg.get_lines():\n            colors.add(line.get_color())\n\n        # TODO: color cycle problems\n        assert len(colors) == 4\n        mpl.pyplot.close()\n\n    def test_secondary_legend_nonts_multi_col(self):\n        # non-ts\n        df = tm.makeDataFrame()\n        fig = mpl.pyplot.figure()\n        ax = fig.add_subplot(211)\n        ax = df.plot(secondary_y=[\"C\", \"D\"], ax=ax)\n        leg = ax.get_legend()\n        assert len(leg.get_lines()) == 4\n        assert ax.right_ax.get_legend() is None\n        colors = set()\n        for line in leg.get_lines():\n            colors.add(line.get_color())\n\n        # TODO: color cycle problems\n        assert len(colors) == 4\n\n    @pytest.mark.xfail(reason=\"Api changed in 3.6.0\")\n    def test_format_date_axis(self):\n        rng = date_range(\"1/1/2012\", periods=12, freq=\"M\")\n        df = DataFrame(np.random.default_rng(2).standard_normal((len(rng), 3)), rng)\n        _, ax = mpl.pyplot.subplots()\n        ax = df.plot(ax=ax)\n        xaxis = ax.get_xaxis()\n        for line in xaxis.get_ticklabels():\n            if len(line.get_text()) > 0:\n                assert line.get_rotation() == 30\n\n    def test_ax_plot(self):\n        x = date_range(start=\"2012-01-02\", periods=10, freq=\"D\")\n        y = list(range(len(x)))\n        _, ax = mpl.pyplot.subplots()\n        lines = ax.plot(x, y, label=\"Y\")\n        tm.assert_index_equal(DatetimeIndex(lines[0].get_xdata()), x)\n\n    def test_mpl_nopandas(self):\n        dates = [date(2008, 12, 31), date(2009, 1, 31)]\n        values1 = np.arange(10.0, 11.0, 0.5)\n        values2 = np.arange(11.0, 12.0, 0.5)\n\n        kw = {\"fmt\": \"-\", \"lw\": 4}\n\n        _, ax = mpl.pyplot.subplots()\n        ax.plot_date([x.toordinal() for x in dates], values1, **kw)\n        ax.plot_date([x.toordinal() for x in dates], values2, **kw)\n\n        line1, line2 = ax.get_lines()\n\n        exp = np.array([x.toordinal() for x in dates], dtype=np.float64)\n        tm.assert_numpy_array_equal(line1.get_xydata()[:, 0], exp)\n        exp = np.array([x.toordinal() for x in dates], dtype=np.float64)",
      "\n    def time_constructor_dict(self):\n        Series(data=self.data, index=self.idx)\n\n    def time_constructor_no_data(self):\n        Series(data=None, index=self.idx)\n\n    def time_constructor_fastpath(self):\n        Series(self.array, index=self.idx2, name=\"name\", fastpath=True)\n\n\nclass ToFrame:\n    params = [[\"int64\", \"datetime64[ns]\", \"category\", \"Int64\"], [None, \"foo\"]]\n    param_names = [\"dtype\", \"name\"]\n\n    def setup(self, dtype, name):\n        arr = np.arange(10**5)\n        ser = Series(arr, dtype=dtype)\n        self.ser = ser\n\n    def time_to_frame(self, dtype, name):\n        self.ser.to_frame(name)\n\n\nclass NSort:\n    params = [\"first\", \"last\", \"all\"]\n    param_names = [\"keep\"]\n\n    def setup(self, keep):\n        self.s = Series(np.random.randint(1, 10, 100000))\n\n    def time_nlargest(self, keep):\n        self.s.nlargest(3, keep=keep)\n\n    def time_nsmallest(self, keep):\n        self.s.nsmallest(3, keep=keep)\n\n\nclass Dropna:\n    params = [\"int\", \"datetime\"]\n    param_names = [\"dtype\"]\n\n    def setup(self, dtype):\n        N = 10**6\n        data = {\n            \"int\": np.random.randint(1, 10, N),\n            \"datetime\": date_range(\"2000-01-01\", freq=\"S\", periods=N),\n        }\n        self.s = Series(data[dtype])\n        if dtype == \"datetime\":\n            self.s[np.random.randint(1, N, 100)] = NaT\n\n    def time_dropna(self, dtype):\n        self.s.dropna()\n\n\nclass Fillna:\n    params = [\n        [\n            \"datetime64[ns]\",\n            \"float32\",\n            \"float64\",\n            \"Float64\",\n            \"Int64\",",
      "            result.index.levels[0], pairwise_frames.index, check_names=False\n        )\n        tm.assert_index_equal(\n            safe_sort(result.index.levels[1]),\n            safe_sort(pairwise_frames.columns.unique()),\n        )\n        tm.assert_index_equal(result.columns, pairwise_frames.columns)\n        expected = f(pairwise_target_frame)\n        # since we have sorted the results\n        # we can only compare non-nans\n        result = result.dropna().values\n        expected = expected.dropna().values\n\n        tm.assert_numpy_array_equal(result, expected, check_dtype=False)\n\n    @pytest.mark.parametrize(\n        \"f\",\n        [\n            lambda x: x.expanding().cov(pairwise=False),\n            lambda x: x.expanding().corr(pairwise=False),\n            lambda x: x.rolling(window=3).cov(pairwise=False),\n            lambda x: x.rolling(window=3).corr(pairwise=False),\n            lambda x: x.ewm(com=3).cov(pairwise=False),\n            lambda x: x.ewm(com=3).corr(pairwise=False),\n        ],\n    )\n    def test_no_pairwise_with_self(self, pairwise_frames, pairwise_target_frame, f):\n        # DataFrame with itself, pairwise=False\n        result = f(pairwise_frames)\n        tm.assert_index_equal(result.index, pairwise_frames.index)\n        tm.assert_index_equal(result.columns, pairwise_frames.columns)\n        expected = f(pairwise_target_frame)\n        # since we have sorted the results\n        # we can only compare non-nans\n        result = result.dropna().values\n        expected = expected.dropna().values\n\n        tm.assert_numpy_array_equal(result, expected, check_dtype=False)\n\n    @pytest.mark.parametrize(\n        \"f\",\n        [\n            lambda x, y: x.expanding().cov(y, pairwise=True),\n            lambda x, y: x.expanding().corr(y, pairwise=True),\n            lambda x, y: x.rolling(window=3).cov(y, pairwise=True),\n            # TODO: We're missing a flag somewhere in meson\n            pytest.param(\n                lambda x, y: x.rolling(window=3).corr(y, pairwise=True),\n                marks=pytest.mark.xfail(\n                    not IS64, reason=\"Precision issues on 32 bit\", strict=False\n                ),\n            ),\n            lambda x, y: x.ewm(com=3).cov(y, pairwise=True),\n            lambda x, y: x.ewm(com=3).corr(y, pairwise=True),\n        ],\n    )\n    def test_pairwise_with_other(\n        self, pairwise_frames, pairwise_target_frame, pairwise_other_frame, f\n    ):\n        # DataFrame with another DataFrame, pairwise=True\n        result = f(pairwise_frames, pairwise_other_frame)\n        tm.assert_index_equal(\n            result.index.levels[0], pairwise_frames.index, check_names=False\n        )",
      "            html_style_tpl=self.template_html_style,\n        )\n\n    def _render_latex(\n        self, sparse_index: bool, sparse_columns: bool, clines: str | None, **kwargs\n    ) -> str:\n        \"\"\"\n        Render a Styler in latex format\n        \"\"\"\n        d = self._render(sparse_index, sparse_columns, None, None)\n        self._translate_latex(d, clines=clines)\n        self.template_latex.globals[\"parse_wrap\"] = _parse_latex_table_wrapping\n        self.template_latex.globals[\"parse_table\"] = _parse_latex_table_styles\n        self.template_latex.globals[\"parse_cell\"] = _parse_latex_cell_styles\n        self.template_latex.globals[\"parse_header\"] = _parse_latex_header_span\n        d.update(kwargs)\n        return self.template_latex.render(**d)\n\n    def _render_string(\n        self,\n        sparse_index: bool,\n        sparse_columns: bool,\n        max_rows: int | None = None,\n        max_cols: int | None = None,\n        **kwargs,\n    ) -> str:\n        \"\"\"\n        Render a Styler in string format\n        \"\"\"\n        d = self._render(sparse_index, sparse_columns, max_rows, max_cols)\n        d.update(kwargs)\n        return self.template_string.render(**d)\n\n    def _compute(self):\n        \"\"\"\n        Execute the style functions built up in `self._todo`.\n\n        Relies on the conventions that all style functions go through\n        .apply or .map. The append styles to apply as tuples of\n\n        (application method, *args, **kwargs)\n        \"\"\"\n        self.ctx.clear()\n        self.ctx_index.clear()\n        self.ctx_columns.clear()\n        r = self\n        for func, args, kwargs in self._todo:\n            r = func(self)(*args, **kwargs)\n        return r\n\n    def _translate(\n        self,\n        sparse_index: bool,\n        sparse_cols: bool,\n        max_rows: int | None = None,\n        max_cols: int | None = None,\n        blank: str = \"&nbsp;\",\n        dxs: list[dict] | None = None,\n    ):\n        \"\"\"\n        Process Styler data and settings into a dict for template rendering.\n\n        Convert data and settings from ``Styler`` attributes such as ``self.data``,\n        ``self.tooltips`` including applying any methods in ``self._todo``.",
      "    )\n    def test_dtypes_are_correct_after_groupby_last(self, data):\n        # GH46409\n        df = DataFrame(\n            {\"id\": [1, 2, 3, 4], \"test\": [True, pd.NA, data, False]}\n        ).convert_dtypes()\n        result = df.groupby(\"id\").last().test\n        expected = df.set_index(\"id\").test\n        assert result.dtype == pd.BooleanDtype()\n        tm.assert_series_equal(expected, result)\n\n    def test_dtypes_gh8722(self, float_string_frame):\n        float_string_frame[\"bool\"] = float_string_frame[\"A\"] > 0\n        result = float_string_frame.dtypes\n        expected = Series(\n            {k: v.dtype for k, v in float_string_frame.items()}, index=result.index\n        )\n        tm.assert_series_equal(result, expected)\n\n        # compat, GH 8722\n        msg = \"use_inf_as_na option is deprecated\"\n        with tm.assert_produces_warning(FutureWarning, match=msg):\n            with option_context(\"use_inf_as_na\", True):\n                df = DataFrame([[1]])\n                result = df.dtypes\n                tm.assert_series_equal(result, Series({0: np.dtype(\"int64\")}))\n\n    def test_dtypes_timedeltas(self):\n        df = DataFrame(\n            {\n                \"A\": Series(date_range(\"2012-1-1\", periods=3, freq=\"D\")),\n                \"B\": Series([timedelta(days=i) for i in range(3)]),\n            }\n        )\n        result = df.dtypes\n        expected = Series(\n            [np.dtype(\"datetime64[ns]\"), np.dtype(\"timedelta64[ns]\")], index=list(\"AB\")\n        )\n        tm.assert_series_equal(result, expected)\n\n        df[\"C\"] = df[\"A\"] + df[\"B\"]\n        result = df.dtypes\n        expected = Series(\n            [\n                np.dtype(\"datetime64[ns]\"),\n                np.dtype(\"timedelta64[ns]\"),\n                np.dtype(\"datetime64[ns]\"),\n            ],\n            index=list(\"ABC\"),\n        )\n        tm.assert_series_equal(result, expected)\n\n        # mixed int types\n        df[\"D\"] = 1\n        result = df.dtypes\n        expected = Series(\n            [\n                np.dtype(\"datetime64[ns]\"),\n                np.dtype(\"timedelta64[ns]\"),\n                np.dtype(\"datetime64[ns]\"),\n                np.dtype(\"int64\"),\n            ],\n            index=list(\"ABCD\"),\n        )",
      "            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        return NumpyExtensionArray\n    raise AttributeError(f\"module 'pandas.arrays' has no attribute '{name}'\")\n",
      "    tm.assert_equal(result, expected)\n"
    ]
  },
  {
    "id": "hakimel/reveal.js",
    "org": "hakimel",
    "avatarURL": "https://avatars.githubusercontent.com/u/629429?v=4",
    "name": "hakimel/reveal.js",
    "url": "https://github.com/hakimel/reveal.js",
    "lang": "JavaScript",
    "desc": "The HTML Presentation Framework.",
    "star_num": 64526,
    "fork_num": 16669,
    "snippets": [
      "\n// Regex for retrieving the fragment style from a class attribute\nexport const FRAGMENT_STYLE_REGEX = /fade-(down|up|right|left|out|in-then-out|in-then-semi-out)|semi-fade-out|current-visible|shrink|grow/;",
      "\t\t\t\tmessageData.notes = fragmentNotes.innerHTML;\n\t\t\t\tmessageData.markdown = typeof fragmentNotes.getAttribute( 'data-markdown' ) === 'string';\n\n\t\t\t\t// Ignore other slide notes\n\t\t\t\tnotesElements = null;\n\t\t\t}\n\t\t\telse if( fragmentElement.hasAttribute( 'data-notes' ) ) {\n\t\t\t\tmessageData.notes = fragmentElement.getAttribute( 'data-notes' );\n\t\t\t\tmessageData.whitespace = 'pre-wrap';\n\n\t\t\t\t// In case there are slide notes\n\t\t\t\tnotesElements = null;\n\t\t\t}\n\t\t}\n\n\t\t// Look for notes defined in an aside element\n\t\tif( notesElements ) {\n\t\t\tmessageData.notes = Array.from(notesElements).map( notesElement => notesElement.innerHTML ).join( '\\n' );\n\t\t\tmessageData.markdown = notesElements[0] && typeof notesElements[0].getAttribute( 'data-markdown' ) === 'string';\n\t\t}\n\n\t\tspeakerWindow.postMessage( JSON.stringify( messageData ), '*' );\n\n\t}\n\n\t/**\n\t * Check if the given event is from the same origin as the\n\t * current window.\n\t */\n\tfunction isSameOriginEvent( event ) {\n\n\t\ttry {\n\t\t\treturn window.location.origin === event.source.location.origin;\n\t\t}\n\t\tcatch ( error ) {\n\t\t\treturn false;\n\t\t}\n\n\t}\n\n\tfunction onPostMessage( event ) {\n\n\t\t// Only allow same-origin messages\n\t\t// (added 12/5/22 as a XSS safeguard)\n\t\tif( isSameOriginEvent( event ) ) {\n\n\t\t\tlet data = JSON.parse( event.data );\n\t\t\tif( data && data.namespace === 'reveal-notes' && data.type === 'connected' ) {\n\t\t\t\tclearInterval( connectInterval );\n\t\t\t\tonConnected();\n\t\t\t}\n\t\t\telse if( data && data.namespace === 'reveal-notes' && data.type === 'call' ) {\n\t\t\t\tcallRevealApi( data.methodName, data.arguments, data.callId );\n\t\t\t}\n\n\t\t}\n\n\t}\n\n\t/**\n\t * Called once we have established a connection to the notes\n\t * window.\n\t */\n\tfunction onConnected() {",
      "\n\t/**\n\t * Creates the slide background elements and appends them\n\t * to the background container. One element is created per\n\t * slide no matter if the given slide has visible background.\n\t */\n\tcreate() {\n\n\t\t// Clear prior backgrounds\n\t\tthis.element.innerHTML = '';\n\t\tthis.element.classList.add( 'no-transition' );\n\n\t\t// Iterate over all horizontal slides\n\t\tthis.Reveal.getHorizontalSlides().forEach( slideh => {\n\n\t\t\tlet backgroundStack = this.createBackground( slideh, this.element );\n\n\t\t\t// Iterate over all vertical slides\n\t\t\tqueryAll( slideh, 'section' ).forEach( slidev => {\n\n\t\t\t\tthis.createBackground( slidev, backgroundStack );\n\n\t\t\t\tbackgroundStack.classList.add( 'stack' );\n\n\t\t\t} );\n\n\t\t} );\n\n\t\t// Add parallax background if specified\n\t\tif( this.Reveal.getConfig().parallaxBackgroundImage ) {\n\n\t\t\tthis.element.style.backgroundImage = 'url(\"' + this.Reveal.getConfig().parallaxBackgroundImage + '\")';\n\t\t\tthis.element.style.backgroundSize = this.Reveal.getConfig().parallaxBackgroundSize;\n\t\t\tthis.element.style.backgroundRepeat = this.Reveal.getConfig().parallaxBackgroundRepeat;\n\t\t\tthis.element.style.backgroundPosition = this.Reveal.getConfig().parallaxBackgroundPosition;\n\n\t\t\t// Make sure the below properties are set on the element - these properties are\n\t\t\t// needed for proper transitions to be set on the element via CSS. To remove\n\t\t\t// annoying background slide-in effect when the presentation starts, apply\n\t\t\t// these properties after short time delay\n\t\t\tsetTimeout( () => {\n\t\t\t\tthis.Reveal.getRevealElement().classList.add( 'has-parallax-background' );\n\t\t\t}, 1 );\n\n\t\t}\n\t\telse {\n\n\t\t\tthis.element.style.backgroundImage = '';\n\t\t\tthis.Reveal.getRevealElement().classList.remove( 'has-parallax-background' );\n\n\t\t}\n\n\t}\n\n\t/**\n\t * Creates a background for the given slide.\n\t *\n\t * @param {HTMLElement} slide\n\t * @param {HTMLElement} container The element that the background\n\t * should be appended to\n\t * @return {HTMLElement} New background div\n\t */\n\tcreateBackground( slide, container ) {\n",
      "\t\t\t\tif( style.property === 'line-height' ) {\n\t\t\t\t\tvalue = parseFloat( computedStyles['line-height'] ) / parseFloat( computedStyles['font-size'] );\n\t\t\t\t}\n\n\t\t\t\tif( isNaN(value) ) {\n\t\t\t\t\tvalue = computedStyles[style.property];\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif( value !== '' ) {\n\t\t\t\tproperties.styles[style.property] = value;\n\t\t\t}\n\t\t} );\n\n\t\treturn properties;\n\n\t}\n\n\t/**\n\t * Get a list of all element pairs that we can animate\n\t * between the given slides.\n\t *\n\t * @param {HTMLElement} fromSlide\n\t * @param {HTMLElement} toSlide\n\t *\n\t * @return {Array} Each value is an array where [0] is\n\t * the element we're animating from and [1] is the\n\t * element we're animating to\n\t */\n\tgetAutoAnimatableElements( fromSlide, toSlide ) {\n\n\t\tlet matcher = typeof this.Reveal.getConfig().autoAnimateMatcher === 'function' ? this.Reveal.getConfig().autoAnimateMatcher : this.getAutoAnimatePairs;\n\n\t\tlet pairs = matcher.call( this, fromSlide, toSlide );\n\n\t\tlet reserved = [];\n\n\t\t// Remove duplicate pairs\n\t\treturn pairs.filter( ( pair, index ) => {\n\t\t\tif( reserved.indexOf( pair.to ) === -1 ) {\n\t\t\t\treserved.push( pair.to );\n\t\t\t\treturn true;\n\t\t\t}\n\t\t} );\n\n\t}\n\n\t/**\n\t * Identifies matching elements between slides.\n\t *\n\t * You can specify a custom matcher function by using\n\t * the `autoAnimateMatcher` config option.\n\t */\n\tgetAutoAnimatePairs( fromSlide, toSlide ) {\n\n\t\tlet pairs = [];\n\n\t\tconst codeNodes = 'pre';\n\t\tconst textNodes = 'h1, h2, h3, h4, h5, h6, p, li';\n\t\tconst mediaNodes = 'img, video, iframe';\n\n\t\t// Explicit matches via data-id\n\t\tthis.findAutoAnimateMatches( pairs, fromSlide, toSlide, '[data-id]', node => {\n\t\t\treturn node.nodeName + ':::' + node.getAttribute( 'data-id' );",
      "\t\t\t\tf;\n\n\t\t\tif( config.fragmentInURL ) {\n\t\t\t\tf = parseInt( bits[2], 10 );\n\t\t\t\tif( isNaN( f ) ) {\n\t\t\t\t\tf = undefined;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn { h, v, f };\n\t\t}\n\n\t\t// The hash couldn't be parsed or no matching named link was found\n\t\treturn null\n\n\t}\n\n\t/**\n\t * Reads the current URL (hash) and navigates accordingly.\n\t */\n\treadURL() {\n\n\t\tconst currentIndices = this.Reveal.getIndices();\n\t\tconst newIndices = this.getIndicesFromHash();\n\n\t\tif( newIndices ) {\n\t\t\tif( ( newIndices.h !== currentIndices.h || newIndices.v !== currentIndices.v || newIndices.f !== undefined ) ) {\n\t\t\t\t\tthis.Reveal.slide( newIndices.h, newIndices.v, newIndices.f );\n\t\t\t}\n\t\t}\n\t\t// If no new indices are available, we're trying to navigate to\n\t\t// a slide hash that does not exist\n\t\telse {\n\t\t\tthis.Reveal.slide( currentIndices.h || 0, currentIndices.v || 0 );\n\t\t}\n\n\t}\n\n\t/**\n\t * Updates the page URL (hash) to reflect the current\n\t * state.\n\t *\n\t * @param {number} delay The time in ms to wait before\n\t * writing the hash\n\t */\n\twriteURL( delay ) {\n\n\t\tlet config = this.Reveal.getConfig();\n\t\tlet currentSlide = this.Reveal.getCurrentSlide();\n\n\t\t// Make sure there's never more than one timeout running\n\t\tclearTimeout( this.writeURLTimeout );\n\n\t\t// If a delay is specified, timeout this call\n\t\tif( typeof delay === 'number' ) {\n\t\t\tthis.writeURLTimeout = setTimeout( this.writeURL, delay );\n\t\t}\n\t\telse if( currentSlide ) {\n\n\t\t\tlet hash = this.getHash();\n\n\t\t\t// If we're configured to push to history OR the history\n\t\t\t// API is not available.\n\t\t\tif( config.history ) {",
      "\t// - \"c\":\t  Flattened slide number\n\t// - \"c/t\":\t  Flattened slide number / total slides\n\t//\n\t// Alternatively, you can provide a function that returns the slide\n\t// number for the current slide. The function should take in a slide\n\t// object and return an array with one string [slideNumber] or\n\t// three strings [n1,delimiter,n2]. See #formatSlideNumber().\n\tslideNumber: false,\n\n\t// Can be used to limit the contexts in which the slide number appears\n\t// - \"all\":      Always show the slide number\n\t// - \"print\":    Only when printing to PDF\n\t// - \"speaker\":  Only in the speaker view\n\tshowSlideNumber: 'all',\n\n\t// Use 1 based indexing for # links to match slide number (default is zero\n\t// based)\n\thashOneBasedIndex: false,\n\n\t// Add the current slide number to the URL hash so that reloading the\n\t// page/copying the URL will return you to the same slide\n\thash: false,\n\n\t// Flags if we should monitor the hash and change slides accordingly\n\trespondToHashChanges: true,\n\n\t// Enable support for jump-to-slide navigation shortcuts\n\tjumpToSlide: true,\n\n\t// Push each slide change to the browser history.  Implies `hash: true`\n\thistory: false,\n\n\t// Enable keyboard shortcuts for navigation\n\tkeyboard: true,\n\n\t// Optional function that blocks keyboard events when retuning false\n\t//\n\t// If you set this to 'focused', we will only capture keyboard events\n\t// for embedded decks when they are in focus\n\tkeyboardCondition: null,\n\n\t// Disables the default reveal.js slide layout (scaling and centering)\n\t// so that you can use custom CSS layout\n\tdisableLayout: false,\n\n\t// Enable the slide overview mode\n\toverview: true,\n\n\t// Vertical centering of slides\n\tcenter: true,\n\n\t// Enables touch navigation on devices with touch input\n\ttouch: true,\n\n\t// Loop the presentation\n\tloop: false,\n\n\t// Change the presentation direction to be RTL\n\trtl: false,\n\n\t// Changes the behavior of our navigation directions.\n\t//\n\t// \"default\"\n\t// Left/right arrow keys step between horizontal slides, up/down",
      "\tfunction doSearch() {\n\t\t//if there's been a change in the search term, perform a new search:\n\t\tif (searchboxDirty) {\n\t\t\tvar searchstring = searchInput.value;\n\n\t\t\tif (searchstring === '') {\n\t\t\t\tif(hilitor) hilitor.remove();\n\t\t\t\tmatchedSlides = null;\n\t\t\t}\n\t\t\telse {\n\t\t\t\t//find the keyword amongst the slides\n\t\t\t\thilitor = new Hilitor(\"slidecontent\");\n\t\t\t\tmatchedSlides = hilitor.apply(searchstring);\n\t\t\t\tcurrentMatchedIndex = 0;\n\t\t\t}\n\t\t}\n\n\t\tif (matchedSlides) {\n\t\t\t//navigate to the next slide that has the keyword, wrapping to the first if necessary\n\t\t\tif (matchedSlides.length && (matchedSlides.length <= currentMatchedIndex)) {\n\t\t\t\tcurrentMatchedIndex = 0;\n\t\t\t}\n\t\t\tif (matchedSlides.length > currentMatchedIndex) {\n\t\t\t\tdeck.slide(matchedSlides[currentMatchedIndex].h, matchedSlides[currentMatchedIndex].v);\n\t\t\t\tcurrentMatchedIndex++;\n\t\t\t}\n\t\t}\n\t}\n\n\t// Original JavaScript code by Chirp Internet: www.chirp.com.au\n\t// Please acknowledge use of this code by including this header.\n\t// 2/2013 jon: modified regex to display any match, not restricted to word boundaries.\n\tfunction Hilitor(id, tag) {\n\n\t\tvar targetNode = document.getElementById(id) || document.body;\n\t\tvar hiliteTag = tag || \"EM\";\n\t\tvar skipTags = new RegExp(\"^(?:\" + hiliteTag + \"|SCRIPT|FORM)$\");\n\t\tvar colors = [\"#ff6\", \"#a0ffff\", \"#9f9\", \"#f99\", \"#f6f\"];\n\t\tvar wordColor = [];\n\t\tvar colorIdx = 0;\n\t\tvar matchRegex = \"\";\n\t\tvar matchingSlides = [];\n\n\t\tthis.setRegex = function(input)\n\t\t{\n\t\t\tinput = input.replace(/^[^\\w]+|[^\\w]+$/g, \"\").replace(/[^\\w'-]+/g, \"|\");\n\t\t\tmatchRegex = new RegExp(\"(\" + input + \")\",\"i\");\n\t\t}\n\n\t\tthis.getRegex = function()\n\t\t{\n\t\t\treturn matchRegex.toString().replace(/^\\/\\\\b\\(|\\)\\\\b\\/i$/g, \"\").replace(/\\|/g, \" \");\n\t\t}\n\n\t\t// recursively apply word highlighting\n\t\tthis.hiliteWords = function(node)\n\t\t{\n\t\t\tif(node == undefined || !node) return;\n\t\t\tif(!matchRegex) return;\n\t\t\tif(skipTags.test(node.nodeName)) return;\n\n\t\t\tif(node.hasChildNodes()) {\n\t\t\t\tfor(var i=0; i < node.childNodes.length; i++)\n\t\t\t\t\tthis.hiliteWords(node.childNodes[i]);",
      "\t *\n\t * @param {number} delay The time in ms to wait before\n\t * writing the hash\n\t */\n\twriteURL( delay ) {\n\n\t\tlet config = this.Reveal.getConfig();\n\t\tlet currentSlide = this.Reveal.getCurrentSlide();\n\n\t\t// Make sure there's never more than one timeout running\n\t\tclearTimeout( this.writeURLTimeout );\n\n\t\t// If a delay is specified, timeout this call\n\t\tif( typeof delay === 'number' ) {\n\t\t\tthis.writeURLTimeout = setTimeout( this.writeURL, delay );\n\t\t}\n\t\telse if( currentSlide ) {\n\n\t\t\tlet hash = this.getHash();\n\n\t\t\t// If we're configured to push to history OR the history\n\t\t\t// API is not available.\n\t\t\tif( config.history ) {\n\t\t\t\twindow.location.hash = hash;\n\t\t\t}\n\t\t\t// If we're configured to reflect the current slide in the\n\t\t\t// URL without pushing to history.\n\t\t\telse if( config.hash ) {\n\t\t\t\t// If the hash is empty, don't add it to the URL\n\t\t\t\tif( hash === '/' ) {\n\t\t\t\t\tthis.debouncedReplaceState( window.location.pathname + window.location.search );\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tthis.debouncedReplaceState( '#' + hash );\n\t\t\t\t}\n\t\t\t}\n\t\t\t// UPDATE: The below nuking of all hash changes breaks\n\t\t\t// anchors on pages where reveal.js is running. Removed\n\t\t\t// in 4.0. Why was it here in the first place? ¯\\_(ツ)_/¯\n\t\t\t//\n\t\t\t// If history and hash are both disabled, a hash may still\n\t\t\t// be added to the URL by clicking on a href with a hash\n\t\t\t// target. Counter this by always removing the hash.\n\t\t\t// else {\n\t\t\t// \twindow.history.replaceState( null, null, window.location.pathname + window.location.search );\n\t\t\t// }\n\n\t\t}\n\n\t}\n\n\treplaceState( url ) {\n\n\t\twindow.history.replaceState( null, null, url );\n\t\tthis.replaceStateTimestamp = Date.now();\n\n\t}\n\n\tdebouncedReplaceState( url ) {\n\n\t\tclearTimeout( this.replaceStateTimeout );\n\n\t\tif( Date.now() - this.replaceStateTimestamp > this.MAX_REPLACE_STATE_FREQUENCY ) {\n\t\t\tthis.replaceState( url );",
      " * This plugin is a wrapper for the MathJax2,\n * MathJax3 and KaTeX typesetter plugins.\n */\nexport default Plugin = Object.assign( defaultTypesetter(), {\n\tKaTeX,\n\tMathJax2,\n\tMathJax3\n} );",
      "\t}\n\n\tbind() {\n\n\t\tif( this.Reveal.getConfig().embedded ) {\n\t\t\tthis.Reveal.getRevealElement().addEventListener( 'pointerdown', this.onRevealPointerDown, false );\n\t\t}\n\n\t}\n\n\tunbind() {\n\n\t\tthis.Reveal.getRevealElement().removeEventListener( 'pointerdown', this.onRevealPointerDown, false );\n\t\tdocument.removeEventListener( 'pointerdown', this.onDocumentPointerDown, false );\n\n\t}\n\n\tfocus() {\n\n\t\tif( this.state !== STATE_FOCUS ) {\n\t\t\tthis.Reveal.getRevealElement().classList.add( 'focused' );\n\t\t\tdocument.addEventListener( 'pointerdown', this.onDocumentPointerDown, false );\n\t\t}\n\n\t\tthis.state = STATE_FOCUS;\n\n\t}\n\n\tblur() {\n\n\t\tif( this.state !== STATE_BLUR ) {\n\t\t\tthis.Reveal.getRevealElement().classList.remove( 'focused' );\n\t\t\tdocument.removeEventListener( 'pointerdown', this.onDocumentPointerDown, false );\n\t\t}\n\n\t\tthis.state = STATE_BLUR;\n\n\t}\n\n\tisFocused() {\n\n\t\treturn this.state === STATE_FOCUS;\n\n\t}\n\n\tdestroy() {\n\n\t\tthis.Reveal.getRevealElement().classList.remove( 'focused' );\n\n\t}\n\n\tonRevealPointerDown( event ) {\n\n\t\tthis.focus();\n\n\t}\n\n\tonDocumentPointerDown( event ) {\n\n\t\tlet revealElement = closest( event.target, '.reveal' );\n\t\tif( !revealElement || revealElement !== this.Reveal.getRevealElement() ) {\n\t\t\tthis.blur();\n\t\t}\n"
    ]
  },
  {
    "id": "yarnpkg/yarn",
    "org": "yarnpkg",
    "avatarURL": "https://avatars.githubusercontent.com/u/22247014?v=4",
    "name": "yarnpkg/yarn",
    "url": "https://github.com/yarnpkg/yarn",
    "lang": "JavaScript",
    "desc": "Fast, reliable, and secure dependency management.",
    "star_num": 41261,
    "fork_num": 2908,
    "snippets": [
      "/* @flow */\n\nmodule.exports = require(`./package.json`);\n\nfor (const key of [`dependencies`, `devDependencies`, `peerDependencies`]) {\n  for (const dep of Object.keys(module.exports[key] || {})) {\n    // $FlowFixMe The whole point of this file is to be dynamic\n    module.exports[key][dep] = require(dep);\n  }\n}\n",
      "  // pad `str`!\n  return pad + str;\n}\n",
      "        chop++;\n      }\n    } else if (input[0] === '\"') {\n      let i = 1;\n      for (; i < input.length; i++) {\n        if (input[i] === '\"') {\n          const isEscaped = input[i - 1] === '\\\\' && input[i - 2] !== '\\\\';\n          if (!isEscaped) {\n            i++;\n            break;\n          }\n        }\n      }\n      const val = input.substring(0, i);\n\n      chop = i;\n\n      try {\n        yield buildToken(TOKEN_TYPES.string, JSON.parse(val));\n      } catch (err) {\n        if (err instanceof SyntaxError) {\n          yield buildToken(TOKEN_TYPES.invalid);\n        } else {\n          throw err;\n        }\n      }\n    } else if (/^[0-9]/.test(input)) {\n      const val = /^[0-9]+/.exec(input)[0];\n      chop = val.length;\n\n      yield buildToken(TOKEN_TYPES.number, +val);\n    } else if (/^true/.test(input)) {\n      yield buildToken(TOKEN_TYPES.boolean, true);\n      chop = 4;\n    } else if (/^false/.test(input)) {\n      yield buildToken(TOKEN_TYPES.boolean, false);\n      chop = 5;\n    } else if (input[0] === ':') {\n      yield buildToken(TOKEN_TYPES.colon);\n      chop++;\n    } else if (input[0] === ',') {\n      yield buildToken(TOKEN_TYPES.comma);\n      chop++;\n    } else if (/^[a-zA-Z\\/.-]/g.test(input)) {\n      let i = 0;\n      for (; i < input.length; i++) {\n        const char = input[i];\n        if (char === ':' || char === ' ' || char === '\\n' || char === '\\r' || char === ',') {\n          break;\n        }\n      }\n      const name = input.substring(0, i);\n      chop = i;\n\n      yield buildToken(TOKEN_TYPES.string, name);\n    } else {\n      yield buildToken(TOKEN_TYPES.invalid);\n    }\n\n    if (!chop) {\n      // will trigger infinite recursion\n      yield buildToken(TOKEN_TYPES.invalid);\n    }\n",
      "      if (ws) {\n        if (!expectEligibility) {\n          expect(ws.nohoist).toBeUndefined();\n        } else {\n          expect(ws.nohoist).toEqual(nohoist);\n        }\n      } else {\n        if (expectEligibility) {\n          expect(ws).not.toBeUndefined();\n        }\n      }\n    }\n\n    testNohoistEligibility(true, false, false);\n    testNohoistEligibility(false, true, false);\n    testNohoistEligibility(true, true, true);\n  });\n  test('can throw exception for eligibility violation', async () => {\n    const config = await initConfig({});\n    const packages = ['w1', 'w2'];\n    const nohoist = ['a'];\n    const manifest = createWorkspaceManifest(packages, nohoist);\n\n    manifest.private = false;\n    const ws = config.getWorkspaces(manifest);\n    expect(ws).toBeUndefined();\n\n    try {\n      config.getWorkspaces(manifest, true);\n      expect(`exception to be thrown`).toEqual('but it did not...');\n    } catch (e) {\n      // ok\n    }\n\n    // should not thrown if manifest is eligible\n    manifest.private = true;\n    expect(config.getWorkspaces(manifest, true)).not.toBeUndefined();\n  });\n  test('can report eligibility warnings', async () => {\n    const config = await initConfig({});\n    const nohoist = ['a'];\n    const manifest = createWorkspaceManifest([], nohoist);\n\n    function getNohoist(ws: ?WorkspacesConfig): ?Array<string> {\n      return ws ? ws.nohoist : undefined;\n    }\n\n    const mockReporter = new MockReporter();\n    config.reporter = mockReporter;\n\n    // when everything is fine, reporter should be empty\n    expect(getNohoist(config.getWorkspaces(manifest, false))).not.toBeUndefined();\n    expect(mockReporter.numberOfCalls()).toEqual(0);\n\n    config.workspacesNohoistEnabled = false;\n    expect(getNohoist(config.getWorkspaces(manifest, false))).toBeUndefined();\n    expect(mockReporter.numberOfCalls()).toEqual(1);\n    expect(mockReporter.findCalls('workspacesNohoistDisabled').length).toEqual(1);\n\n    mockReporter.reset();\n\n    config.workspacesNohoistEnabled = true;\n    manifest.private = false;\n    expect(getNohoist(config.getWorkspaces(manifest, false))).toBeUndefined();",
      "});\n\ntest('isFakeRoot', () => {\n  const hasFakerootPreviously = 'FAKEROOTKEY' in process.env;\n  const oldValue = process.env.FAKEROOTKEY;\n  delete process.env.FAKEROOTKEY;\n\n  expect(isFakeRoot()).toBe(false);\n\n  process.env.FAKEROOTKEY = '15574641';\n  expect(isFakeRoot()).toBe(true);\n\n  if (hasFakerootPreviously) {\n    process.env.FAKEROOTKEY = oldValue;\n  }\n});\n",
      "  patterns.foobar1 = patterns.foobar2;\n\n  const actual = new Lockfile().getLockfile(patterns);\n\n  const expectedFoobar = {\n    name: 'foobar',\n    version: '0.0.0',\n    uid: undefined,\n    resolved: 'http://example.com/foobar',\n    registry: undefined,\n    dependencies: undefined,\n    optionalDependencies: undefined,\n    permissions: undefined,\n  };\n\n  const expected = {\n    foobar1: expectedFoobar,\n    foobar2: expectedFoobar,\n  };\n\n  expect(actual).toEqual(expected);\n});\n\ntest('Lockfile.getLockfile handles integrity field', () => {\n  const integrity = ssri.parse('sha1-foo sha512-bar');\n  const patterns = {\n    foobar: {\n      name: 'foobar',\n      version: '0.0.0',\n      uid: '0.0.0',\n      dependencies: {},\n      optionalDependencies: {},\n      _reference: {\n        permissions: {},\n      },\n      _remote: {\n        resolved: 'http://example.com/foobar',\n        registry: 'npm',\n        integrity,\n      },\n    },\n  };\n\n  const actual = new Lockfile().getLockfile(patterns);\n\n  const expected = {\n    foobar: {\n      version: '0.0.0',\n      uid: undefined,\n      resolved: 'http://example.com/foobar',\n      registry: undefined,\n      dependencies: undefined,\n      optionalDependencies: undefined,\n      permissions: undefined,\n      integrity: integrity.toString().split(' ').sort().join(' '),\n    },\n  };\n\n  expect(actual).toEqual(expected);\n});\n\ntest('parse single merge conflict', () => {\n  const file = `\na:",
      "'use strict';\n\nmodule.exports = function (str, len, ch) {\n  str = str + '';\n\n  len = len - str.length;\n  if (len <= 0) return str;\n\n  if (!ch && ch !== 0) ch = ' ';\n  ch = ch + '';\n\n  return ch.repeat(len) + str;\n};\n",
      "  expect(result).toContainPackage('c@1.0.0', atPath('a', 'node_modules', 'c'));\n  expect(result).toContainPackage('d@1.0.0', atPath('a', 'node_modules', 'd'));\n});\n\ndescribe('nohoist', () => {\n  test('nohoist can be turned off by disable workspaces (workspaces-experimental)', () => {\n    const {atPath, packageHoister, packageResolver} = createTestFixture(\n      {\n        'a@1.0.0': ['c@1.0.0'],\n        'b@2.0.0': [],\n        'c@1.0.0': [],\n      },\n      false,\n    );\n    const pkg: any = packageResolver.getStrictResolvedPattern('a@1.0.0');\n    pkg.workspaces = {nohoist: ['c']};\n\n    packageHoister.seed(['a@1.0.0', 'b@2.0.0']);\n    const result = packageHoister.init();\n\n    expect(result.length).toEqual(3);\n    expect(result).toContainPackage('a@1.0.0', atPath('a'));\n    expect(result).toContainPackage('b@2.0.0', atPath('b'));\n    expect(result).toContainPackage('c@1.0.0', atPath('c'));\n  });\n  describe('nohoist = hoist to the top of its branch instead of root', () => {\n    function fixture(): any {\n      return createTestFixture({\n        'a@1.0.0': ['c@1.0.0'],\n        'b@1.0.0': ['c@1.0.0'],\n        'c@1.0.0': ['d@1.0.0'],\n        'd@1.0.0': [],\n      });\n    }\n    test('shallow nohoist', () => {\n      const {atPath, packageHoister, packageResolver} = fixture();\n\n      // nohoist a/c\n      const pkg: any = packageResolver.getStrictResolvedPattern('a@1.0.0');\n      pkg.workspaces = {nohoist: ['c']};\n\n      packageHoister.seed(['a@1.0.0', 'b@1.0.0']);\n      const result = packageHoister.init();\n\n      expect(result.length).toEqual(5);\n      expect(result).toContainPackage('a@1.0.0', atPath('a'));\n      expect(result).toContainPackage('b@1.0.0', atPath('b'));\n      expect(result).toContainPackage('c@1.0.0', atPath('c'));\n      expect(result).toContainPackage('d@1.0.0', atPath('d'));\n      expect(result).toContainPackage('c@1.0.0', atPath('a', 'node_modules', 'c'));\n    });\n    test('deep nohoist', () => {\n      const {atPath, packageHoister, packageResolver} = fixture();\n\n      // nohoist a/c and everything under a/c\n      const pkg: any = packageResolver.getStrictResolvedPattern('a@1.0.0');\n      pkg.workspaces = {nohoist: ['c', 'c/**']};\n\n      packageHoister.seed(['a@1.0.0', 'b@1.0.0']);\n      const result = packageHoister.init();\n\n      expect(result.length).toEqual(6);\n      expect(result).toContainPackage('a@1.0.0', atPath('a'));\n      expect(result).toContainPackage('b@1.0.0', atPath('b'));",
      "/* @flow */\n\nmodule.exports = require(`./package.json`);\n\nfor (const key of [`dependencies`, `devDependencies`, `peerDependencies`]) {\n  for (const dep of Object.keys(module.exports[key] || {})) {\n    // $FlowFixMe The whole point of this file is to be dynamic\n    module.exports[key][dep] = require(dep);\n  }\n}\n",
      "\n    for (const workspaceName of Object.keys(workspaces)) {\n      for (const name of registryNames) {\n        const registry = config.registries[name];\n        locs.add(path.join(workspaces[workspaceName].loc, registry.folder));\n      }\n    }\n  }\n\n  for (const folder of locs) {\n    if (!await fs.exists(folder)) {\n      continue;\n    }\n\n    const spinner = reporter.activity();\n    const files = await fs.walk(folder);\n    const {ignoreFiles} = sortFilter(files, filters);\n    spinner.end();\n\n    const tick = reporter.progress(ignoreFiles.size);\n    // TODO make sure `main` field of all modules isn't ignored\n\n    for (const file of ignoreFiles) {\n      const loc = path.join(folder, file);\n      const stat = await fs.lstat(loc);\n      removedSize += stat.size;\n      removedFiles++;\n    }\n\n    for (const file of ignoreFiles) {\n      const loc = path.join(folder, file);\n      await fs.unlink(loc);\n      tick();\n    }\n  }\n\n  return {removedFiles, removedSize};\n}\n\nasync function runInit(cwd: string, reporter: Reporter): Promise<void> {\n  reporter.step(1, 1, reporter.lang('cleanCreatingFile', CLEAN_FILENAME));\n  const cleanLoc = path.join(cwd, CLEAN_FILENAME);\n  await fs.writeFile(cleanLoc, `${DEFAULT_FILTER}\\n`, {flag: 'wx'});\n  reporter.info(reporter.lang('cleanCreatedFile', CLEAN_FILENAME));\n}\n\nasync function runAutoClean(config: Config, reporter: Reporter): Promise<void> {\n  reporter.step(1, 1, reporter.lang('cleaning'));\n  const {removedFiles, removedSize} = await clean(config, reporter);\n  reporter.info(reporter.lang('cleanRemovedFiles', removedFiles));\n  reporter.info(reporter.lang('cleanSavedSize', Number((removedSize / 1024 / 1024).toFixed(2))));\n}\n\nasync function checkForCleanFile(cwd: string): Promise<boolean> {\n  const cleanLoc = path.join(cwd, CLEAN_FILENAME);\n  const exists = await fs.exists(cleanLoc);\n  return exists;\n}\n\nexport async function run(config: Config, reporter: Reporter, flags: Object, args: Array<string>): Promise<void> {\n  const cleanFileExists = await checkForCleanFile(config.cwd);\n\n  if (flags.init && cleanFileExists) {\n    reporter.info(reporter.lang('cleanAlreadyExists', CLEAN_FILENAME));"
    ]
  },
  {
    "id": "mrdoob/three.js",
    "org": "mrdoob",
    "avatarURL": "https://avatars.githubusercontent.com/u/97088?v=4",
    "name": "mrdoob/three.js",
    "url": "https://github.com/mrdoob/three.js",
    "lang": "JavaScript",
    "desc": "JavaScript 3D library.",
    "star_num": 94291,
    "fork_num": 34969,
    "snippets": [
      "\t\t\t\tthis.title.dom.dispatchEvent( new MouseEvent( 'dblclick' ) );\n\n\t\t\t} ) );\n\n\t\t};\n\n\t\tconst context = new ContextMenu( this.dom );\n\t\tcontext.addEventListener( 'show', onAddButtons );\n\n\t\tthis.title = title;\n\n\t\tif ( this.icon ) this.setIcon( 'ti ti-' + this.icon );\n\n\t\tthis.contextButton = contextButton;\n\t\tthis.context = context;\n\n\t\ttitle.addButton( contextButton );\n\n\t\tthis.add( title );\n\n\t\tthis.editor = null;\n\n\t\tthis.value = value;\n\n\t\tthis.onValidElement = onValidNode;\n\n\t\tthis.updateOutputConnection();\n\n\t}\n\n\tgetOutputType() {\n\n\t\treturn getTypeFromValue( this.value );\n\n\t}\n\n\tgetColor() {\n\n\t\treturn ( getColorFromType( this.getOutputType() ) || '#777777' ) + 'BB';\n\n\t}\n\n\thasJSON() {\n\n\t\treturn this.value && typeof this.value.toJSON === 'function';\n\n\t}\n\n\texportJSON() {\n\n\t\treturn this.value.toJSON();\n\n\t}\n\n\tserialize( data ) {\n\n\t\tsuper.serialize( data );\n\n\t\tdelete data.width;\n\n\t}\n\n\tdeserialize( data ) {\n",
      "} from 'three';\nimport { LineSegmentsGeometry } from '../lines/LineSegmentsGeometry.js';\n\nclass WireframeGeometry2 extends LineSegmentsGeometry {\n\n\tconstructor( geometry ) {\n\n\t\tsuper();\n\n\t\tthis.isWireframeGeometry2 = true;\n\n\t\tthis.type = 'WireframeGeometry2';\n\n\t\tthis.fromWireframeGeometry( new WireframeGeometry( geometry ) );\n\n\t\t// set colors, maybe\n\n\t}\n\n}\n\nexport { WireframeGeometry2 };\n",
      "\t\t\tif ( texture.image !== null ) {\n\n\t\t\t\tconst map = textureLoader.load( texture.image );\n\n\t\t\t\tswitch ( texture.type ) {\n\n\t\t\t\t\tcase 'Diffuse':\n\n\t\t\t\t\t\tmat.map = map;\n\n\t\t\t\t\t\tbreak;\n\n\t\t\t\t\tcase 'Bump':\n\n\t\t\t\t\t\tmat.bumpMap = map;\n\n\t\t\t\t\t\tbreak;\n\n\t\t\t\t\tcase 'Transparency':\n\n\t\t\t\t\t\tmat.alphaMap = map;\n\t\t\t\t\t\tmat.transparent = true;\n\n\t\t\t\t\t\tbreak;\n\n\t\t\t\t\tcase 'Emap':\n\n\t\t\t\t\t\tmat.envMap = map;\n\n\t\t\t\t\t\tbreak;\n\n\t\t\t\t}\n\n\t\t\t\tmap.wrapS = texture.wrapU === 0 ? RepeatWrapping : ClampToEdgeWrapping;\n\t\t\t\tmap.wrapT = texture.wrapV === 0 ? RepeatWrapping : ClampToEdgeWrapping;\n\t\t\t\tmap.repeat.set( texture.repeat[ 0 ], texture.repeat[ 1 ] );\n\n\t\t\t}\n\n\t\t}\n\n\t\treturn mat;\n\n\t}\n\n\t_createGeometry( data ) {\n\n\t\t// console.log(data);\n\n\t\tconst object = new Object3D();\n\t\tconst instanceDefinitionObjects = [];\n\t\tconst instanceDefinitions = [];\n\t\tconst instanceReferences = [];\n\n\t\tobject.userData[ 'layers' ] = data.layers;\n\t\tobject.userData[ 'groups' ] = data.groups;\n\t\tobject.userData[ 'settings' ] = data.settings;\n\t\tobject.userData[ 'objectType' ] = 'File3dm';\n\t\tobject.userData[ 'materials' ] = null;\n\t\tobject.name = this.url;\n\n\t\tlet objects = data.objects;\n\t\tconst materials = data.materials;\n",
      "\n\tconsole.log( JSON.stringify( countriesById, null, 2 ) );\n\n\tconst pick = pickCtx.getImageData( 0, 0, pickCtx.canvas.width, pickCtx.canvas.height );\n\tconst outline = outlineCtx.getImageData( 0, 0, outlineCtx.canvas.width, outlineCtx.canvas.height );\n\n\tfunction getId( imageData, x, y ) {\n\n\t\tconst off = ( ( ( y + imageData.height ) % imageData.height ) * imageData.width + ( ( x + imageData.width ) % imageData.width ) ) * 4;\n\t\treturn imageData.data[ off + 0 ] +\n           imageData.data[ off + 1 ] * 256 +\n           imageData.data[ off + 2 ] * 256 * 256 +\n           imageData.data[ off + 3 ] * 256 * 256 * 256;\n\n\t}\n\n\tfunction putPixel( imageData, x, y, color ) {\n\n\t\tconst off = ( y * imageData.width + x ) * 4;\n\t\timageData.data.set( color, off );\n\n\t}\n\n\n\tfor ( let y = 0; y < pick.height; ++ y ) {\n\n\t\tfor ( let x = 0; x < pick.width; ++ x ) {\n\n\t\t\tconst s = getId( pick, x, y );\n\t\t\tconst r = getId( pick, x + 1, y );\n\t\t\tconst d = getId( pick, x, y + 1 );\n\t\t\tlet v = 0;\n\t\t\tif ( s !== r || s !== d ) {\n\n\t\t\t\tv = 255;\n\n\t\t\t}\n\n\t\t\tputPixel( outline, x, y, [ v, v, v, v ] );\n\n\t\t}\n\n\t}\n\n\tfor ( let y = 0; y < outline.height; ++ y ) {\n\n\t\tfor ( let x = 0; x < outline.width; ++ x ) {\n\n\t\t\tconst s = getId( outline, x, y );\n\t\t\tconst l = getId( outline, x - 1, y );\n\t\t\tconst u = getId( outline, x, y - 1 );\n\t\t\tconst r = getId( outline, x + 1, y );\n\t\t\tconst d = getId( outline, x, y + 1 );\n\t\t\t//const rd = getId(outline, x + 1, y + 1);\n\t\t\tlet v = s;\n\t\t\tif ( ( s && r && d ) ||\n          ( s && l && d ) ||\n          ( s && r && u ) ||\n          ( s && l && u ) ) {\n\n\t\t\t\tv = 0;\n\n\t\t\t}\n",
      "export * from './BoxGeometry.js';\nexport * from './CapsuleGeometry.js';\nexport * from './CircleGeometry.js';\nexport * from './ConeGeometry.js';\nexport * from './CylinderGeometry.js';\nexport * from './DodecahedronGeometry.js';\nexport * from './EdgesGeometry.js';\nexport * from './ExtrudeGeometry.js';\nexport * from './IcosahedronGeometry.js';\nexport * from './LatheGeometry.js';\nexport * from './OctahedronGeometry.js';\nexport * from './PlaneGeometry.js';\nexport * from './PolyhedronGeometry.js';\nexport * from './RingGeometry.js';\nexport * from './ShapeGeometry.js';\nexport * from './SphereGeometry.js';\nexport * from './TetrahedronGeometry.js';\nexport * from './TorusGeometry.js';\nexport * from './TorusKnotGeometry.js';\nexport * from './TubeGeometry.js';\nexport * from './WireframeGeometry.js';\n",
      "        \"rotateOnAxis\": {\n          \"!type\": \"fn(axis: +THREE.Vector3, angle: number) -> +THREE.Object3D\",\n          \"!doc\": \"Rotate an object along an axis in object space. The axis is assumed to be normalized.\"\n        },\n        \"raycast\": {\n          \"!type\": \"fn(raycaster: +THREE.Raycaster, intersects: []) -> []\",\n          \"!doc\": \"Abstract method to get intersections between a casted ray and this object. Subclasses such as [page:Mesh], [page:Line], and [page:PointCloud] implement this method in order to participate in raycasting.\"\n        }\n      },\n      \"!doc\": \"Base class for scene graph objects.\",\n      \"!type\": \"fn()\"\n    },\n    \"Raycaster\": {\n      \"!url\": \"http://threejs.org/docs/#Reference/core/Raycaster\",\n      \"prototype\": {\n        \"ray\": {\n          \"!type\": \"+THREE.Ray\",\n          \"!doc\": \"The Ray used for the raycasting.\"\n        },\n        \"near\": {\n          \"!type\": \"float\",\n          \"!doc\": \"The near factor of the raycaster. This value indicates which objects can be discarded based on the distance.<br>\\n\\t\\tThis value shouldn't be negative and should be smaller than the far property.\"\n        },\n        \"far\": {\n          \"!type\": \"float\",\n          \"!doc\": \"The far factor of the raycaster. This value indicates which objects can be discarded based on the distance.<br>\\n\\t\\tThis value shouldn't be negative and should be larger than the near property.\"\n        },\n        \"precision\": {\n          \"!type\": \"float\",\n          \"!doc\": \"The precision factor of the raycaster when intersecting [page:Mesh] objects.\"\n        },\n        \"set\": {\n          \"!type\": \"fn(origin: +THREE.Vector3, direction: +THREE.Vector3)\",\n          \"!doc\": \"Updates the ray with a new origin and direction.\"\n        },\n        \"setFromCamera\": {\n          \"!type\": \"fn(coords: +THREE.Vector2, camera: +THREE.Camera)\",\n          \"!doc\": \"Updates the ray with a new origin and direction.\"\n        },\n        \"intersectObject\": {\n          \"!type\": \"fn(object: +THREE.Object3D, recursive: bool) -> []\",\n          \"!doc\": \"Checks all intersection between the ray and the object with or without the descendants. Intersections are returned sorted by distance, closest first. An array of intersections is returned...\\n        <code>\\n            [ { distance, point, face, faceIndex, indices, object }, ... ]\\n        </code>\\n        <p>\\n        [page:Float distance] – distance between the origin of the ray and the intersection<br>\\n        [page:Vector3 point] – point of intersection, in world coordinates<br>\\n        [page:Face3 face] – intersected face<br>\\n        [page:Integer faceIndex] – index of the intersected face<br>\\n        [page:Array indices] – indices of vertices comprising the intersected face<br>\\n        [page:Object3D object] – the intersected object\\n    \\t</p>\\n        <p>\\n        When intersecting a [page:Mesh] with a [page:BufferGeometry], the *faceIndex* will be *undefined*, and *indices* will be set; when intersecting a [page:Mesh] with a [page:Geometry], *indices* will be *undefined*. \\n        </p>\\n\\t\\t<p>\\n\\t\\t*Raycaster* delegates to the [page:Object3D.raycast raycast] method of the passed object, when evaluating whether the ray intersects the object or not. This allows [page:Mesh meshes] to respond differently to ray casting than [page:Line lines] and [page:PointCloud pointclouds].\\n\\t\\t</p>\\n\\t\\t<p>\\n\\t\\t*Note* that for meshes, faces must be pointed towards the origin of the [page:.ray ray] in order to be detected; intersections of the ray passing through the back of a face will not be detected. To raycast against both faces of an object, you'll want to set the [page:Mesh.material material]'s [page:Material.side side] property to *THREE.DoubleSide*.  \\n\\t\\t</p>\"\n        },\n        \"intersectObjects\": {\n          \"!type\": \"fn(objects: [], recursive: bool) -> []\",\n          \"!doc\": \"Checks all intersection between the ray and the objects with or without the descendants. Intersections are returned sorted by distance, closest first. Intersections are of the same form as those returned by [page:.intersectObject].\"\n        }\n      },\n      \"!doc\": \"This class makes raycasting easier. Raycasting is used for picking and more.\",\n      \"!type\": \"fn(origin: +THREE.Vector3, direction: +THREE.Vector3, near: number, far: number)\"\n    },\n    \"Lut\": {\n      \"!url\": \"http://threejs.org/docs/#Reference/examples/Lut\",\n      \"prototype\": {\n        \"minV\": {\n          \"!type\": \"number\",\n          \"!doc\": \"The minimum value to be represented with the lookup table. Default is 0.\"\n        },\n        \"maxV\": {\n          \"!type\": \"number\",\n          \"!doc\": \"The maximum value to be represented with the lookup table. Default is 1.\"\n        },\n        \"copy\": {\n          \"!type\": \"fn(lut: +THREE.Lut)\",",
      "\t\t_m1.elements[ 2 ] *= invSX;\n\n\t\t_m1.elements[ 4 ] *= invSY;\n\t\t_m1.elements[ 5 ] *= invSY;\n\t\t_m1.elements[ 6 ] *= invSY;\n\n\t\t_m1.elements[ 8 ] *= invSZ;\n\t\t_m1.elements[ 9 ] *= invSZ;\n\t\t_m1.elements[ 10 ] *= invSZ;\n\n\t\tquaternion.setFromRotationMatrix( _m1 );\n\n\t\tscale.x = sx;\n\t\tscale.y = sy;\n\t\tscale.z = sz;\n\n\t\treturn this;\n\n\t}\n\n\tmakePerspective( left, right, top, bottom, near, far, coordinateSystem = WebGLCoordinateSystem ) {\n\n\t\tconst te = this.elements;\n\t\tconst x = 2 * near / ( right - left );\n\t\tconst y = 2 * near / ( top - bottom );\n\n\t\tconst a = ( right + left ) / ( right - left );\n\t\tconst b = ( top + bottom ) / ( top - bottom );\n\n\t\tlet c, d;\n\n\t\tif ( coordinateSystem === WebGLCoordinateSystem ) {\n\n\t\t\tc = - ( far + near ) / ( far - near );\n\t\t\td = ( - 2 * far * near ) / ( far - near );\n\n\t\t} else if ( coordinateSystem === WebGPUCoordinateSystem ) {\n\n\t\t\tc = - far / ( far - near );\n\t\t\td = ( - far * near ) / ( far - near );\n\n\t\t} else {\n\n\t\t\tthrow new Error( 'THREE.Matrix4.makePerspective(): Invalid coordinate system: ' + coordinateSystem );\n\n\t\t}\n\n\t\tte[ 0 ] = x;\tte[ 4 ] = 0;\tte[ 8 ] = a; \tte[ 12 ] = 0;\n\t\tte[ 1 ] = 0;\tte[ 5 ] = y;\tte[ 9 ] = b; \tte[ 13 ] = 0;\n\t\tte[ 2 ] = 0;\tte[ 6 ] = 0;\tte[ 10 ] = c; \tte[ 14 ] = d;\n\t\tte[ 3 ] = 0;\tte[ 7 ] = 0;\tte[ 11 ] = - 1;\tte[ 15 ] = 0;\n\n\t\treturn this;\n\n\t}\n\n\tmakeOrthographic( left, right, top, bottom, near, far, coordinateSystem = WebGLCoordinateSystem ) {\n\n\t\tconst te = this.elements;\n\t\tconst w = 1.0 / ( right - left );\n\t\tconst h = 1.0 / ( top - bottom );\n\t\tconst p = 1.0 / ( far - near );\n\n\t\tconst x = ( right + left ) * w;",
      "\t#ifdef USE_SPECULAR_COLORMAP\n\t\tuniform sampler2D specularColorMap;\n\t#endif\n\n\t#ifdef USE_SPECULAR_INTENSITYMAP\n\t\tuniform sampler2D specularIntensityMap;\n\t#endif\n#endif\n\n#ifdef USE_CLEARCOAT\n\tuniform float clearcoat;\n\tuniform float clearcoatRoughness;\n#endif\n\n#ifdef USE_IRIDESCENCE\n\tuniform float iridescence;\n\tuniform float iridescenceIOR;\n\tuniform float iridescenceThicknessMinimum;\n\tuniform float iridescenceThicknessMaximum;\n#endif\n\n#ifdef USE_SHEEN\n\tuniform vec3 sheenColor;\n\tuniform float sheenRoughness;\n\n\t#ifdef USE_SHEEN_COLORMAP\n\t\tuniform sampler2D sheenColorMap;\n\t#endif\n\n\t#ifdef USE_SHEEN_ROUGHNESSMAP\n\t\tuniform sampler2D sheenRoughnessMap;\n\t#endif\n#endif\n\n#ifdef USE_ANISOTROPY\n\tuniform vec2 anisotropyVector;\n\n\t#ifdef USE_ANISOTROPYMAP\n\t\tuniform sampler2D anisotropyMap;\n\t#endif\n#endif\n\nvarying vec3 vViewPosition;\n\n#include <common>\n#include <packing>\n#include <dithering_pars_fragment>\n#include <color_pars_fragment>\n#include <uv_pars_fragment>\n#include <map_pars_fragment>\n#include <alphamap_pars_fragment>\n#include <alphatest_pars_fragment>\n#include <alphahash_pars_fragment>\n#include <aomap_pars_fragment>\n#include <lightmap_pars_fragment>\n#include <emissivemap_pars_fragment>\n#include <iridescence_fragment>\n#include <cube_uv_reflection_fragment>\n#include <envmap_common_pars_fragment>\n#include <envmap_physical_pars_fragment>\n#include <fog_pars_fragment>\n#include <lights_pars_begin>\n#include <normal_pars_fragment>\n#include <lights_physical_pars_fragment>",
      "\tconst style = document.createElement( 'style' );\n\tstyle.type = 'text/css';\n\tstyle.innerHTML = '#info, button, input, body > div.lil-gui, body > div.lbl { display: none !important; }';\n\n\tdocument.querySelector( 'head' ).appendChild( style );\n\n\t/* Remove Stats.js */\n\n\tfor ( const element of document.querySelectorAll( 'div' ) ) {\n\n\t\tif ( getComputedStyle( element ).zIndex === '10000' ) {\n\n\t\t\telement.remove();\n\t\t\tbreak;\n\n\t\t}\n\n\t}\n\n}() );\n",
      "#include <displacementmap_pars_vertex>\n#include <morphtarget_pars_vertex>\n#include <skinning_pars_vertex>\n#include <clipping_planes_pars_vertex>\n\nvoid main() {\n\n\t#include <uv_vertex>\n\n\t#include <skinbase_vertex>\n\n\t#ifdef USE_DISPLACEMENTMAP\n\n\t\t#include <beginnormal_vertex>\n\t\t#include <morphnormal_vertex>\n\t\t#include <skinnormal_vertex>\n\n\t#endif\n\n\t#include <begin_vertex>\n\t#include <morphtarget_vertex>\n\t#include <skinning_vertex>\n\t#include <displacementmap_vertex>\n\t#include <project_vertex>\n\t#include <worldpos_vertex>\n\t#include <clipping_planes_vertex>\n\n\tvWorldPosition = worldPosition.xyz;\n\n}\n`;\n\nexport const fragment = /* glsl */`\n#define DISTANCE\n\nuniform vec3 referencePosition;\nuniform float nearDistance;\nuniform float farDistance;\nvarying vec3 vWorldPosition;\n\n#include <common>\n#include <packing>\n#include <uv_pars_fragment>\n#include <map_pars_fragment>\n#include <alphamap_pars_fragment>\n#include <alphatest_pars_fragment>\n#include <alphahash_pars_fragment>\n#include <clipping_planes_pars_fragment>\n\nvoid main () {\n\n\t#include <clipping_planes_fragment>\n\n\tvec4 diffuseColor = vec4( 1.0 );\n\n\t#include <map_fragment>\n\t#include <alphamap_fragment>\n\t#include <alphatest_fragment>\n\t#include <alphahash_fragment>\n\n\tfloat dist = length( vWorldPosition - referencePosition );\n\tdist = ( dist - nearDistance ) / ( farDistance - nearDistance );\n\tdist = saturate( dist ); // clamp to [ 0, 1 ]\n"
    ]
  },
  {
    "id": "moment/moment",
    "org": "moment",
    "avatarURL": "https://avatars.githubusercontent.com/u/4129662?v=4",
    "name": "moment/moment",
    "url": "https://github.com/moment/moment",
    "lang": "JavaScript",
    "desc": "Parse, validate, manipulate, and display dates in JavaScript.",
    "star_num": 47531,
    "fork_num": 7185,
    "snippets": [
      "//! moment.js locale configuration\n//! locale : English (Canada) [en-ca]\n//! author : Jonathan Abourbih : https://github.com/jonbca\n\nimport moment from '../moment';\n\nexport default moment.defineLocale('en-ca', {\n    months: 'January_February_March_April_May_June_July_August_September_October_November_December'.split(\n        '_'\n    ),\n    monthsShort: 'Jan_Feb_Mar_Apr_May_Jun_Jul_Aug_Sep_Oct_Nov_Dec'.split('_'),\n    weekdays: 'Sunday_Monday_Tuesday_Wednesday_Thursday_Friday_Saturday'.split(\n        '_'\n    ),\n    weekdaysShort: 'Sun_Mon_Tue_Wed_Thu_Fri_Sat'.split('_'),\n    weekdaysMin: 'Su_Mo_Tu_We_Th_Fr_Sa'.split('_'),\n    longDateFormat: {\n        LT: 'h:mm A',\n        LTS: 'h:mm:ss A',\n        L: 'YYYY-MM-DD',\n        LL: 'MMMM D, YYYY',\n        LLL: 'MMMM D, YYYY h:mm A',\n        LLLL: 'dddd, MMMM D, YYYY h:mm A',\n    },\n    calendar: {\n        sameDay: '[Today at] LT',\n        nextDay: '[Tomorrow at] LT',\n        nextWeek: 'dddd [at] LT',\n        lastDay: '[Yesterday at] LT',\n        lastWeek: '[Last] dddd [at] LT',\n        sameElse: 'L',\n    },\n    relativeTime: {\n        future: 'in %s',\n        past: '%s ago',\n        s: 'a few seconds',\n        ss: '%d seconds',\n        m: 'a minute',\n        mm: '%d minutes',\n        h: 'an hour',\n        hh: '%d hours',\n        d: 'a day',\n        dd: '%d days',\n        M: 'a month',\n        MM: '%d months',\n        y: 'a year',\n        yy: '%d years',\n    },\n    dayOfMonthOrdinalParse: /\\d{1,2}(st|nd|rd|th)/,\n    ordinal: function (number) {\n        var b = number % 10,\n            output =\n                ~~((number % 100) / 10) === 1\n                    ? 'th'\n                    : b === 1\n                    ? 'st'\n                    : b === 2\n                    ? 'nd'\n                    : b === 3\n                    ? 'rd'\n                    : 'th';\n        return number + output;\n    },\n});",
      "//! author : Jatin Agrawal : https://github.com/jatinag22\n\nimport moment from '../moment';\n\nexport default moment.defineLocale('en-in', {\n    months: 'January_February_March_April_May_June_July_August_September_October_November_December'.split(\n        '_'\n    ),\n    monthsShort: 'Jan_Feb_Mar_Apr_May_Jun_Jul_Aug_Sep_Oct_Nov_Dec'.split('_'),\n    weekdays: 'Sunday_Monday_Tuesday_Wednesday_Thursday_Friday_Saturday'.split(\n        '_'\n    ),\n    weekdaysShort: 'Sun_Mon_Tue_Wed_Thu_Fri_Sat'.split('_'),\n    weekdaysMin: 'Su_Mo_Tu_We_Th_Fr_Sa'.split('_'),\n    longDateFormat: {\n        LT: 'h:mm A',\n        LTS: 'h:mm:ss A',\n        L: 'DD/MM/YYYY',\n        LL: 'D MMMM YYYY',\n        LLL: 'D MMMM YYYY h:mm A',\n        LLLL: 'dddd, D MMMM YYYY h:mm A',\n    },\n    calendar: {\n        sameDay: '[Today at] LT',\n        nextDay: '[Tomorrow at] LT',\n        nextWeek: 'dddd [at] LT',\n        lastDay: '[Yesterday at] LT',\n        lastWeek: '[Last] dddd [at] LT',\n        sameElse: 'L',\n    },\n    relativeTime: {\n        future: 'in %s',\n        past: '%s ago',\n        s: 'a few seconds',\n        ss: '%d seconds',\n        m: 'a minute',\n        mm: '%d minutes',\n        h: 'an hour',\n        hh: '%d hours',\n        d: 'a day',\n        dd: '%d days',\n        M: 'a month',\n        MM: '%d months',\n        y: 'a year',\n        yy: '%d years',\n    },\n    dayOfMonthOrdinalParse: /\\d{1,2}(st|nd|rd|th)/,\n    ordinal: function (number) {\n        var b = number % 10,\n            output =\n                ~~((number % 100) / 10) === 1\n                    ? 'th'\n                    : b === 1\n                    ? 'st'\n                    : b === 2\n                    ? 'nd'\n                    : b === 3\n                    ? 'rd'\n                    : 'th';\n        return number + output;\n    },\n    week: {\n        dow: 0, // Sunday is the first day of the week.\n        doy: 6, // The week that contains Jan 1st is the first week of the year.",
      "//! moment.js locale configuration\n//! locale : Italian (Switzerland) [it-ch]\n//! author : xfh : https://github.com/xfh\n\nimport moment from '../moment';\n\nexport default moment.defineLocale('it-ch', {\n    months: 'gennaio_febbraio_marzo_aprile_maggio_giugno_luglio_agosto_settembre_ottobre_novembre_dicembre'.split(\n        '_'\n    ),\n    monthsShort: 'gen_feb_mar_apr_mag_giu_lug_ago_set_ott_nov_dic'.split('_'),\n    weekdays: 'domenica_lunedì_martedì_mercoledì_giovedì_venerdì_sabato'.split(\n        '_'\n    ),\n    weekdaysShort: 'dom_lun_mar_mer_gio_ven_sab'.split('_'),\n    weekdaysMin: 'do_lu_ma_me_gi_ve_sa'.split('_'),\n    longDateFormat: {\n        LT: 'HH:mm',\n        LTS: 'HH:mm:ss',\n        L: 'DD.MM.YYYY',\n        LL: 'D MMMM YYYY',\n        LLL: 'D MMMM YYYY HH:mm',\n        LLLL: 'dddd D MMMM YYYY HH:mm',\n    },\n    calendar: {\n        sameDay: '[Oggi alle] LT',\n        nextDay: '[Domani alle] LT',\n        nextWeek: 'dddd [alle] LT',\n        lastDay: '[Ieri alle] LT',\n        lastWeek: function () {\n            switch (this.day()) {\n                case 0:\n                    return '[la scorsa] dddd [alle] LT';\n                default:\n                    return '[lo scorso] dddd [alle] LT';\n            }\n        },\n        sameElse: 'L',\n    },\n    relativeTime: {\n        future: function (s) {\n            return (/^[0-9].+$/.test(s) ? 'tra' : 'in') + ' ' + s;\n        },\n        past: '%s fa',\n        s: 'alcuni secondi',\n        ss: '%d secondi',\n        m: 'un minuto',\n        mm: '%d minuti',\n        h: \"un'ora\",\n        hh: '%d ore',\n        d: 'un giorno',\n        dd: '%d giorni',\n        M: 'un mese',\n        MM: '%d mesi',\n        y: 'un anno',\n        yy: '%d anni',\n    },\n    dayOfMonthOrdinalParse: /\\d{1,2}º/,\n    ordinal: '%dº',\n    week: {\n        dow: 1, // Monday is the first day of the week.\n        doy: 4, // The week that contains Jan 4th is the first week of the year.\n    },\n});",
      "\n    //! moment.js locale configuration\n\n    /*jshint -W100*/\n    var si = moment.defineLocale('si', {\n        months: 'ජනවාරි_පෙබරවාරි_මාර්තු_අප්‍රේල්_මැයි_ජූනි_ජූලි_අගෝස්තු_සැප්තැම්බර්_ඔක්තෝබර්_නොවැම්බර්_දෙසැම්බර්'.split(\n            '_'\n        ),\n        monthsShort: 'ජන_පෙබ_මාර්_අප්_මැයි_ජූනි_ජූලි_අගෝ_සැප්_ඔක්_නොවැ_දෙසැ'.split(\n            '_'\n        ),\n        weekdays:\n            'ඉරිදා_සඳුදා_අඟහරුවාදා_බදාදා_බ්‍රහස්පතින්දා_සිකුරාදා_සෙනසුරාදා'.split(\n                '_'\n            ),\n        weekdaysShort: 'ඉරි_සඳු_අඟ_බදා_බ්‍රහ_සිකු_සෙන'.split('_'),\n        weekdaysMin: 'ඉ_ස_අ_බ_බ්‍ර_සි_සෙ'.split('_'),\n        weekdaysParseExact: true,\n        longDateFormat: {\n            LT: 'a h:mm',\n            LTS: 'a h:mm:ss',\n            L: 'YYYY/MM/DD',\n            LL: 'YYYY MMMM D',\n            LLL: 'YYYY MMMM D, a h:mm',\n            LLLL: 'YYYY MMMM D [වැනි] dddd, a h:mm:ss',\n        },\n        calendar: {\n            sameDay: '[අද] LT[ට]',\n            nextDay: '[හෙට] LT[ට]',\n            nextWeek: 'dddd LT[ට]',\n            lastDay: '[ඊයේ] LT[ට]',\n            lastWeek: '[පසුගිය] dddd LT[ට]',\n            sameElse: 'L',\n        },\n        relativeTime: {\n            future: '%sකින්',\n            past: '%sකට පෙර',\n            s: 'තත්පර කිහිපය',\n            ss: 'තත්පර %d',\n            m: 'මිනිත්තුව',\n            mm: 'මිනිත්තු %d',\n            h: 'පැය',\n            hh: 'පැය %d',\n            d: 'දිනය',\n            dd: 'දින %d',\n            M: 'මාසය',\n            MM: 'මාස %d',\n            y: 'වසර',\n            yy: 'වසර %d',\n        },\n        dayOfMonthOrdinalParse: /\\d{1,2} වැනි/,\n        ordinal: function (number) {\n            return number + ' වැනි';\n        },\n        meridiemParse: /පෙර වරු|පස් වරු|පෙ.ව|ප.ව./,\n        isPM: function (input) {\n            return input === 'ප.ව.' || input === 'පස් වරු';\n        },\n        meridiem: function (hours, minutes, isLower) {\n            if (hours > 11) {\n                return isLower ? 'ප.ව.' : 'පස් වරු';\n            } else {\n                return isLower ? 'පෙ.ව.' : 'පෙර වරු';\n            }",
      "\n;(function (global, factory) {\n   typeof exports === 'object' && typeof module !== 'undefined'\n       && typeof require === 'function' ? factory(require('../moment')) :\n   typeof define === 'function' && define.amd ? define(['../moment'], factory) :\n   factory(global.moment)\n}(this, (function (moment) { 'use strict';\n\n    //! moment.js locale configuration\n\n    var translator = {\n        words: {\n            //Different grammatical cases\n            ss: ['sekunda', 'sekunde', 'sekundi'],\n            m: ['jedan minut', 'jednog minuta'],\n            mm: ['minut', 'minuta', 'minuta'],\n            h: ['jedan sat', 'jednog sata'],\n            hh: ['sat', 'sata', 'sati'],\n            d: ['jedan dan', 'jednog dana'],\n            dd: ['dan', 'dana', 'dana'],\n            M: ['jedan mesec', 'jednog meseca'],\n            MM: ['mesec', 'meseca', 'meseci'],\n            y: ['jednu godinu', 'jedne godine'],\n            yy: ['godinu', 'godine', 'godina'],\n        },\n        correctGrammaticalCase: function (number, wordKey) {\n            if (\n                number % 10 >= 1 &&\n                number % 10 <= 4 &&\n                (number % 100 < 10 || number % 100 >= 20)\n            ) {\n                return number % 10 === 1 ? wordKey[0] : wordKey[1];\n            }\n            return wordKey[2];\n        },\n        translate: function (number, withoutSuffix, key, isFuture) {\n            var wordKey = translator.words[key],\n                word;\n\n            if (key.length === 1) {\n                // Nominativ\n                if (key === 'y' && withoutSuffix) return 'jedna godina';\n                return isFuture || withoutSuffix ? wordKey[0] : wordKey[1];\n            }\n\n            word = translator.correctGrammaticalCase(number, wordKey);\n            // Nominativ\n            if (key === 'yy' && withoutSuffix && word === 'godinu') {\n                return number + ' godina';\n            }\n\n            return number + ' ' + word;\n        },\n    };\n\n    var sr = moment.defineLocale('sr', {\n        months: 'januar_februar_mart_april_maj_jun_jul_avgust_septembar_oktobar_novembar_decembar'.split(\n            '_'\n        ),\n        monthsShort:\n            'jan._feb._mar._apr._maj_jun_jul_avg._sep._okt._nov._dec.'.split('_'),\n        monthsParseExact: true,\n        weekdays: 'nedelja_ponedeljak_utorak_sreda_četvrtak_petak_subota'.split(\n            '_'",
      "export default moment.defineLocale('gom-latn', {\n    months: {\n        standalone:\n            'Janer_Febrer_Mars_Abril_Mai_Jun_Julai_Agost_Setembr_Otubr_Novembr_Dezembr'.split(\n                '_'\n            ),\n        format: 'Janerachea_Febrerachea_Marsachea_Abrilachea_Maiachea_Junachea_Julaiachea_Agostachea_Setembrachea_Otubrachea_Novembrachea_Dezembrachea'.split(\n            '_'\n        ),\n        isFormat: /MMMM(\\s)+D[oD]?/,\n    },\n    monthsShort:\n        'Jan._Feb._Mars_Abr._Mai_Jun_Jul._Ago._Set._Otu._Nov._Dez.'.split('_'),\n    monthsParseExact: true,\n    weekdays: \"Aitar_Somar_Mongllar_Budhvar_Birestar_Sukrar_Son'var\".split('_'),\n    weekdaysShort: 'Ait._Som._Mon._Bud._Bre._Suk._Son.'.split('_'),\n    weekdaysMin: 'Ai_Sm_Mo_Bu_Br_Su_Sn'.split('_'),\n    weekdaysParseExact: true,\n    longDateFormat: {\n        LT: 'A h:mm [vazta]',\n        LTS: 'A h:mm:ss [vazta]',\n        L: 'DD-MM-YYYY',\n        LL: 'D MMMM YYYY',\n        LLL: 'D MMMM YYYY A h:mm [vazta]',\n        LLLL: 'dddd, MMMM Do, YYYY, A h:mm [vazta]',\n        llll: 'ddd, D MMM YYYY, A h:mm [vazta]',\n    },\n    calendar: {\n        sameDay: '[Aiz] LT',\n        nextDay: '[Faleam] LT',\n        nextWeek: '[Fuddlo] dddd[,] LT',\n        lastDay: '[Kal] LT',\n        lastWeek: '[Fattlo] dddd[,] LT',\n        sameElse: 'L',\n    },\n    relativeTime: {\n        future: '%s',\n        past: '%s adim',\n        s: processRelativeTime,\n        ss: processRelativeTime,\n        m: processRelativeTime,\n        mm: processRelativeTime,\n        h: processRelativeTime,\n        hh: processRelativeTime,\n        d: processRelativeTime,\n        dd: processRelativeTime,\n        M: processRelativeTime,\n        MM: processRelativeTime,\n        y: processRelativeTime,\n        yy: processRelativeTime,\n    },\n    dayOfMonthOrdinalParse: /\\d{1,2}(er)/,\n    ordinal: function (number, period) {\n        switch (period) {\n            // the ordinal 'er' only applies to day of the month\n            case 'D':\n                return number + 'er';\n            default:\n            case 'M':\n            case 'Q':\n            case 'DDD':\n            case 'd':\n            case 'w':\n            case 'W':",
      "        case 'd':\n            return 'egy' + (isFuture || withoutSuffix ? ' nap' : ' napja');\n        case 'dd':\n            return num + (isFuture || withoutSuffix ? ' nap' : ' napja');\n        case 'M':\n            return 'egy' + (isFuture || withoutSuffix ? ' hónap' : ' hónapja');\n        case 'MM':\n            return num + (isFuture || withoutSuffix ? ' hónap' : ' hónapja');\n        case 'y':\n            return 'egy' + (isFuture || withoutSuffix ? ' év' : ' éve');\n        case 'yy':\n            return num + (isFuture || withoutSuffix ? ' év' : ' éve');\n    }\n    return '';\n}\nfunction week(isFuture) {\n    return (\n        (isFuture ? '' : '[múlt] ') +\n        '[' +\n        weekEndings[this.day()] +\n        '] LT[-kor]'\n    );\n}\n\nexport default moment.defineLocale('hu', {\n    months: 'január_február_március_április_május_június_július_augusztus_szeptember_október_november_december'.split(\n        '_'\n    ),\n    monthsShort:\n        'jan._feb._márc._ápr._máj._jún._júl._aug._szept._okt._nov._dec.'.split(\n            '_'\n        ),\n    monthsParseExact: true,\n    weekdays: 'vasárnap_hétfő_kedd_szerda_csütörtök_péntek_szombat'.split('_'),\n    weekdaysShort: 'vas_hét_kedd_sze_csüt_pén_szo'.split('_'),\n    weekdaysMin: 'v_h_k_sze_cs_p_szo'.split('_'),\n    longDateFormat: {\n        LT: 'H:mm',\n        LTS: 'H:mm:ss',\n        L: 'YYYY.MM.DD.',\n        LL: 'YYYY. MMMM D.',\n        LLL: 'YYYY. MMMM D. H:mm',\n        LLLL: 'YYYY. MMMM D., dddd H:mm',\n    },\n    meridiemParse: /de|du/i,\n    isPM: function (input) {\n        return input.charAt(1).toLowerCase() === 'u';\n    },\n    meridiem: function (hours, minutes, isLower) {\n        if (hours < 12) {\n            return isLower === true ? 'de' : 'DE';\n        } else {\n            return isLower === true ? 'du' : 'DU';\n        }\n    },\n    calendar: {\n        sameDay: '[ma] LT[-kor]',\n        nextDay: '[holnap] LT[-kor]',\n        nextWeek: function () {\n            return week.call(this, true);\n        },\n        lastDay: '[tegnap] LT[-kor]',\n        lastWeek: function () {\n            return week.call(this, false);",
      "            M: [\n                'أقل من شهر',\n                'شهر واحد',\n                ['شهران', 'شهرين'],\n                '%d أشهر',\n                '%d شهرا',\n                '%d شهر',\n            ],\n            y: [\n                'أقل من عام',\n                'عام واحد',\n                ['عامان', 'عامين'],\n                '%d أعوام',\n                '%d عامًا',\n                '%d عام',\n            ],\n        },\n        pluralize = function (u) {\n            return function (number, withoutSuffix, string, isFuture) {\n                var f = pluralForm(number),\n                    str = plurals[u][pluralForm(number)];\n                if (f === 2) {\n                    str = str[withoutSuffix ? 0 : 1];\n                }\n                return str.replace(/%d/i, number);\n            };\n        },\n        months = [\n            'يناير',\n            'فبراير',\n            'مارس',\n            'أبريل',\n            'مايو',\n            'يونيو',\n            'يوليو',\n            'أغسطس',\n            'سبتمبر',\n            'أكتوبر',\n            'نوفمبر',\n            'ديسمبر',\n        ];\n\n    var ar = moment.defineLocale('ar', {\n        months: months,\n        monthsShort: months,\n        weekdays: 'الأحد_الإثنين_الثلاثاء_الأربعاء_الخميس_الجمعة_السبت'.split('_'),\n        weekdaysShort: 'أحد_إثنين_ثلاثاء_أربعاء_خميس_جمعة_سبت'.split('_'),\n        weekdaysMin: 'ح_ن_ث_ر_خ_ج_س'.split('_'),\n        weekdaysParseExact: true,\n        longDateFormat: {\n            LT: 'HH:mm',\n            LTS: 'HH:mm:ss',\n            L: 'D/\\u200FM/\\u200FYYYY',\n            LL: 'D MMMM YYYY',\n            LLL: 'D MMMM YYYY HH:mm',\n            LLLL: 'dddd D MMMM YYYY HH:mm',\n        },\n        meridiemParse: /ص|م/,\n        isPM: function (input) {\n            return 'م' === input;\n        },\n        meridiem: function (hour, minute, isLower) {\n            if (hour < 12) {\n                return 'ص';",
      "            ) {\n                // if there is a day number before 'MMMM'\n                return this._monthsGenitiveEl[momentToFormat.month()];\n            } else {\n                return this._monthsNominativeEl[momentToFormat.month()];\n            }\n        },\n        monthsShort: 'Ιαν_Φεβ_Μαρ_Απρ_Μαϊ_Ιουν_Ιουλ_Αυγ_Σεπ_Οκτ_Νοε_Δεκ'.split('_'),\n        weekdays: 'Κυριακή_Δευτέρα_Τρίτη_Τετάρτη_Πέμπτη_Παρασκευή_Σάββατο'.split(\n            '_'\n        ),\n        weekdaysShort: 'Κυρ_Δευ_Τρι_Τετ_Πεμ_Παρ_Σαβ'.split('_'),\n        weekdaysMin: 'Κυ_Δε_Τρ_Τε_Πε_Πα_Σα'.split('_'),\n        meridiem: function (hours, minutes, isLower) {\n            if (hours > 11) {\n                return isLower ? 'μμ' : 'ΜΜ';\n            } else {\n                return isLower ? 'πμ' : 'ΠΜ';\n            }\n        },\n        isPM: function (input) {\n            return (input + '').toLowerCase()[0] === 'μ';\n        },\n        meridiemParse: /[ΠΜ]\\.?Μ?\\.?/i,\n        longDateFormat: {\n            LT: 'h:mm A',\n            LTS: 'h:mm:ss A',\n            L: 'DD/MM/YYYY',\n            LL: 'D MMMM YYYY',\n            LLL: 'D MMMM YYYY h:mm A',\n            LLLL: 'dddd, D MMMM YYYY h:mm A',\n        },\n        calendarEl: {\n            sameDay: '[Σήμερα {}] LT',\n            nextDay: '[Αύριο {}] LT',\n            nextWeek: 'dddd [{}] LT',\n            lastDay: '[Χθες {}] LT',\n            lastWeek: function () {\n                switch (this.day()) {\n                    case 6:\n                        return '[το προηγούμενο] dddd [{}] LT';\n                    default:\n                        return '[την προηγούμενη] dddd [{}] LT';\n                }\n            },\n            sameElse: 'L',\n        },\n        calendar: function (key, mom) {\n            var output = this._calendarEl[key],\n                hours = mom && mom.hours();\n            if (isFunction(output)) {\n                output = output.apply(mom);\n            }\n            return output.replace('{}', hours % 12 === 1 ? 'στη' : 'στις');\n        },\n        relativeTime: {\n            future: 'σε %s',\n            past: '%s πριν',\n            s: 'λίγα δευτερόλεπτα',\n            ss: '%d δευτερόλεπτα',\n            m: 'ένα λεπτό',\n            mm: '%d λεπτά',\n            h: 'μία ώρα',\n            hh: '%d ώρες',"
    ]
  },
  {
    "id": "laravel/laravel",
    "org": "laravel",
    "avatarURL": "https://avatars.githubusercontent.com/u/958072?v=4",
    "name": "laravel/laravel",
    "url": "https://github.com/laravel/laravel",
    "lang": "PHP",
    "desc": "A PHP framework for web artisans.",
    "star_num": 74588,
    "fork_num": 23967,
    "snippets": [
      "     */\n    protected $except = [\n        //\n    ];\n}\n",
      "\n    'deprecations' => [\n        'channel' => env('LOG_DEPRECATIONS_CHANNEL', 'null'),\n        'trace' => false,\n    ],\n\n    /*\n    |--------------------------------------------------------------------------\n    | Log Channels\n    |--------------------------------------------------------------------------\n    |\n    | Here you may configure the log channels for your application. Out of\n    | the box, Laravel uses the Monolog PHP logging library. This gives\n    | you a variety of powerful log handlers / formatters to utilize.\n    |\n    | Available Drivers: \"single\", \"daily\", \"slack\", \"syslog\",\n    |                    \"errorlog\", \"monolog\",\n    |                    \"custom\", \"stack\"\n    |\n    */\n\n    'channels' => [\n        'stack' => [\n            'driver' => 'stack',\n            'channels' => ['single'],\n            'ignore_exceptions' => false,\n        ],\n\n        'single' => [\n            'driver' => 'single',\n            'path' => storage_path('logs/laravel.log'),\n            'level' => env('LOG_LEVEL', 'debug'),\n            'replace_placeholders' => true,\n        ],\n\n        'daily' => [\n            'driver' => 'daily',\n            'path' => storage_path('logs/laravel.log'),\n            'level' => env('LOG_LEVEL', 'debug'),\n            'days' => 14,\n            'replace_placeholders' => true,\n        ],\n\n        'slack' => [\n            'driver' => 'slack',\n            'url' => env('LOG_SLACK_WEBHOOK_URL'),\n            'username' => 'Laravel Log',\n            'emoji' => ':boom:',\n            'level' => env('LOG_LEVEL', 'critical'),\n            'replace_placeholders' => true,\n        ],\n\n        'papertrail' => [\n            'driver' => 'monolog',\n            'level' => env('LOG_LEVEL', 'debug'),\n            'handler' => env('LOG_PAPERTRAIL_HANDLER', SyslogUdpHandler::class),\n            'handler_with' => [\n                'host' => env('PAPERTRAIL_URL'),\n                'port' => env('PAPERTRAIL_PORT'),\n                'connectionString' => 'tls://'.env('PAPERTRAIL_URL').':'.env('PAPERTRAIL_PORT'),\n            ],\n            'processors' => [PsrLogMessageProcessor::class],\n        ],\n",
      "    protected $hidden = [\n        'password',\n        'remember_token',\n    ];\n\n    /**\n     * The attributes that should be cast.\n     *\n     * @var array<string, string>\n     */\n    protected $casts = [\n        'email_verified_at' => 'datetime',\n        'password' => 'hashed',\n    ];\n}\n",
      "<?php\n\nnamespace Tests;\n\nuse Illuminate\\Foundation\\Testing\\TestCase as BaseTestCase;\n\nabstract class TestCase extends BaseTestCase\n{\n    use CreatesApplication;\n}\n",
      "            'channels' => ['single'],\n            'ignore_exceptions' => false,\n        ],\n\n        'single' => [\n            'driver' => 'single',\n            'path' => storage_path('logs/laravel.log'),\n            'level' => env('LOG_LEVEL', 'debug'),\n            'replace_placeholders' => true,\n        ],\n\n        'daily' => [\n            'driver' => 'daily',\n            'path' => storage_path('logs/laravel.log'),\n            'level' => env('LOG_LEVEL', 'debug'),\n            'days' => 14,\n            'replace_placeholders' => true,\n        ],\n\n        'slack' => [\n            'driver' => 'slack',\n            'url' => env('LOG_SLACK_WEBHOOK_URL'),\n            'username' => 'Laravel Log',\n            'emoji' => ':boom:',\n            'level' => env('LOG_LEVEL', 'critical'),\n            'replace_placeholders' => true,\n        ],\n\n        'papertrail' => [\n            'driver' => 'monolog',\n            'level' => env('LOG_LEVEL', 'debug'),\n            'handler' => env('LOG_PAPERTRAIL_HANDLER', SyslogUdpHandler::class),\n            'handler_with' => [\n                'host' => env('PAPERTRAIL_URL'),\n                'port' => env('PAPERTRAIL_PORT'),\n                'connectionString' => 'tls://'.env('PAPERTRAIL_URL').':'.env('PAPERTRAIL_PORT'),\n            ],\n            'processors' => [PsrLogMessageProcessor::class],\n        ],\n\n        'stderr' => [\n            'driver' => 'monolog',\n            'level' => env('LOG_LEVEL', 'debug'),\n            'handler' => StreamHandler::class,\n            'formatter' => env('LOG_STDERR_FORMATTER'),\n            'with' => [\n                'stream' => 'php://stderr',\n            ],\n            'processors' => [PsrLogMessageProcessor::class],\n        ],\n\n        'syslog' => [\n            'driver' => 'syslog',\n            'level' => env('LOG_LEVEL', 'debug'),\n            'facility' => LOG_USER,\n            'replace_placeholders' => true,\n        ],\n\n        'errorlog' => [\n            'driver' => 'errorlog',\n            'level' => env('LOG_LEVEL', 'debug'),\n            'replace_placeholders' => true,\n        ],\n",
      "    | users are actually retrieved out of your database or other storage\n    | mechanisms used by this application to persist your user's data.\n    |\n    | Supported: \"session\"\n    |\n    */\n\n    'guards' => [\n        'web' => [\n            'driver' => 'session',\n            'provider' => 'users',\n        ],\n    ],\n\n    /*\n    |--------------------------------------------------------------------------\n    | User Providers\n    |--------------------------------------------------------------------------\n    |\n    | All authentication drivers have a user provider. This defines how the\n    | users are actually retrieved out of your database or other storage\n    | mechanisms used by this application to persist your user's data.\n    |\n    | If you have multiple user tables or models you may configure multiple\n    | sources which represent each model / table. These sources may then\n    | be assigned to any extra authentication guards you have defined.\n    |\n    | Supported: \"database\", \"eloquent\"\n    |\n    */\n\n    'providers' => [\n        'users' => [\n            'driver' => 'eloquent',\n            'model' => App\\Models\\User::class,\n        ],\n\n        // 'users' => [\n        //     'driver' => 'database',\n        //     'table' => 'users',\n        // ],\n    ],\n\n    /*\n    |--------------------------------------------------------------------------\n    | Resetting Passwords\n    |--------------------------------------------------------------------------\n    |\n    | You may specify multiple password reset configurations if you have more\n    | than one user table or model in the application and you want to have\n    | separate password reset settings based on the specific user types.\n    |\n    | The expiry time is the number of minutes that each reset token will be\n    | considered valid. This security feature keeps tokens short-lived so\n    | they have less time to be guessed. You may change this as needed.\n    |\n    | The throttle setting is the number of seconds a user must wait before\n    | generating more password reset tokens. This prevents the user from\n    | quickly generating a very large amount of password reset tokens.\n    |\n    */\n\n    'passwords' => [\n        'users' => [",
      "\nuse Laravel\\Sanctum\\Sanctum;\n\nreturn [\n\n    /*\n    |--------------------------------------------------------------------------\n    | Stateful Domains\n    |--------------------------------------------------------------------------\n    |\n    | Requests from the following domains / hosts will receive stateful API\n    | authentication cookies. Typically, these should include your local\n    | and production domains which access your API via a frontend SPA.\n    |\n    */\n\n    'stateful' => explode(',', env('SANCTUM_STATEFUL_DOMAINS', sprintf(\n        '%s%s',\n        'localhost,localhost:3000,127.0.0.1,127.0.0.1:8000,::1',\n        Sanctum::currentApplicationUrlWithPort()\n    ))),\n\n    /*\n    |--------------------------------------------------------------------------\n    | Sanctum Guards\n    |--------------------------------------------------------------------------\n    |\n    | This array contains the authentication guards that will be checked when\n    | Sanctum is trying to authenticate a request. If none of these guards\n    | are able to authenticate the request, Sanctum will use the bearer\n    | token that's present on an incoming request for authentication.\n    |\n    */\n\n    'guard' => ['web'],\n\n    /*\n    |--------------------------------------------------------------------------\n    | Expiration Minutes\n    |--------------------------------------------------------------------------\n    |\n    | This value controls the number of minutes until an issued token will be\n    | considered expired. If this value is null, personal access tokens do\n    | not expire. This won't tweak the lifetime of first-party sessions.\n    |\n    */\n\n    'expiration' => null,\n\n    /*\n    |--------------------------------------------------------------------------\n    | Sanctum Middleware\n    |--------------------------------------------------------------------------\n    |\n    | When authenticating your first-party SPA with Sanctum you may need to\n    | customize some of the middleware Sanctum uses while processing the\n    | request. You may change the middleware listed below as required.\n    |\n    */\n\n    'middleware' => [\n        'verify_csrf_token' => App\\Http\\Middleware\\VerifyCsrfToken::class,\n        'encrypt_cookies' => App\\Http\\Middleware\\EncryptCookies::class,\n    ],",
      "<?php\n\nnamespace App\\Http\\Middleware;\n\nuse Illuminate\\Foundation\\Http\\Middleware\\TrimStrings as Middleware;\n\nclass TrimStrings extends Middleware\n{\n    /**\n     * The names of the attributes that should not be trimmed.\n     *\n     * @var array<int, string>\n     */\n    protected $except = [\n        'current_password',\n        'password',\n        'password_confirmation',\n    ];\n}\n",
      "<?php\n\nnamespace Tests\\Unit;\n\nuse PHPUnit\\Framework\\TestCase;\n\nclass ExampleTest extends TestCase\n{\n    /**\n     * A basic test example.\n     */\n    public function test_that_true_is_true(): void\n    {\n        $this->assertTrue(true);\n    }\n}\n",
      "<?php\n\nnamespace App\\Providers;\n\nuse Illuminate\\Support\\ServiceProvider;\n\nclass AppServiceProvider extends ServiceProvider\n{\n    /**\n     * Register any application services.\n     */\n    public function register(): void\n    {\n        //\n    }\n\n    /**\n     * Bootstrap any application services.\n     */\n    public function boot(): void\n    {\n        //\n    }\n}\n"
    ]
  },
  {
    "id": "expressjs/express",
    "org": "expressjs",
    "avatarURL": "https://avatars.githubusercontent.com/u/5658226?v=4",
    "name": "expressjs/express",
    "url": "https://github.com/expressjs/express",
    "lang": "JavaScript",
    "desc": "Fast, unopinionated, minimalist web framework for Node.js.",
    "star_num": 61784,
    "fork_num": 10894,
    "snippets": [
      "    res.setEncoding('utf8');\n    res.on('data', function(str){ buf += str });\n    res.on('end', function(){\n      self.engine(buf, options, fn);\n    });\n  }).end();\n};\n",
      "      .get('/users')\n      .set('Accept', 'text/plain')\n      .expect(200, ' - Tobi\\n - Loki\\n - Jane\\n', done)\n    })\n\n    it('should accept to application/json', function(done){\n      request(app)\n      .get('/users')\n      .set('Accept', 'application/json')\n      .expect(200, '[{\"name\":\"Tobi\"},{\"name\":\"Loki\"},{\"name\":\"Jane\"}]', done)\n    })\n  })\n})\n",
      "app.use(vhost('example.com', main)); // Serves top level domain via Main server app\n\n/* istanbul ignore next */\nif (!module.parent) {\n  app.listen(3000);\n  console.log('Express started on port 3000');\n}\n",
      "});\n\napp.listen(3000);\nconsole.log('Express app started on port 3000');\n",
      "  saveUninitialized: false, // don't create session until something stored\n  secret: 'keyboard cat',\n  store: new RedisStore\n}));\n\napp.get('/', function(req, res){\n  var body = '';\n  if (req.session.views) {\n    ++req.session.views;\n  } else {\n    req.session.views = 1;\n    body += '<p>First time visiting? view this page in several browsers :)</p>';\n  }\n  res.send(body + '<p>viewed <strong>' + req.session.views + '</strong> times.</p>');\n});\n\napp.listen(3000);\nconsole.log('Express app started on port 3000');\n",
      "    if (route) {\n      req.route = route;\n    }\n\n    // Capture one-time layer values\n    req.params = self.mergeParams\n      ? mergeParams(layer.params, parentParams)\n      : layer.params;\n    var layerPath = layer.path;\n\n    // this should be done for the layer\n    self.process_params(layer, paramcalled, req, res, function (err) {\n      if (err) {\n        next(layerError || err)\n      } else if (route) {\n        layer.handle_request(req, res, next)\n      } else {\n        trim_prefix(layer, layerError, layerPath, path)\n      }\n\n      sync = 0\n    });\n  }\n\n  function trim_prefix(layer, layerError, layerPath, path) {\n    if (layerPath.length !== 0) {\n      // Validate path is a prefix match\n      if (layerPath !== path.slice(0, layerPath.length)) {\n        next(layerError)\n        return\n      }\n\n      // Validate path breaks on a path separator\n      var c = path[layerPath.length]\n      if (c && c !== '/' && c !== '.') return next(layerError)\n\n      // Trim off the part of the url that matches the route\n      // middleware (.use stuff) needs to have the path stripped\n      debug('trim prefix (%s) from url %s', layerPath, req.url);\n      removed = layerPath;\n      req.url = protohost + req.url.slice(protohost.length + removed.length)\n\n      // Ensure leading slash\n      if (!protohost && req.url[0] !== '/') {\n        req.url = '/' + req.url;\n        slashAdded = true;\n      }\n\n      // Setup base URL (no trailing slash)\n      req.baseUrl = parentUrl + (removed[removed.length - 1] === '/'\n        ? removed.substring(0, removed.length - 1)\n        : removed);\n    }\n\n    debug('%s %s : %s', layer.name, layerPath, req.originalUrl);\n\n    if (layerError) {\n      layer.handle_error(layerError, req, res, next);\n    } else {\n      layer.handle_request(req, res, next);\n    }\n  }\n};\n",
      "  }\n  res.send(body + '<p>viewed <strong>' + req.session.views + '</strong> times.</p>');\n});\n\napp.listen(3000);\nconsole.log('Express app started on port 3000');\n",
      "        req.on('end', function(){\n          res.end(buf);\n        });\n      });\n\n      request(app)\n      .get('/')\n      .set('Content-Type', 'application/json')\n      .send('{\"foo\":\"bar\"}')\n      .expect('Content-Type', 'application/json')\n      .expect(function () { assert.deepEqual(calls, ['one', 'two']) })\n      .expect(200, '{\"foo\":\"bar\"}', done)\n    })\n  })\n})\n",
      "    })\n\n    it('should set the Content-Type with type/subtype', function(done){\n      var app = express();\n\n      app.use(function(req, res){\n        res.type('application/vnd.amazon.ebook')\n          .end('var name = \"tj\";');\n      });\n\n      request(app)\n      .get('/')\n      .expect('Content-Type', 'application/vnd.amazon.ebook', done);\n    })\n  })\n})\n",
      "\n    it('should contain lower path', function(done){\n      var app = express()\n      var sub = express.Router()\n\n      sub.get('/:b', function(req, res){\n        res.end(req.baseUrl)\n      })\n      app.use('/:a', sub)\n\n      request(app)\n      .get('/foo/bar')\n      .expect(200, '/foo', done);\n    })\n\n    it('should contain full lower path', function(done){\n      var app = express()\n      var sub1 = express.Router()\n      var sub2 = express.Router()\n      var sub3 = express.Router()\n\n      sub3.get('/:d', function(req, res){\n        res.end(req.baseUrl)\n      })\n      sub2.use('/:c', sub3)\n      sub1.use('/:b', sub2)\n      app.use('/:a', sub1)\n\n      request(app)\n      .get('/foo/bar/baz/zed')\n      .expect(200, '/foo/bar/baz', done);\n    })\n\n    it('should travel through routers correctly', function(done){\n      var urls = []\n      var app = express()\n      var sub1 = express.Router()\n      var sub2 = express.Router()\n      var sub3 = express.Router()\n\n      sub3.get('/:d', function(req, res, next){\n        urls.push('0@' + req.baseUrl)\n        next()\n      })\n      sub2.use('/:c', sub3)\n      sub1.use('/', function(req, res, next){\n        urls.push('1@' + req.baseUrl)\n        next()\n      })\n      sub1.use('/bar', sub2)\n      sub1.use('/bar', function(req, res, next){\n        urls.push('2@' + req.baseUrl)\n        next()\n      })\n      app.use(function(req, res, next){\n        urls.push('3@' + req.baseUrl)\n        next()\n      })\n      app.use('/:a', sub1)\n      app.use(function(req, res, next){\n        urls.push('4@' + req.baseUrl)\n        res.end(urls.join(','))\n      })\n"
    ]
  },
  {
    "id": "WordPress/WordPress",
    "org": "WordPress",
    "avatarURL": "https://avatars.githubusercontent.com/u/276006?v=4",
    "name": "WordPress/WordPress",
    "url": "https://github.com/WordPress/WordPress",
    "lang": "PHP",
    "desc": "WordPress, open source software you can use to create a beautiful website, blog, or app.",
    "star_num": 17928,
    "fork_num": 12435,
    "snippets": [
      " */\n\nnamespace PHPMailer\\PHPMailer;\n\n/**\n * PHPMailer exception handler.\n *\n * @author Marcus Bointon <phpmailer@synchromedia.co.uk>\n */\nclass Exception extends \\Exception\n{\n    /**\n     * Prettify error message output.\n     *\n     * @return string\n     */\n    public function errorMessage()\n    {\n        return '<strong>' . htmlspecialchars($this->getMessage(), ENT_COMPAT | ENT_HTML401) . \"</strong><br />\\n\";\n    }\n}\n",
      "<?php\n/**\n * Text-only header with tagline and black background block pattern\n */\nreturn array(\n\t'title'      => __( 'Text-only header with tagline and background', 'twentytwentytwo' ),\n\t'categories' => array( 'header' ),\n\t'blockTypes' => array( 'core/template-part/header' ),\n\t'content'    => '<!-- wp:group {\"align\":\"full\",\"style\":{\"elements\":{\"link\":{\"color\":{\"text\":\"var:preset|color|secondary\"}}},\"spacing\":{\"padding\":{\"top\":\"var(--wp--custom--spacing--small, 1.25rem)\",\"bottom\":\"var(--wp--custom--spacing--small, 1.25rem)\"}}},\"backgroundColor\":\"foreground\",\"textColor\":\"secondary\",\"layout\":{\"inherit\":true}} -->\n\t\t\t\t\t<div class=\"wp-block-group alignfull has-secondary-color has-foreground-background-color has-text-color has-background has-link-color\" style=\"padding-top:var(--wp--custom--spacing--small, 1.25rem);padding-bottom:var(--wp--custom--spacing--small, 1.25rem)\"><!-- wp:group {\"align\":\"wide\",\"style\":{\"spacing\":{\"padding\":{\"bottom\":\"0rem\",\"top\":\"0px\",\"right\":\"0px\",\"left\":\"0px\"}}},\"layout\":{\"type\":\"flex\",\"justifyContent\":\"space-between\"}} -->\n\t\t\t\t\t<div class=\"wp-block-group alignwide\" style=\"padding-top:0px;padding-right:0px;padding-bottom:0rem;padding-left:0px\"><!-- wp:group {\"layout\":{\"type\":\"flex\",\"justifyContent\":\"left\"}} -->\n\t\t\t\t\t<div class=\"wp-block-group\"><!-- wp:site-title {\"style\":{\"typography\":{\"fontStyle\":\"normal\",\"fontWeight\":\"700\"}}} /-->\n\n\t\t\t\t\t<!-- wp:site-tagline {\"style\":{\"typography\":{\"fontStyle\":\"italic\",\"fontWeight\":\"400\"}},\"fontSize\":\"small\"} /--></div>\n\t\t\t\t\t<!-- /wp:group -->\n\n\t\t\t\t\t<!-- wp:navigation {\"layout\":{\"type\":\"flex\",\"setCascadingProperties\":true,\"justifyContent\":\"right\"}} -->\n\t\t\t\t\t<!-- wp:page-list /-->\n\t\t\t\t\t<!-- /wp:navigation --></div>\n\t\t\t\t\t<!-- /wp:group --></div>\n\t\t\t\t\t<!-- /wp:group -->',\n);\n",
      "\t * @return mixed Value if available, or null if the attribute value is invalid (and should be skipped)\n\t */\n\tprotected function normalize_attribute($name, $value) {\n\t\tswitch (strtolower($name)) {\n\t\t\tcase 'expires':\n\t\t\t\t// Expiration parsing, as per RFC 6265 section 5.2.1\n\t\t\t\tif (is_int($value)) {\n\t\t\t\t\treturn $value;\n\t\t\t\t}\n\n\t\t\t\t$expiry_time = strtotime($value);\n\t\t\t\tif ($expiry_time === false) {\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\n\t\t\t\treturn $expiry_time;\n\n\t\t\tcase 'max-age':\n\t\t\t\t// Expiration parsing, as per RFC 6265 section 5.2.2\n\t\t\t\tif (is_int($value)) {\n\t\t\t\t\treturn $value;\n\t\t\t\t}\n\n\t\t\t\t// Check that we have a valid age\n\t\t\t\tif (!preg_match('/^-?\\d+$/', $value)) {\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\n\t\t\t\t$delta_seconds = (int) $value;\n\t\t\t\tif ($delta_seconds <= 0) {\n\t\t\t\t\t$expiry_time = 0;\n\t\t\t\t} else {\n\t\t\t\t\t$expiry_time = $this->reference_time + $delta_seconds;\n\t\t\t\t}\n\n\t\t\t\treturn $expiry_time;\n\n\t\t\tcase 'domain':\n\t\t\t\t// Domains are not required as per RFC 6265 section 5.2.3\n\t\t\t\tif (empty($value)) {\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\n\t\t\t\t// Domain normalization, as per RFC 6265 section 5.2.3\n\t\t\t\tif ($value[0] === '.') {\n\t\t\t\t\t$value = substr($value, 1);\n\t\t\t\t}\n\n\t\t\t\treturn $value;\n\n\t\t\tdefault:\n\t\t\t\treturn $value;\n\t\t}\n\t}\n\n\t/**\n\t * Format a cookie for a Cookie header\n\t *\n\t * This is used when sending cookies to a server.\n\t *\n\t * @return string Cookie formatted for Cookie header\n\t */\n\tpublic function format_for_header() {\n\t\treturn sprintf('%s=%s', $this->name, $this->value);",
      "\tprotected $reason = 'Bad Request';\n}\n",
      "\t\treturn false;\n\t}\n\n\t$nav_menu .= sprintf( $args->items_wrap, esc_attr( $wrap_id ), esc_attr( $wrap_class ), $items );\n\tunset( $items );\n\n\tif ( $show_container ) {\n\t\t$nav_menu .= '</' . $args->container . '>';\n\t}\n\n\t/**\n\t * Filters the HTML content for navigation menus.\n\t *\n\t * @since 3.0.0\n\t *\n\t * @see wp_nav_menu()\n\t *\n\t * @param string   $nav_menu The HTML content for the navigation menu.\n\t * @param stdClass $args     An object containing wp_nav_menu() arguments.\n\t */\n\t$nav_menu = apply_filters( 'wp_nav_menu', $nav_menu, $args );\n\n\tif ( $args->echo ) {\n\t\techo $nav_menu;\n\t} else {\n\t\treturn $nav_menu;\n\t}\n}\n\n/**\n * Adds the class property classes for the current context, if applicable.\n *\n * @access private\n * @since 3.0.0\n *\n * @global WP_Query   $wp_query   WordPress Query object.\n * @global WP_Rewrite $wp_rewrite WordPress rewrite component.\n *\n * @param array $menu_items The current menu item objects to which to add the class property information.\n */\nfunction _wp_menu_item_classes_by_context( &$menu_items ) {\n\tglobal $wp_query, $wp_rewrite;\n\n\t$queried_object    = $wp_query->get_queried_object();\n\t$queried_object_id = (int) $wp_query->queried_object_id;\n\n\t$active_object               = '';\n\t$active_ancestor_item_ids    = array();\n\t$active_parent_item_ids      = array();\n\t$active_parent_object_ids    = array();\n\t$possible_taxonomy_ancestors = array();\n\t$possible_object_parents     = array();\n\t$home_page_id                = (int) get_option( 'page_for_posts' );\n\n\tif ( $wp_query->is_singular && ! empty( $queried_object->post_type ) && ! is_post_type_hierarchical( $queried_object->post_type ) ) {\n\t\tforeach ( (array) get_object_taxonomies( $queried_object->post_type ) as $taxonomy ) {\n\t\t\tif ( is_taxonomy_hierarchical( $taxonomy ) ) {\n\t\t\t\t$term_hierarchy = _get_term_hierarchy( $taxonomy );\n\t\t\t\t$terms          = wp_get_object_terms( $queried_object_id, $taxonomy, array( 'fields' => 'ids' ) );\n\t\t\t\tif ( is_array( $terms ) ) {\n\t\t\t\t\t$possible_object_parents = array_merge( $possible_object_parents, $terms );\n\t\t\t\t\t$term_to_ancestor        = array();\n\t\t\t\t\tforeach ( (array) $term_hierarchy as $anc => $descs ) {\n\t\t\t\t\t\tforeach ( (array) $descs as $desc ) {",
      "/**\n * Global public interface method to generate styles from a single style object,\n * e.g. the value of a block's attributes.style object or the top level styles in theme.json.\n *\n * Example usage:\n *\n *     $styles = wp_style_engine_get_styles(\n *         array(\n *             'color' => array( 'text' => '#cccccc' ),\n *         )\n *     );\n *\n * Returns:\n *\n *     array(\n *         'css'          => 'color: #cccccc',\n *         'declarations' => array( 'color' => '#cccccc' ),\n *         'classnames'   => 'has-color',\n *     )\n *\n * @since 6.1.0\n *\n * @see https://developer.wordpress.org/block-editor/reference-guides/theme-json-reference/theme-json-living/#styles\n * @see https://developer.wordpress.org/block-editor/reference-guides/block-api/block-supports/\n *\n * @param array $block_styles The style object.\n * @param array $options {\n *     Optional. An array of options. Default empty array.\n *\n *     @type string|null $context                    An identifier describing the origin of the style object,\n *                                                   e.g. 'block-supports' or 'global-styles'. Default null.\n *                                                   When set, the style engine will attempt to store the CSS rules,\n *                                                   where a selector is also passed.\n *     @type bool        $convert_vars_to_classnames Whether to skip converting incoming CSS var patterns,\n *                                                   e.g. `var:preset|<PRESET_TYPE>|<PRESET_SLUG>`,\n *                                                   to `var( --wp--preset--* )` values. Default false.\n *     @type string      $selector                   Optional. When a selector is passed,\n *                                                   the value of `$css` in the return value will comprise\n *                                                   a full CSS rule `$selector { ...$css_declarations }`,\n *                                                   otherwise, the value will be a concatenated string\n *                                                   of CSS declarations.\n * }\n * @return array {\n *     @type string   $css          A CSS ruleset or declarations block\n *                                  formatted to be placed in an HTML `style` attribute or tag.\n *     @type string[] $declarations An associative array of CSS definitions,\n *                                  e.g. `array( \"$property\" => \"$value\", \"$property\" => \"$value\" )`.\n *     @type string   $classnames   Classnames separated by a space.\n * }\n */\nfunction wp_style_engine_get_styles( $block_styles, $options = array() ) {\n\t$options = wp_parse_args(\n\t\t$options,\n\t\tarray(\n\t\t\t'selector'                   => null,\n\t\t\t'context'                    => null,\n\t\t\t'convert_vars_to_classnames' => false,\n\t\t)\n\t);\n\n\t$parsed_styles = WP_Style_Engine::parse_block_styles( $block_styles, $options );\n\n\t// Output.\n\t$styles_output = array();",
      "get_footer();\n",
      "\nuse WpOrg\\Requests\\Exception\\Http;\n\n/**\n * Exception for 428 Precondition Required responses\n *\n * @link https://tools.ietf.org/html/rfc6585\n *\n * @package Requests\\Exceptions\n */\nfinal class Status428 extends Http {\n\t/**\n\t * HTTP status code\n\t *\n\t * @var integer\n\t */\n\tprotected $code = 428;\n\n\t/**\n\t * Reason phrase\n\t *\n\t * @var string\n\t */\n\tprotected $reason = 'Precondition Required';\n}\n",
      "\t\t\t\t\t/* translators: %s: Capability name. */\n\t\t\t\t\t$message = __( 'When checking for the %s capability, you must always check it against a specific page.' );\n\t\t\t\t}\n\n\t\t\t\t_doing_it_wrong(\n\t\t\t\t\t__FUNCTION__,\n\t\t\t\t\tsprintf( $message, '<code>' . $cap . '</code>' ),\n\t\t\t\t\t'6.1.0'\n\t\t\t\t);\n\n\t\t\t\t$caps[] = 'do_not_allow';\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t$post = get_post( $args[0] );\n\t\t\tif ( ! $post ) {\n\t\t\t\t$caps[] = 'do_not_allow';\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif ( 'revision' === $post->post_type ) {\n\t\t\t\t$post = get_post( $post->post_parent );\n\t\t\t\tif ( ! $post ) {\n\t\t\t\t\t$caps[] = 'do_not_allow';\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t$post_type = get_post_type_object( $post->post_type );\n\t\t\tif ( ! $post_type ) {\n\t\t\t\t/* translators: 1: Post type, 2: Capability name. */\n\t\t\t\t$message = __( 'The post type %1$s is not registered, so it may not be reliable to check the capability %2$s against a post of that type.' );\n\n\t\t\t\t_doing_it_wrong(\n\t\t\t\t\t__FUNCTION__,\n\t\t\t\t\tsprintf(\n\t\t\t\t\t\t$message,\n\t\t\t\t\t\t'<code>' . $post->post_type . '</code>',\n\t\t\t\t\t\t'<code>' . $cap . '</code>'\n\t\t\t\t\t),\n\t\t\t\t\t'4.4.0'\n\t\t\t\t);\n\n\t\t\t\t$caps[] = 'edit_others_posts';\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif ( ! $post_type->map_meta_cap ) {\n\t\t\t\t$caps[] = $post_type->cap->$cap;\n\t\t\t\t// Prior to 3.1 we would re-call map_meta_cap here.\n\t\t\t\tif ( 'edit_post' === $cap ) {\n\t\t\t\t\t$cap = $post_type->cap->$cap;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t// If the post author is set and the user is the author...\n\t\t\tif ( $post->post_author && $user_id == $post->post_author ) {\n\t\t\t\t// If the post is published or scheduled...\n\t\t\t\tif ( in_array( $post->post_status, array( 'publish', 'future' ), true ) ) {\n\t\t\t\t\t$caps[] = $post_type->cap->edit_published_posts;\n\t\t\t\t} elseif ( 'trash' === $post->post_status ) {\n\t\t\t\t\t$status = get_post_meta( $post->ID, '_wp_trash_meta_status', true );\n\t\t\t\t\tif ( in_array( $status, array( 'publish', 'future' ), true ) ) {",
      "\tprotected $code = 429;\n\n\t/**\n\t * Reason phrase\n\t *\n\t * @var string\n\t */\n\tprotected $reason = 'Too Many Requests';\n}\n"
    ]
  },
  {
    "id": "rust-lang/rust",
    "org": "rust-lang",
    "avatarURL": "https://avatars.githubusercontent.com/u/5430905?v=4",
    "name": "rust-lang/rust",
    "url": "https://github.com/rust-lang/rust",
    "lang": "Rust",
    "desc": "A language empowering everyone to build reliable and efficient software.",
    "star_num": 85032,
    "fork_num": 11265,
    "snippets": [
      "//! OS-specific networking functionality.\n\n// See cfg macros in `library/std/src/os/mod.rs` for why these platforms must\n// be special-cased during rustdoc generation.\n#[cfg(not(all(\n    doc,\n    any(\n        all(target_arch = \"wasm32\", not(target_os = \"wasi\")),\n        all(target_vendor = \"fortanix\", target_env = \"sgx\")\n    )\n)))]\n#[cfg(any(target_os = \"linux\", target_os = \"android\", doc))]\npub(super) mod linux_ext;\n",
      "        // test the case where there are other statements in the following unsafe block\n        vec.set_len(200);\n        assert!(vec.len() == 200);\n    }\n\n    // handle vec stored in the field of a struct\n    let mut my_vec = MyVec::default();\n    my_vec.vec.reserve(1000);\n    //~^ ERROR: calling `set_len()` immediately after reserving a buffer creates uninitial\n    unsafe {\n        my_vec.vec.set_len(200);\n    }\n\n    my_vec.vec = Vec::with_capacity(1000);\n    //~^ ERROR: calling `set_len()` immediately after reserving a buffer creates uninitial\n    unsafe {\n        my_vec.vec.set_len(200);\n    }\n\n    // Test `#[allow(...)]` attributes on inner unsafe block (shouldn't trigger)\n    let mut vec: Vec<u8> = Vec::with_capacity(1000);\n    #[allow(clippy::uninit_vec)]\n    unsafe {\n        vec.set_len(200);\n    }\n\n    // MaybeUninit-wrapped types should not be detected\n    unsafe {\n        let mut vec: Vec<MaybeUninit<u8>> = Vec::with_capacity(1000);\n        vec.set_len(200);\n\n        let mut vec: Vec<(MaybeUninit<u8>, MaybeUninit<bool>)> = Vec::with_capacity(1000);\n        vec.set_len(200);\n\n        let mut vec: Vec<(MaybeUninit<u8>, [MaybeUninit<bool>; 2])> = Vec::with_capacity(1000);\n        vec.set_len(200);\n    }\n\n    // known false negative\n    let mut vec1: Vec<u8> = Vec::with_capacity(1000);\n    let mut vec2: Vec<u8> = Vec::with_capacity(1000);\n    unsafe {\n        vec1.set_len(200);\n        vec2.set_len(200);\n    }\n\n    // set_len(0) should not be detected\n    let mut vec: Vec<u8> = Vec::with_capacity(1000);\n    unsafe {\n        vec.set_len(0);\n    }\n\n    // ZSTs should not be detected\n    let mut vec: Vec<()> = Vec::with_capacity(1000);\n    unsafe {\n        vec.set_len(10);\n    }\n\n    // unions should not be detected\n    let mut vec: Vec<MyOwnMaybeUninit> = Vec::with_capacity(1000);\n    unsafe {\n        vec.set_len(10);\n    }\n",
      "// check-pass\n// issue: 114113\n// revisions: current next\n//[next] compile-flags: -Ztrait-solver=next\n\n#![feature(trait_upcasting)]\n\ntrait Mirror {\n    type Assoc;\n}\nimpl<T> Mirror for T {\n    type Assoc = T;\n}\n\ntrait Bar<T> {}\ntrait Foo<T>: Bar<<T as Mirror>::Assoc> {}\n\nfn upcast<T>(x: &dyn Foo<T>) -> &dyn Bar<T> { x }\n\nfn main() {}\n",
      "}\n",
      "// EMIT_MIR_FOR_EACH_PANIC_STRATEGY\n// This is a copy of the `dead_stores_79191` test, except that we turn on DSE. This demonstrates\n// that that pass enables this one to do more optimizations.\n\n// unit-test: CopyProp\n// compile-flags: -Zmir-enable-passes=+DeadStoreElimination\n\nfn id<T>(x: T) -> T {\n    x\n}\n\n// EMIT_MIR dead_stores_better.f.CopyProp.after.mir\npub fn f(mut a: usize) -> usize {\n    let b = a;\n    a = 5;\n    a = b;\n    id(a)\n}\n\nfn main() {\n    f(0);\n}\n",
      "\nuse std::mem::transmute;\n\nstruct Foo<T: ?Sized> {\n    t: Box<T>\n}\n\nimpl<T: ?Sized> Foo<T> {\n    fn m(x: &T) -> &isize where T : Sized {\n        // OK here, because T : Sized is in scope.\n        unsafe { transmute(x) }\n    }\n\n    fn n(x: &T) -> &isize {\n        // Not OK here, because T : Sized is not in scope.\n        unsafe { transmute(x) } //~ ERROR cannot transmute between types of different sizes\n    }\n}\n\nfn main() { }\n",
      "    pub fn new(conf_disallowed: Vec<conf::DisallowedPath>) -> Self {\n        Self {\n            conf_disallowed,\n            disallowed: DefIdMap::default(),\n            seen: FxHashSet::default(),\n        }\n    }\n\n    fn check(&mut self, cx: &LateContext<'_>, span: Span) {\n        if self.conf_disallowed.is_empty() {\n            return;\n        }\n\n        for mac in macro_backtrace(span) {\n            if !self.seen.insert(mac.expn) {\n                return;\n            }\n\n            if let Some(&index) = self.disallowed.get(&mac.def_id) {\n                let conf = &self.conf_disallowed[index];\n\n                span_lint_and_then(\n                    cx,\n                    DISALLOWED_MACROS,\n                    mac.span,\n                    &format!(\"use of a disallowed macro `{}`\", conf.path()),\n                    |diag| {\n                        if let Some(reason) = conf.reason() {\n                            diag.note(reason);\n                        }\n                    },\n                );\n            }\n        }\n    }\n}\n\nimpl_lint_pass!(DisallowedMacros => [DISALLOWED_MACROS]);\n\nimpl LateLintPass<'_> for DisallowedMacros {\n    fn check_crate(&mut self, cx: &LateContext<'_>) {\n        for (index, conf) in self.conf_disallowed.iter().enumerate() {\n            let segs: Vec<_> = conf.path().split(\"::\").collect();\n            for id in clippy_utils::def_path_def_ids(cx, &segs) {\n                self.disallowed.insert(id, index);\n            }\n        }\n    }\n\n    fn check_expr(&mut self, cx: &LateContext<'_>, expr: &Expr<'_>) {\n        self.check(cx, expr.span);\n    }\n\n    fn check_stmt(&mut self, cx: &LateContext<'_>, stmt: &Stmt<'_>) {\n        self.check(cx, stmt.span);\n    }\n\n    fn check_ty(&mut self, cx: &LateContext<'_>, ty: &Ty<'_>) {\n        self.check(cx, ty.span);\n    }\n\n    fn check_pat(&mut self, cx: &LateContext<'_>, pat: &Pat<'_>) {\n        self.check(cx, pat.span);\n    }",
      "struct Parser {\n    tokens: Vec<isize> ,\n}\n\ntrait Parse {\n    fn parse(&self) -> Vec<isize> ;\n}\n\nimpl Parse for Parser {\n    fn parse(&self) -> Vec<isize> {\n        self.tokens //~ ERROR cannot move out\n    }\n}\n\nfn main() {}\n",
      "// created via FRU and control-flow breaks in the middle of\n// construction.\n\nuse std::sync::atomic::{Ordering, AtomicUsize};\n\n#[derive(Debug)]\nstruct Noisy(u8);\nimpl Drop for Noisy {\n    fn drop(&mut self) {\n        // println!(\"splat #{}\", self.0);\n        event(self.0);\n    }\n}\n\n#[allow(dead_code)]\n#[derive(Debug)]\nstruct Foo { n0: Noisy, n1: Noisy }\nimpl Foo {\n    fn vals(&self) -> (u8, u8) { (self.n0.0, self.n1.0) }\n}\n\nfn leak_1_ret() -> Foo {\n    let _old_foo = Foo { n0: Noisy(1), n1: Noisy(2) };\n    Foo { n0: { return Foo { n0: Noisy(3), n1: Noisy(4) } },\n          .._old_foo\n    };\n}\n\nfn leak_2_ret() -> Foo {\n    let _old_foo = Foo { n0: Noisy(1), n1: Noisy(2) };\n    Foo { n1: { return Foo { n0: Noisy(3), n1: Noisy(4) } },\n          .._old_foo\n    };\n}\n\n// In this case, the control flow break happens *before* we construct\n// `Foo(Noisy(1),Noisy(2))`, so there should be no record of it in the\n// event log.\nfn leak_3_ret() -> Foo {\n    let _old_foo = || Foo { n0: Noisy(1), n1: Noisy(2) };\n    Foo { n1: { return Foo { n0: Noisy(3), n1: Noisy(4) } },\n          .._old_foo()\n    };\n}\n\npub fn main() {\n    reset_log();\n    assert_eq!(leak_1_ret().vals(), (3,4));\n    assert_eq!(0x01_02_03_04, event_log());\n\n    reset_log();\n    assert_eq!(leak_2_ret().vals(), (3,4));\n    assert_eq!(0x01_02_03_04, event_log());\n\n    reset_log();\n    assert_eq!(leak_3_ret().vals(), (3,4));\n    assert_eq!(0x03_04, event_log());\n}\n\nstatic LOG: AtomicUsize = AtomicUsize::new(0);\n\nfn reset_log() {\n    LOG.store(0, Ordering::SeqCst);\n}",
      "      //^ error: cannot move `X` out of reference\n}\n\"#,\n        );\n    }\n\n    #[test]\n    fn move_out_of_field() {\n        check_diagnostics(\n            r#\"\n//- minicore: copy\nstruct X;\nstruct Y(X, i32);\nfn main() {\n    let a = &Y(X, 5);\n    let b = a.0;\n      //^ error: cannot move `X` out of reference\n    let y = a.1;\n}\n\"#,\n        );\n    }\n\n    #[test]\n    fn move_out_of_static() {\n        check_diagnostics(\n            r#\"\n//- minicore: copy\nstruct X;\nfn main() {\n    static S: X = X;\n    let s = S;\n      //^ error: cannot move `X` out of reference\n}\n\"#,\n        );\n    }\n\n    #[test]\n    fn generic_types() {\n        check_diagnostics(\n            r#\"\n//- minicore: derive, copy\n\n#[derive(Copy)]\nstruct X<T>(T);\nstruct Y;\n\nfn consume<T>(_: X<T>) {\n\n}\n\nfn main() {\n    let a = &X(Y);\n    consume(*a);\n  //^^^^^^^^^^^ error: cannot move `X<Y>` out of reference\n    let a = &X(5);\n    consume(*a);\n}\n\"#,\n        );\n    }\n\n    #[test]"
    ]
  },
  {
    "id": "git/git",
    "org": "git",
    "avatarURL": "https://avatars.githubusercontent.com/u/18133?v=4",
    "name": "git/git",
    "url": "https://github.com/git/git",
    "lang": "C",
    "desc": "Git is a free and open source distributed version control system.",
    "star_num": 47249,
    "fork_num": 25392,
    "snippets": [
      "\t\t\tXML_ParserFree(parser);\n\t\t}\n\t} else {\n\t\tfprintf(stderr, \"Unable to start PROPFIND request\\n\");\n\t}\n\n\tfree(ls.path);\n\tfree(url);\n\tstrbuf_release(&out_buffer.buf);\n\tstrbuf_release(&in_buffer);\n\tcurl_slist_free_all(dav_headers);\n}\n\nstatic void get_remote_object_list(unsigned char parent)\n{\n\tchar path[] = \"objects/XX/\";\n\tstatic const char hex[] = \"0123456789abcdef\";\n\tunsigned int val = parent;\n\n\tpath[8] = hex[val >> 4];\n\tpath[9] = hex[val & 0xf];\n\tremote_dir_exists[val] = 0;\n\tremote_ls(path, (PROCESS_FILES | PROCESS_DIRS),\n\t\t  process_ls_object, &val);\n}\n\nstatic int locking_available(void)\n{\n\tstruct active_request_slot *slot;\n\tstruct slot_results results;\n\tstruct strbuf in_buffer = STRBUF_INIT;\n\tstruct buffer out_buffer = { STRBUF_INIT, 0 };\n\tstruct curl_slist *dav_headers = http_copy_default_headers();\n\tstruct xml_ctx ctx;\n\tint lock_flags = 0;\n\tchar *escaped;\n\n\tescaped = xml_entities(repo->url);\n\tstrbuf_addf(&out_buffer.buf, PROPFIND_SUPPORTEDLOCK_REQUEST, escaped);\n\tfree(escaped);\n\n\tdav_headers = curl_slist_append(dav_headers, \"Depth: 0\");\n\tdav_headers = curl_slist_append(dav_headers, \"Content-Type: text/xml\");\n\n\tslot = get_active_slot();\n\tslot->results = &results;\n\tcurl_setup_http(slot->curl, repo->url, DAV_PROPFIND,\n\t\t\t&out_buffer, fwrite_buffer);\n\tcurl_easy_setopt(slot->curl, CURLOPT_HTTPHEADER, dav_headers);\n\tcurl_easy_setopt(slot->curl, CURLOPT_WRITEDATA, &in_buffer);\n\n\tif (start_active_slot(slot)) {\n\t\trun_active_slot(slot);\n\t\tif (results.curl_result == CURLE_OK) {\n\t\t\tXML_Parser parser = XML_ParserCreate(NULL);\n\t\t\tenum XML_Status result;\n\t\t\tctx.name = xcalloc(10, 1);\n\t\t\tctx.len = 0;\n\t\t\tctx.cdata = NULL;\n\t\t\tctx.userFunc = handle_lockprop_ctx;\n\t\t\tctx.userData = &lock_flags;\n\t\t\tXML_SetUserData(parser, &ctx);\n\t\t\tXML_SetElementHandler(parser, xml_start_tag,\n\t\t\t\t\t      xml_end_tag);",
      "\t\tstrvec_pushf(&pack_objects.args, \"--filter=%s\", spec);\n\t}\n\tif (uri_protocols) {\n\t\tfor (i = 0; i < uri_protocols->nr; i++)\n\t\t\tstrvec_pushf(&pack_objects.args, \"--uri-protocol=%s\",\n\t\t\t\t\t uri_protocols->items[i].string);\n\t}\n\n\tpack_objects.in = -1;\n\tpack_objects.out = -1;\n\tpack_objects.err = -1;\n\tpack_objects.clean_on_exit = 1;\n\n\tif (start_command(&pack_objects))\n\t\tdie(\"git upload-pack: unable to fork git-pack-objects\");\n\n\tpipe_fd = xfdopen(pack_objects.in, \"w\");\n\n\tif (pack_data->shallow_nr)\n\t\tfor_each_commit_graft(write_one_shallow, pipe_fd);\n\n\tfor (i = 0; i < pack_data->want_obj.nr; i++)\n\t\tfprintf(pipe_fd, \"%s\\n\",\n\t\t\toid_to_hex(&pack_data->want_obj.objects[i].item->oid));\n\tfprintf(pipe_fd, \"--not\\n\");\n\tfor (i = 0; i < pack_data->have_obj.nr; i++)\n\t\tfprintf(pipe_fd, \"%s\\n\",\n\t\t\toid_to_hex(&pack_data->have_obj.objects[i].item->oid));\n\tfor (i = 0; i < pack_data->extra_edge_obj.nr; i++)\n\t\tfprintf(pipe_fd, \"%s\\n\",\n\t\t\toid_to_hex(&pack_data->extra_edge_obj.objects[i].item->oid));\n\tfprintf(pipe_fd, \"\\n\");\n\tfflush(pipe_fd);\n\tfclose(pipe_fd);\n\n\t/* We read from pack_objects.err to capture stderr output for\n\t * progress bar, and pack_objects.out to capture the pack data.\n\t */\n\n\twhile (1) {\n\t\tstruct pollfd pfd[2];\n\t\tint pe, pu, pollsize, polltimeout;\n\t\tint ret;\n\n\t\treset_timeout(pack_data->timeout);\n\n\t\tpollsize = 0;\n\t\tpe = pu = -1;\n\n\t\tif (0 <= pack_objects.out) {\n\t\t\tpfd[pollsize].fd = pack_objects.out;\n\t\t\tpfd[pollsize].events = POLLIN;\n\t\t\tpu = pollsize;\n\t\t\tpollsize++;\n\t\t}\n\t\tif (0 <= pack_objects.err) {\n\t\t\tpfd[pollsize].fd = pack_objects.err;\n\t\t\tpfd[pollsize].events = POLLIN;\n\t\t\tpe = pollsize;\n\t\t\tpollsize++;\n\t\t}\n\n\t\tif (!pollsize)\n\t\t\tbreak;",
      "\t\tOPT_BOOL(0, \"progress\", &opts.progress,\n\t\t\t N_(\"force progress reporting\")),\n\t\tOPT_END(),\n\t};\n\tstruct option *options = add_common_options(builtin_commit_graph_verify_options);\n\n\ttrace2_cmd_mode(\"verify\");\n\n\topts.progress = isatty(2);\n\targc = parse_options(argc, argv, prefix,\n\t\t\t     options,\n\t\t\t     builtin_commit_graph_verify_usage, 0);\n\tif (argc)\n\t\tusage_with_options(builtin_commit_graph_verify_usage, options);\n\n\tif (!opts.obj_dir)\n\t\topts.obj_dir = get_object_directory();\n\tif (opts.shallow)\n\t\tflags |= COMMIT_GRAPH_VERIFY_SHALLOW;\n\tif (opts.progress)\n\t\tflags |= COMMIT_GRAPH_WRITE_PROGRESS;\n\n\todb = find_odb(the_repository, opts.obj_dir);\n\tgraph_name = get_commit_graph_filename(odb);\n\topen_ok = open_commit_graph(graph_name, &fd, &st);\n\tif (!open_ok && errno != ENOENT)\n\t\tdie_errno(_(\"Could not open commit-graph '%s'\"), graph_name);\n\n\tFREE_AND_NULL(graph_name);\n\tFREE_AND_NULL(options);\n\n\tif (open_ok)\n\t\tgraph = load_commit_graph_one_fd_st(the_repository, fd, &st, odb);\n\telse\n\t\tgraph = read_commit_graph_one(the_repository, odb);\n\n\t/* Return failure if open_ok predicted success */\n\tif (!graph)\n\t\treturn !!open_ok;\n\n\tret = verify_commit_graph(the_repository, graph, flags);\n\tfree_commit_graph(graph);\n\treturn ret;\n}\n\nextern int read_replace_refs;\nstatic struct commit_graph_opts write_opts;\n\nstatic int write_option_parse_split(const struct option *opt, const char *arg,\n\t\t\t\t    int unset)\n{\n\tenum commit_graph_split_flags *flags = opt->value;\n\n\tBUG_ON_OPT_NEG(unset);\n\n\topts.split = 1;\n\tif (!arg)\n\t\treturn 0;\n\n\tif (!strcmp(arg, \"no-merge\"))\n\t\t*flags = COMMIT_GRAPH_SPLIT_MERGE_PROHIBITED;\n\telse if (!strcmp(arg, \"replace\"))\n\t\t*flags = COMMIT_GRAPH_SPLIT_REPLACE;\n\telse",
      "\t\t\t\tX_array[X_nr++] = c;\n\t\t\t\tadd_object_array(orig, NULL, &X_obj);\n\t\t\t\tbreak;\n\n\t\t\tcase 'Y':\n\t\t\t\tcommit_list_insert(c, &Y);\n\t\t\t\tALLOC_GROW(Y_array, Y_nr + 1, Y_alloc);\n\t\t\t\tY_array[Y_nr++] = c;\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tdie(\"unexpected start of line: %c\", buf.buf[0]);\n\t\t}\n\t}\n\tstrbuf_release(&buf);\n\n\tif (!strcmp(av[1], \"ref_newer\"))\n\t\tprintf(\"%s(A,B):%d\\n\", av[1], ref_newer(&oid_A, &oid_B));\n\telse if (!strcmp(av[1], \"in_merge_bases\"))\n\t\tprintf(\"%s(A,B):%d\\n\", av[1],\n\t\t       repo_in_merge_bases(the_repository, A, B));\n\telse if (!strcmp(av[1], \"in_merge_bases_many\"))\n\t\tprintf(\"%s(A,X):%d\\n\", av[1],\n\t\t       repo_in_merge_bases_many(the_repository, A, X_nr, X_array));\n\telse if (!strcmp(av[1], \"is_descendant_of\"))\n\t\tprintf(\"%s(A,X):%d\\n\", av[1], repo_is_descendant_of(r, A, X));\n\telse if (!strcmp(av[1], \"get_merge_bases_many\")) {\n\t\tstruct commit_list *list = repo_get_merge_bases_many(the_repository,\n\t\t\t\t\t\t\t\t     A, X_nr,\n\t\t\t\t\t\t\t\t     X_array);\n\t\tprintf(\"%s(A,X):\\n\", av[1]);\n\t\tprint_sorted_commit_ids(list);\n\t} else if (!strcmp(av[1], \"reduce_heads\")) {\n\t\tstruct commit_list *list = reduce_heads(X);\n\t\tprintf(\"%s(X):\\n\", av[1]);\n\t\tprint_sorted_commit_ids(list);\n\t} else if (!strcmp(av[1], \"can_all_from_reach\")) {\n\t\tprintf(\"%s(X,Y):%d\\n\", av[1], can_all_from_reach(X, Y, 1));\n\t} else if (!strcmp(av[1], \"can_all_from_reach_with_flag\")) {\n\t\tstruct commit_list *iter = Y;\n\n\t\twhile (iter) {\n\t\t\titer->item->object.flags |= 2;\n\t\t\titer = iter->next;\n\t\t}\n\n\t\tprintf(\"%s(X,_,_,0,0):%d\\n\", av[1], can_all_from_reach_with_flag(&X_obj, 2, 4, 0, 0));\n\t} else if (!strcmp(av[1], \"commit_contains\")) {\n\t\tstruct ref_filter filter = REF_FILTER_INIT;\n\t\tstruct contains_cache cache;\n\t\tinit_contains_cache(&cache);\n\n\t\tif (ac > 2 && !strcmp(av[2], \"--tag\"))\n\t\t\tfilter.with_commit_tag_algo = 1;\n\t\telse\n\t\t\tfilter.with_commit_tag_algo = 0;\n\n\t\tprintf(\"%s(_,A,X,_):%d\\n\", av[1], commit_contains(&filter, A, X, &cache));\n\t} else if (!strcmp(av[1], \"get_reachable_subset\")) {\n\t\tconst int reachable_flag = 1;\n\t\tint i, count = 0;\n\t\tstruct commit_list *current;\n\t\tstruct commit_list *list = get_reachable_subset(X_array, X_nr,\n\t\t\t\t\t\t\t\tY_array, Y_nr,",
      "\tfor_each_string_list_item (item, oid_str_list) {\n\t\tconst char *oid_str = item->string;\n\t\tstruct object_id oid;\n\t\tunsigned long object_size;\n\n\t\tif (get_oid_hex(oid_str, &oid) < 0) {\n\t\t\tpacket_writer_error(\n\t\t\t\twriter,\n\t\t\t\t\"object-info: protocol error, expected to get oid, not '%s'\",\n\t\t\t\toid_str);\n\t\t\tcontinue;\n\t\t}\n\n\t\tstrbuf_addstr(&send_buffer, oid_str);\n\n\t\tif (info->size) {\n\t\t\tif (oid_object_info(r, &oid, &object_size) < 0) {\n\t\t\t\tstrbuf_addstr(&send_buffer, \" \");\n\t\t\t} else {\n\t\t\t\tstrbuf_addf(&send_buffer, \" %lu\", object_size);\n\t\t\t}\n\t\t}\n\n\t\tpacket_writer_write(writer, \"%s\", send_buffer.buf);\n\t\tstrbuf_reset(&send_buffer);\n\t}\n\tstrbuf_release(&send_buffer);\n}\n\nint cap_object_info(struct repository *r, struct packet_reader *request)\n{\n\tstruct requested_info info = { 0 };\n\tstruct packet_writer writer;\n\tstruct string_list oid_str_list = STRING_LIST_INIT_DUP;\n\n\tpacket_writer_init(&writer, 1);\n\n\twhile (packet_reader_read(request) == PACKET_READ_NORMAL) {\n\t\tif (!strcmp(\"size\", request->line)) {\n\t\t\tinfo.size = 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (parse_oid(request->line, &oid_str_list))\n\t\t\tcontinue;\n\n\t\tpacket_writer_error(&writer,\n\t\t\t\t    \"object-info: unexpected line: '%s'\",\n\t\t\t\t    request->line);\n\t}\n\n\tif (request->status != PACKET_READ_FLUSH) {\n\t\tpacket_writer_error(\n\t\t\t&writer, \"object-info: expected flush after arguments\");\n\t\tdie(_(\"object-info: expected flush after arguments\"));\n\t}\n\n\tsend_info(r, &writer, &oid_str_list, &info);\n\n\tstring_list_clear(&oid_str_list, 1);\n\n\tpacket_flush(1);\n\n\treturn 0;",
      "#include \"oidset.h\"\n#include \"hex.h\"\n#include \"strbuf.h\"\n\nvoid oidset_init(struct oidset *set, size_t initial_size)\n{\n\tmemset(&set->set, 0, sizeof(set->set));\n\tif (initial_size)\n\t\tkh_resize_oid_set(&set->set, initial_size);\n}\n\nint oidset_contains(const struct oidset *set, const struct object_id *oid)\n{\n\tkhiter_t pos = kh_get_oid_set(&set->set, *oid);\n\treturn pos != kh_end(&set->set);\n}\n\nint oidset_insert(struct oidset *set, const struct object_id *oid)\n{\n\tint added;\n\tkh_put_oid_set(&set->set, *oid, &added);\n\treturn !added;\n}\n\nint oidset_remove(struct oidset *set, const struct object_id *oid)\n{\n\tkhiter_t pos = kh_get_oid_set(&set->set, *oid);\n\tif (pos == kh_end(&set->set))\n\t\treturn 0;\n\tkh_del_oid_set(&set->set, pos);\n\treturn 1;\n}\n\nvoid oidset_clear(struct oidset *set)\n{\n\tkh_release_oid_set(&set->set);\n\toidset_init(set, 0);\n}\n\nvoid oidset_parse_file(struct oidset *set, const char *path)\n{\n\toidset_parse_file_carefully(set, path, NULL, NULL);\n}\n\nvoid oidset_parse_file_carefully(struct oidset *set, const char *path,\n\t\t\t\t oidset_parse_tweak_fn fn, void *cbdata)\n{\n\tFILE *fp;\n\tstruct strbuf sb = STRBUF_INIT;\n\tstruct object_id oid;\n\n\tfp = fopen(path, \"r\");\n\tif (!fp)\n\t\tdie(\"could not open object name list: %s\", path);\n\twhile (!strbuf_getline(&sb, fp)) {\n\t\tconst char *p;\n\t\tconst char *name;\n\n\t\t/*\n\t\t * Allow trailing comments, leading whitespace\n\t\t * (including before commits), and empty or whitespace\n\t\t * only lines.\n\t\t */\n\t\tname = strchr(sb.buf, '#');",
      "\t\t}\n\t\telse {\n\t\t\t/* we can borrow from the file in the work tree */\n\t\t\ttemp->name = one->path;\n\t\t\tif (!one->oid_valid)\n\t\t\t\toid_to_hex_r(temp->hex, null_oid());\n\t\t\telse\n\t\t\t\toid_to_hex_r(temp->hex, &one->oid);\n\t\t\t/* Even though we may sometimes borrow the\n\t\t\t * contents from the work tree, we always want\n\t\t\t * one->mode.  mode is trustworthy even when\n\t\t\t * !(one->oid_valid), as long as\n\t\t\t * DIFF_FILE_VALID(one).\n\t\t\t */\n\t\t\txsnprintf(temp->mode, sizeof(temp->mode), \"%06o\", one->mode);\n\t\t}\n\t\treturn temp;\n\t}\n\telse {\n\t\tif (diff_populate_filespec(r, one, NULL))\n\t\t\tdie(\"cannot read data blob for %s\", one->path);\n\t\tprep_temp_blob(r->index, one->path, temp,\n\t\t\t       one->data, one->size,\n\t\t\t       &one->oid, one->mode);\n\t}\n\treturn temp;\n}\n\nstatic void add_external_diff_name(struct repository *r,\n\t\t\t\t   struct strvec *argv,\n\t\t\t\t   struct diff_filespec *df)\n{\n\tstruct diff_tempfile *temp = prepare_temp_file(r, df);\n\tstrvec_push(argv, temp->name);\n\tstrvec_push(argv, temp->hex);\n\tstrvec_push(argv, temp->mode);\n}\n\n/* An external diff command takes:\n *\n * diff-cmd name infile1 infile1-sha1 infile1-mode \\\n *               infile2 infile2-sha1 infile2-mode [ rename-to ]\n *\n */\nstatic void run_external_diff(const char *pgm,\n\t\t\t      const char *name,\n\t\t\t      const char *other,\n\t\t\t      struct diff_filespec *one,\n\t\t\t      struct diff_filespec *two,\n\t\t\t      const char *xfrm_msg,\n\t\t\t      struct diff_options *o)\n{\n\tstruct child_process cmd = CHILD_PROCESS_INIT;\n\tstruct diff_queue_struct *q = &diff_queued_diff;\n\n\tstrvec_push(&cmd.args, pgm);\n\tstrvec_push(&cmd.args, name);\n\n\tif (one && two) {\n\t\tadd_external_diff_name(o->repo, &cmd.args, one);\n\t\tadd_external_diff_name(o->repo, &cmd.args, two);\n\t\tif (other) {\n\t\t\tstrvec_push(&cmd.args, other);\n\t\t\tstrvec_push(&cmd.args, xfrm_msg);",
      "\t/*\n\t * Now create the packed index in array form\n\t * rather than linked lists.\n\t */\n\tmemsize = sizeof(*index)\n\t\t+ sizeof(*packed_hash) * (hsize+1)\n\t\t+ sizeof(*packed_entry) * entries;\n\tmem = malloc(memsize);\n\tif (!mem) {\n\t\tfree(hash);\n\t\treturn NULL;\n\t}\n\n\tindex = mem;\n\tindex->memsize = memsize;\n\tindex->src_buf = buf;\n\tindex->src_size = bufsize;\n\tindex->hash_mask = hmask;\n\n\tmem = index->hash;\n\tpacked_hash = mem;\n\tmem = packed_hash + (hsize+1);\n\tpacked_entry = mem;\n\n\tfor (i = 0; i < hsize; i++) {\n\t\t/*\n\t\t * Coalesce all entries belonging to one linked list\n\t\t * into consecutive array entries.\n\t\t */\n\t\tpacked_hash[i] = packed_entry;\n\t\tfor (entry = hash[i]; entry; entry = entry->next)\n\t\t\t*packed_entry++ = entry->entry;\n\t}\n\n\t/* Sentinel value to indicate the length of the last hash bucket */\n\tpacked_hash[hsize] = packed_entry;\n\n\tassert(packed_entry - (struct index_entry *)mem == entries);\n\tfree(hash);\n\n\treturn index;\n}\n\nvoid free_delta_index(struct delta_index *index)\n{\n\tfree(index);\n}\n\nunsigned long sizeof_delta_index(struct delta_index *index)\n{\n\tif (index)\n\t\treturn index->memsize;\n\telse\n\t\treturn 0;\n}\n\n/*\n * The maximum size for any opcode sequence, including the initial header\n * plus Rabin window plus biggest copy.\n */\n#define MAX_OP_SIZE\t(5 + 5 + 1 + RABIN_WINDOW + 7)\n\nvoid *\ncreate_delta(const struct delta_index *index,",
      "\nstatic void output_pattern(const char *path, struct path_pattern *pattern)\n{\n\tchar *bang  = (pattern && pattern->flags & PATTERN_FLAG_NEGATIVE)  ? \"!\" : \"\";\n\tchar *slash = (pattern && pattern->flags & PATTERN_FLAG_MUSTBEDIR) ? \"/\" : \"\";\n\tif (!nul_term_line) {\n\t\tif (!verbose) {\n\t\t\twrite_name_quoted(path, stdout, '\\n');\n\t\t} else {\n\t\t\tif (pattern) {\n\t\t\t\tquote_c_style(pattern->pl->src, NULL, stdout, 0);\n\t\t\t\tprintf(\":%d:%s%s%s\\t\",\n\t\t\t\t       pattern->srcpos,\n\t\t\t\t       bang, pattern->pattern, slash);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tprintf(\"::\\t\");\n\t\t\t}\n\t\t\tquote_c_style(path, NULL, stdout, 0);\n\t\t\tfputc('\\n', stdout);\n\t\t}\n\t} else {\n\t\tif (!verbose) {\n\t\t\tprintf(\"%s%c\", path, '\\0');\n\t\t} else {\n\t\t\tif (pattern)\n\t\t\t\tprintf(\"%s%c%d%c%s%s%s%c%s%c\",\n\t\t\t\t       pattern->pl->src, '\\0',\n\t\t\t\t       pattern->srcpos, '\\0',\n\t\t\t\t       bang, pattern->pattern, slash, '\\0',\n\t\t\t\t       path, '\\0');\n\t\t\telse\n\t\t\t\tprintf(\"%c%c%c%s%c\", '\\0', '\\0', '\\0', path, '\\0');\n\t\t}\n\t}\n}\n\nstatic int check_ignore(struct dir_struct *dir,\n\t\t\tconst char *prefix, int argc, const char **argv)\n{\n\tconst char *full_path;\n\tchar *seen;\n\tint num_ignored = 0, i;\n\tstruct path_pattern *pattern;\n\tstruct pathspec pathspec;\n\n\tif (!argc) {\n\t\tif (!quiet)\n\t\t\tfprintf(stderr, \"no pathspec given.\\n\");\n\t\treturn 0;\n\t}\n\n\t/*\n\t * check-ignore just needs paths. Magic beyond :/ is really\n\t * irrelevant.\n\t */\n\tparse_pathspec(&pathspec,\n\t\t       PATHSPEC_ALL_MAGIC & ~PATHSPEC_FROMTOP,\n\t\t       PATHSPEC_SYMLINK_LEADING_PATH |\n\t\t       PATHSPEC_KEEP_ORDER,\n\t\t       prefix, argv);\n\n\tdie_path_inside_submodule(&the_index, &pathspec);\n",
      "     return 0;\n}\n"
    ]
  },
  {
    "id": "nvm-sh/nvm",
    "org": "nvm-sh",
    "avatarURL": "https://avatars.githubusercontent.com/u/49963700?v=4",
    "name": "nvm-sh/nvm",
    "url": "https://github.com/nvm-sh/nvm",
    "lang": "Shell",
    "desc": "Node Version Manager - POSIX-compliant bash script to manage multiple active Node.js versions.",
    "star_num": 69835,
    "fork_num": 7437,
    "snippets": [
      "  fi\n}\n\nmain \"$@\"\n",
      "  fi\n\n  nvm_echo\n\n  local NVM_PROFILE\n  NVM_PROFILE=\"$(nvm_detect_profile)\"\n  local PROFILE_INSTALL_DIR\n  PROFILE_INSTALL_DIR=\"$(nvm_install_dir | command sed \"s:^$HOME:\\$HOME:\")\"\n\n  SOURCE_STR=\"\\\\nexport NVM_DIR=\\\"${PROFILE_INSTALL_DIR}\\\"\\\\n[ -s \\\"\\$NVM_DIR/nvm.sh\\\" ] && \\\\. \\\"\\$NVM_DIR/nvm.sh\\\"  # This loads nvm\\\\n\"\n\n  # shellcheck disable=SC2016\n  COMPLETION_STR='[ -s \"$NVM_DIR/bash_completion\" ] && \\. \"$NVM_DIR/bash_completion\"  # This loads nvm bash_completion\\n'\n  BASH_OR_ZSH=false\n\n  if [ -z \"${NVM_PROFILE-}\" ] ; then\n    local TRIED_PROFILE\n    if [ -n \"${PROFILE}\" ]; then\n      TRIED_PROFILE=\"${NVM_PROFILE} (as defined in \\$PROFILE), \"\n    fi\n    nvm_echo \"=> Profile not found. Tried ${TRIED_PROFILE-}~/.bashrc, ~/.bash_profile, ~/.zprofile, ~/.zshrc, and ~/.profile.\"\n    nvm_echo \"=> Create one of them and run this script again\"\n    nvm_echo \"   OR\"\n    nvm_echo \"=> Append the following lines to the correct file yourself:\"\n    command printf \"${SOURCE_STR}\"\n    nvm_echo\n  else\n    if nvm_profile_is_bash_or_zsh \"${NVM_PROFILE-}\"; then\n      BASH_OR_ZSH=true\n    fi\n    if ! command grep -qc '/nvm.sh' \"$NVM_PROFILE\"; then\n      nvm_echo \"=> Appending nvm source string to $NVM_PROFILE\"\n      command printf \"${SOURCE_STR}\" >> \"$NVM_PROFILE\"\n    else\n      nvm_echo \"=> nvm source string already in ${NVM_PROFILE}\"\n    fi\n    # shellcheck disable=SC2016\n    if ${BASH_OR_ZSH} && ! command grep -qc '$NVM_DIR/bash_completion' \"$NVM_PROFILE\"; then\n      nvm_echo \"=> Appending bash_completion source string to $NVM_PROFILE\"\n      command printf \"$COMPLETION_STR\" >> \"$NVM_PROFILE\"\n    else\n      nvm_echo \"=> bash_completion source string already in ${NVM_PROFILE}\"\n    fi\n  fi\n  if ${BASH_OR_ZSH} && [ -z \"${NVM_PROFILE-}\" ] ; then\n    nvm_echo \"=> Please also append the following lines to the if you are using bash/zsh shell:\"\n    command printf \"${COMPLETION_STR}\"\n  fi\n\n  # Source nvm\n  # shellcheck source=/dev/null\n  \\. \"$(nvm_install_dir)/nvm.sh\"\n\n  nvm_check_global_modules\n\n  nvm_install_node\n\n  nvm_reset\n\n  nvm_echo \"=> Close and reopen your terminal to start using nvm or run the following to use it now:\"\n  command printf \"${SOURCE_STR}\"\n  if ${BASH_OR_ZSH} ; then\n    command printf \"${COMPLETION_STR}\"\n  fi",
      "      return 1\n    ;;\n  esac\n\n  local KIND\n  case \"${2-}\" in\n    binary | source) KIND=\"${2}\" ;;\n    *)\n      nvm_err 'supported kinds: binary, source'\n      return 2\n    ;;\n  esac\n\n  local VERSION\n  VERSION=\"${3-}\"\n\n  local NVM_OS\n  NVM_OS=\"$(nvm_get_os)\"\n\n  local NVM_ARCH\n  NVM_ARCH=\"$(nvm_get_arch)\"\n  if ! nvm_is_merged_node_version \"${VERSION}\"; then\n    if [ \"${NVM_ARCH}\" = 'armv6l' ] || [ \"${NVM_ARCH}\" = 'armv7l' ]; then\n      NVM_ARCH=\"arm-pi\"\n    fi\n  fi\n\n  # If running MAC M1 :: Node v14.17.0 was the first version to offer official experimental support:\n  # https://github.com/nodejs/node/issues/40126 (although binary distributions aren't available until v16)\n  if \\\n    nvm_version_greater '14.17.0' \"${VERSION}\" \\\n    || (nvm_version_greater_than_or_equal_to \"${VERSION}\" '15.0.0' && nvm_version_greater '16.0.0' \"${VERSION}\") \\\n  ; then\n    if [ \"_${NVM_OS}\" = '_darwin' ] && [ \"${NVM_ARCH}\" = 'arm64' ]; then\n      NVM_ARCH=x64\n    fi\n  fi\n\n  if [ \"${KIND}\" = 'binary' ]; then\n    nvm_echo \"${FLAVOR}-${VERSION}-${NVM_OS}-${NVM_ARCH}\"\n  elif [ \"${KIND}\" = 'source' ]; then\n    nvm_echo \"${FLAVOR}-${VERSION}\"\n  fi\n}\n\nnvm_get_artifact_compression() {\n  local VERSION\n  VERSION=\"${1-}\"\n\n  local NVM_OS\n  NVM_OS=\"$(nvm_get_os)\"\n\n  local COMPRESSION\n  COMPRESSION='tar.gz'\n  if [ \"_${NVM_OS}\" = '_win' ]; then\n    COMPRESSION='zip'\n  elif nvm_supports_xz \"${VERSION}\"; then\n    COMPRESSION='tar.xz'\n  fi\n\n  nvm_echo \"${COMPRESSION}\"\n}\n\n# args: flavor, kind, type, version",
      "        nvm_echo >&2 'Failed to add remote \"origin\" (or set the URL). Please report this!'\n        exit 2\n      }\n    else\n      # Cloning repo\n      command git clone \"$(nvm_source)\" --depth=1 \"${INSTALL_DIR}\" || {\n        nvm_echo >&2 'Failed to clone nvm repo. Please report this!'\n        exit 2\n      }\n    fi\n  fi\n  # Try to fetch tag\n  if command git --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" fetch origin tag \"$NVM_VERSION\" --depth=1 2>/dev/null; then\n    :\n  # Fetch given version\n  elif ! command git --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" fetch origin \"$NVM_VERSION\" --depth=1; then\n    nvm_echo >&2 \"$fetch_error\"\n    exit 1\n  fi\n  command git -c advice.detachedHead=false --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" checkout -f --quiet FETCH_HEAD || {\n    nvm_echo >&2 \"Failed to checkout the given version $NVM_VERSION. Please report this!\"\n    exit 2\n  }\n  if [ -n \"$(command git --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" show-ref refs/heads/master)\" ]; then\n    if command git --no-pager --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" branch --quiet 2>/dev/null; then\n      command git --no-pager --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" branch --quiet -D master >/dev/null 2>&1\n    else\n      nvm_echo >&2 \"Your version of git is out of date. Please update it!\"\n      command git --no-pager --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" branch -D master >/dev/null 2>&1\n    fi\n  fi\n\n  nvm_echo \"=> Compressing and cleaning up git repository\"\n  if ! command git --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" reflog expire --expire=now --all; then\n    nvm_echo >&2 \"Your version of git is out of date. Please update it!\"\n  fi\n  if ! command git --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" gc --auto --aggressive --prune=now ; then\n    nvm_echo >&2 \"Your version of git is out of date. Please update it!\"\n  fi\n  return\n}\n\n#\n# Automatically install Node.js\n#\nnvm_install_node() {\n  local NODE_VERSION_LOCAL\n  NODE_VERSION_LOCAL=\"$(nvm_node_version)\"\n\n  if [ -z \"$NODE_VERSION_LOCAL\" ]; then\n    return 0\n  fi\n\n  nvm_echo \"=> Installing Node.js version $NODE_VERSION_LOCAL\"\n  nvm install \"$NODE_VERSION_LOCAL\"\n  local CURRENT_NVM_NODE\n\n  CURRENT_NVM_NODE=\"$(nvm_version current)\"\n  if [ \"$(nvm_version \"$NODE_VERSION_LOCAL\")\" == \"$CURRENT_NVM_NODE\" ]; then\n    nvm_echo \"=> Node.js version $NODE_VERSION_LOCAL has been successfully installed\"\n  else\n    nvm_echo >&2 \"Failed to install Node.js $NODE_VERSION_LOCAL\"\n  fi\n}",
      "    check_name\n  else\n    rename_test\n  fi\n}\n\nmain \"$@\"\n",
      "      nvm_echo '* `npm` `v6.x` is the last version that works on `node` below `v10.0.0`'\n      $NVM_NPM_CMD install -g npm@6\n    elif \\\n      [ $NVM_IS_12_LTS_OR_ABOVE -eq 0 ] \\\n      || { [ $NVM_IS_13_OR_ABOVE -eq 1 ] && [ $NVM_IS_14_LTS_OR_ABOVE -eq 0 ]; } \\\n      || { [ $NVM_IS_15_OR_ABOVE -eq 1 ] && [ $NVM_IS_16_OR_ABOVE -eq 0 ]; } \\\n    ; then\n      nvm_echo '* `npm` `v7.x` is the last version that works on `node` `v13`, `v15`, below `v12.13`, or `v14.0` - `v14.15`'\n      $NVM_NPM_CMD install -g npm@7\n    elif \\\n      { [ $NVM_IS_12_LTS_OR_ABOVE -eq 1 ] && [ $NVM_IS_13_OR_ABOVE -eq 0 ]; } \\\n      || { [ $NVM_IS_14_LTS_OR_ABOVE -eq 1 ] && [ $NVM_IS_14_17_OR_ABOVE -eq 0 ]; } \\\n      || { [ $NVM_IS_16_OR_ABOVE -eq 1 ] && [ $NVM_IS_16_LTS_OR_ABOVE -eq 0 ]; } \\\n      || { [ $NVM_IS_17_OR_ABOVE -eq 1 ] && [ $NVM_IS_18_OR_ABOVE -eq 0 ]; } \\\n    ; then\n      nvm_echo '* `npm` `v8.x` is the last version that works on `node` `v12`, `v14.13` - `v14.16`, or `v16.0` - `v16.12`'\n      $NVM_NPM_CMD install -g npm@8\n    elif \\\n      [ $NVM_IS_18_17_OR_ABOVE -eq 0 ] \\\n      || { [ $NVM_IS_19_OR_ABOVE -eq 1 ] && [ $NVM_IS_20_5_OR_ABOVE -eq 0 ]; } \\\n    ; then\n      nvm_echo '* `npm` `v9.x` is the last version that works on `node` `< v18.17`, `v19`, or `v20.0` - `v20.4`'\n      $NVM_NPM_CMD install -g npm@9\n    else\n      nvm_echo '* Installing latest `npm`; if this does not work on your node version, please report a bug!'\n      $NVM_NPM_CMD install -g npm\n    fi\n  fi\n  nvm_echo \"* npm upgraded to: v$(npm --version 2>/dev/null)\"\n}\n\n# Make zsh glob matching behave same as bash\n# This fixes the \"zsh: no matches found\" errors\nif [ -z \"${NVM_CD_FLAGS-}\" ]; then\n  export NVM_CD_FLAGS=''\nfi\nif nvm_is_zsh; then\n  NVM_CD_FLAGS=\"-q\"\nfi\n\n# Auto detect the NVM_DIR when not set\nif [ -z \"${NVM_DIR-}\" ]; then\n  # shellcheck disable=SC2128\n  if [ -n \"${BASH_SOURCE-}\" ]; then\n    # shellcheck disable=SC2169,SC3054\n    NVM_SCRIPT_SOURCE=\"${BASH_SOURCE[0]}\"\n  fi\n  # shellcheck disable=SC2086\n  NVM_DIR=\"$(nvm_cd ${NVM_CD_FLAGS} \"$(dirname \"${NVM_SCRIPT_SOURCE:-$0}\")\" >/dev/null && \\pwd)\"\n  export NVM_DIR\nelse\n  # https://unix.stackexchange.com/a/198289\n  case $NVM_DIR in\n    *[!/]*/)\n      NVM_DIR=\"${NVM_DIR%\"${NVM_DIR##*[!/]}\"}\"\n      export NVM_DIR\n      nvm_err \"Warning: \\$NVM_DIR should not have trailing slashes\"\n    ;;\n  esac\nfi\nunset NVM_SCRIPT_SOURCE 2>/dev/null\n\nnvm_tree_contains_path() {\n  local tree",
      "  local INSTALL_DIR\n  INSTALL_DIR=\"$(nvm_install_dir)\"\n  local NVM_SOURCE_LOCAL\n  NVM_SOURCE_LOCAL=\"$(nvm_source script)\"\n  local NVM_EXEC_SOURCE\n  NVM_EXEC_SOURCE=\"$(nvm_source script-nvm-exec)\"\n  local NVM_BASH_COMPLETION_SOURCE\n  NVM_BASH_COMPLETION_SOURCE=\"$(nvm_source script-nvm-bash-completion)\"\n\n  # Downloading to $INSTALL_DIR\n  mkdir -p \"$INSTALL_DIR\"\n  if [ -f \"$INSTALL_DIR/nvm.sh\" ]; then\n    nvm_echo \"=> nvm is already installed in $INSTALL_DIR, trying to update the script\"\n  else\n    nvm_echo \"=> Downloading nvm as script to '$INSTALL_DIR'\"\n  fi\n  nvm_download -s \"$NVM_SOURCE_LOCAL\" -o \"$INSTALL_DIR/nvm.sh\" || {\n    nvm_echo >&2 \"Failed to download '$NVM_SOURCE_LOCAL'\"\n    return 1\n  } &\n  nvm_download -s \"$NVM_EXEC_SOURCE\" -o \"$INSTALL_DIR/nvm-exec\" || {\n    nvm_echo >&2 \"Failed to download '$NVM_EXEC_SOURCE'\"\n    return 2\n  } &\n  nvm_download -s \"$NVM_BASH_COMPLETION_SOURCE\" -o \"$INSTALL_DIR/bash_completion\" || {\n    nvm_echo >&2 \"Failed to download '$NVM_BASH_COMPLETION_SOURCE'\"\n    return 2\n  } &\n  for job in $(jobs -p | command sort)\n  do\n    wait \"$job\" || return $?\n  done\n  chmod a+x \"$INSTALL_DIR/nvm-exec\" || {\n    nvm_echo >&2 \"Failed to mark '$INSTALL_DIR/nvm-exec' as executable\"\n    return 3\n  }\n}\n\nnvm_try_profile() {\n  if [ -z \"${1-}\" ] || [ ! -f \"${1}\" ]; then\n    return 1\n  fi\n  nvm_echo \"${1}\"\n}\n\n#\n# Detect profile file if not specified as environment variable\n# (eg: PROFILE=~/.myprofile)\n# The echo'ed path is guaranteed to be an existing file\n# Otherwise, an empty string is returned\n#\nnvm_detect_profile() {\n  if [ \"${PROFILE-}\" = '/dev/null' ]; then\n    # the user has specifically requested NOT to have nvm touch their profile\n    return\n  fi\n\n  if [ -n \"${PROFILE}\" ] && [ -f \"${PROFILE}\" ]; then\n    nvm_echo \"${PROFILE}\"\n    return\n  fi\n\n  local DETECTED_PROFILE\n  DETECTED_PROFILE=''",
      "# Node.js version to install\n#\nnvm_node_version() {\n  nvm_echo \"$NODE_VERSION\"\n}\n\nnvm_download() {\n  if nvm_has \"curl\"; then\n    curl --fail --compressed -q \"$@\"\n  elif nvm_has \"wget\"; then\n    # Emulate curl with wget\n    ARGS=$(nvm_echo \"$@\" | command sed -e 's/--progress-bar /--progress=bar /' \\\n                            -e 's/--compressed //' \\\n                            -e 's/--fail //' \\\n                            -e 's/-L //' \\\n                            -e 's/-I /--server-response /' \\\n                            -e 's/-s /-q /' \\\n                            -e 's/-sS /-nv /' \\\n                            -e 's/-o /-O /' \\\n                            -e 's/-C - /-c /')\n    # shellcheck disable=SC2086\n    eval wget $ARGS\n  fi\n}\n\ninstall_nvm_from_git() {\n  local INSTALL_DIR\n  INSTALL_DIR=\"$(nvm_install_dir)\"\n  local NVM_VERSION\n  NVM_VERSION=\"${NVM_INSTALL_VERSION:-$(nvm_latest_version)}\"\n  if [ -n \"${NVM_INSTALL_VERSION:-}\" ]; then\n    # Check if version is an existing ref\n    if command git ls-remote \"$(nvm_source \"git\")\" \"$NVM_VERSION\" | nvm_grep -q \"$NVM_VERSION\" ; then\n      :\n    # Check if version is an existing changeset\n    elif ! nvm_download -o /dev/null \"$(nvm_source \"script-nvm-exec\")\"; then\n      nvm_echo >&2 \"Failed to find '$NVM_VERSION' version.\"\n      exit 1\n    fi\n  fi\n\n  local fetch_error\n  if [ -d \"$INSTALL_DIR/.git\" ]; then\n    # Updating repo\n    nvm_echo \"=> nvm is already installed in $INSTALL_DIR, trying to update using git\"\n    command printf '\\r=> '\n    fetch_error=\"Failed to update nvm with $NVM_VERSION, run 'git fetch' in $INSTALL_DIR yourself.\"\n  else\n    fetch_error=\"Failed to fetch origin with $NVM_VERSION. Please report this!\"\n    nvm_echo \"=> Downloading nvm from git to '$INSTALL_DIR'\"\n    command printf '\\r=> '\n    mkdir -p \"${INSTALL_DIR}\"\n    if [ \"$(ls -A \"${INSTALL_DIR}\")\" ]; then\n      # Initializing repo\n      command git init \"${INSTALL_DIR}\" || {\n        nvm_echo >&2 'Failed to initialize nvm repo. Please report this!'\n        exit 2\n      }\n      command git --git-dir=\"${INSTALL_DIR}/.git\" remote add origin \"$(nvm_source)\" 2> /dev/null \\\n        || command git --git-dir=\"${INSTALL_DIR}/.git\" remote set-url origin \"$(nvm_source)\" || {\n        nvm_echo >&2 'Failed to add remote \"origin\" (or set the URL). Please report this!'\n        exit 2\n      }\n    else",
      "          nvm_err \"nvm: Cannot uninstall currently-active io.js version, ${VERSION} (inferred from ${PATTERN}).\"\n        else\n          nvm_err \"nvm: Cannot uninstall currently-active node version, ${VERSION} (inferred from ${PATTERN}).\"\n        fi\n        return 1\n      fi\n\n      if ! nvm_is_version_installed \"${VERSION}\"; then\n        nvm_err \"${VERSION} version is not installed...\"\n        return\n      fi\n\n      local SLUG_BINARY\n      local SLUG_SOURCE\n      if nvm_is_iojs_version \"${VERSION}\"; then\n        SLUG_BINARY=\"$(nvm_get_download_slug iojs binary std \"${VERSION}\")\"\n        SLUG_SOURCE=\"$(nvm_get_download_slug iojs source std \"${VERSION}\")\"\n      else\n        SLUG_BINARY=\"$(nvm_get_download_slug node binary std \"${VERSION}\")\"\n        SLUG_SOURCE=\"$(nvm_get_download_slug node source std \"${VERSION}\")\"\n      fi\n\n      local NVM_SUCCESS_MSG\n      if nvm_is_iojs_version \"${VERSION}\"; then\n        NVM_SUCCESS_MSG=\"Uninstalled io.js $(nvm_strip_iojs_prefix \"${VERSION}\")\"\n      else\n        NVM_SUCCESS_MSG=\"Uninstalled node ${VERSION}\"\n      fi\n\n      local VERSION_PATH\n      VERSION_PATH=\"$(nvm_version_path \"${VERSION}\")\"\n      if ! nvm_check_file_permissions \"${VERSION_PATH}\"; then\n        nvm_err 'Cannot uninstall, incorrect permissions on installation folder.'\n        nvm_err 'This is usually caused by running `npm install -g` as root. Run the following commands as root to fix the permissions and then try again.'\n        nvm_err\n        nvm_err \"  chown -R $(whoami) \\\"$(nvm_sanitize_path \"${VERSION_PATH}\")\\\"\"\n        nvm_err \"  chmod -R u+w \\\"$(nvm_sanitize_path \"${VERSION_PATH}\")\\\"\"\n        return 1\n      fi\n\n      # Delete all files related to target version.\n      local CACHE_DIR\n      CACHE_DIR=\"$(nvm_cache_dir)\"\n      command rm -rf \\\n        \"${CACHE_DIR}/bin/${SLUG_BINARY}/files\" \\\n        \"${CACHE_DIR}/src/${SLUG_SOURCE}/files\" \\\n        \"${VERSION_PATH}\" 2>/dev/null\n      nvm_echo \"${NVM_SUCCESS_MSG}\"\n\n      # rm any aliases that point to uninstalled version.\n      for ALIAS in $(nvm_grep -l \"${VERSION}\" \"$(nvm_alias_path)/*\" 2>/dev/null); do\n        nvm unalias \"$(command basename \"${ALIAS}\")\"\n      done\n    ;;\n    \"deactivate\")\n      local NVM_SILENT\n      while [ $# -ne 0 ]; do\n        case \"${1}\" in\n          --silent) NVM_SILENT=1 ;;\n          --) ;;\n        esac\n        shift\n      done\n      local NEWPATH",
      "\n  local fetch_error\n  if [ -d \"$INSTALL_DIR/.git\" ]; then\n    # Updating repo\n    nvm_echo \"=> nvm is already installed in $INSTALL_DIR, trying to update using git\"\n    command printf '\\r=> '\n    fetch_error=\"Failed to update nvm with $NVM_VERSION, run 'git fetch' in $INSTALL_DIR yourself.\"\n  else\n    fetch_error=\"Failed to fetch origin with $NVM_VERSION. Please report this!\"\n    nvm_echo \"=> Downloading nvm from git to '$INSTALL_DIR'\"\n    command printf '\\r=> '\n    mkdir -p \"${INSTALL_DIR}\"\n    if [ \"$(ls -A \"${INSTALL_DIR}\")\" ]; then\n      # Initializing repo\n      command git init \"${INSTALL_DIR}\" || {\n        nvm_echo >&2 'Failed to initialize nvm repo. Please report this!'\n        exit 2\n      }\n      command git --git-dir=\"${INSTALL_DIR}/.git\" remote add origin \"$(nvm_source)\" 2> /dev/null \\\n        || command git --git-dir=\"${INSTALL_DIR}/.git\" remote set-url origin \"$(nvm_source)\" || {\n        nvm_echo >&2 'Failed to add remote \"origin\" (or set the URL). Please report this!'\n        exit 2\n      }\n    else\n      # Cloning repo\n      command git clone \"$(nvm_source)\" --depth=1 \"${INSTALL_DIR}\" || {\n        nvm_echo >&2 'Failed to clone nvm repo. Please report this!'\n        exit 2\n      }\n    fi\n  fi\n  # Try to fetch tag\n  if command git --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" fetch origin tag \"$NVM_VERSION\" --depth=1 2>/dev/null; then\n    :\n  # Fetch given version\n  elif ! command git --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" fetch origin \"$NVM_VERSION\" --depth=1; then\n    nvm_echo >&2 \"$fetch_error\"\n    exit 1\n  fi\n  command git -c advice.detachedHead=false --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" checkout -f --quiet FETCH_HEAD || {\n    nvm_echo >&2 \"Failed to checkout the given version $NVM_VERSION. Please report this!\"\n    exit 2\n  }\n  if [ -n \"$(command git --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" show-ref refs/heads/master)\" ]; then\n    if command git --no-pager --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" branch --quiet 2>/dev/null; then\n      command git --no-pager --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" branch --quiet -D master >/dev/null 2>&1\n    else\n      nvm_echo >&2 \"Your version of git is out of date. Please update it!\"\n      command git --no-pager --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" branch -D master >/dev/null 2>&1\n    fi\n  fi\n\n  nvm_echo \"=> Compressing and cleaning up git repository\"\n  if ! command git --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" reflog expire --expire=now --all; then\n    nvm_echo >&2 \"Your version of git is out of date. Please update it!\"\n  fi\n  if ! command git --git-dir=\"$INSTALL_DIR\"/.git --work-tree=\"$INSTALL_DIR\" gc --auto --aggressive --prune=now ; then\n    nvm_echo >&2 \"Your version of git is out of date. Please update it!\"\n  fi\n  return\n}\n\n#\n# Automatically install Node.js"
    ]
  },
  {
    "id": "bitcoin/bitcoin",
    "org": "bitcoin",
    "avatarURL": "https://avatars.githubusercontent.com/u/528860?v=4",
    "name": "bitcoin/bitcoin",
    "url": "https://github.com/bitcoin/bitcoin",
    "lang": "C++",
    "desc": "Bitcoin Core integration/staging tree.",
    "star_num": 71236,
    "fork_num": 35621,
    "snippets": [
      "//   - \"#define BUILD_GIT_TAG ...\", if the top commit is tagged\n//   - \"#define BUILD_GIT_COMMIT ...\", if the top commit is not tagged\n//   - \"// No build information available\", if proper git information is not available\n#endif\n\n//! git will put \"#define GIT_COMMIT_ID ...\" on the next line inside archives. $Format:%n#define GIT_COMMIT_ID \"%H\"$\n\n#ifdef BUILD_GIT_TAG\n    #define BUILD_DESC BUILD_GIT_TAG\n    #define BUILD_SUFFIX \"\"\n#else\n    #define BUILD_DESC \"v\" PACKAGE_VERSION\n    #if CLIENT_VERSION_IS_RELEASE\n        #define BUILD_SUFFIX \"\"\n    #elif defined(BUILD_GIT_COMMIT)\n        #define BUILD_SUFFIX \"-\" BUILD_GIT_COMMIT\n    #elif defined(GIT_COMMIT_ID)\n        #define BUILD_SUFFIX \"-g\" GIT_COMMIT_ID\n    #else\n        #define BUILD_SUFFIX \"-unk\"\n    #endif\n#endif\n\nstatic std::string FormatVersion(int nVersion)\n{\n    return strprintf(\"%d.%d.%d\", nVersion / 10000, (nVersion / 100) % 100, nVersion % 100);\n}\n\nstd::string FormatFullVersion()\n{\n    static const std::string CLIENT_BUILD(BUILD_DESC BUILD_SUFFIX);\n    return CLIENT_BUILD;\n}\n\n/**\n * Format the subversion field according to BIP 14 spec (https://github.com/bitcoin/bips/blob/master/bip-0014.mediawiki)\n */\nstd::string FormatSubVersion(const std::string& name, int nClientVersion, const std::vector<std::string>& comments)\n{\n    std::ostringstream ss;\n    ss << \"/\";\n    ss << name << \":\" << FormatVersion(nClientVersion);\n    if (!comments.empty())\n    {\n        std::vector<std::string>::const_iterator it(comments.begin());\n        ss << \"(\" << *it;\n        for(++it; it != comments.end(); ++it)\n            ss << \"; \" << *it;\n        ss << \")\";\n    }\n    ss << \"/\";\n    return ss.str();\n}\n\nstd::string CopyrightHolders(const std::string& strPrefix)\n{\n    const auto copyright_devs = strprintf(_(COPYRIGHT_HOLDERS).translated, COPYRIGHT_HOLDERS_SUBSTITUTION);\n    std::string strCopyrightHolders = strPrefix + copyright_devs;\n\n    // Make sure Bitcoin Core copyright is not removed by accident\n    if (copyright_devs.find(\"Bitcoin Core\") == std::string::npos) {\n        strCopyrightHolders += \"\\n\" + strPrefix + \"The Bitcoin Core developers\";\n    }\n    return strCopyrightHolders;",
      "// Copyright (c) 2021-2022 The Bitcoin Core developers\n// Distributed under the MIT software license, see the accompanying\n// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n\n#include <interfaces/init.h>\n\n#include <memory>\n\nnamespace interfaces {\nstd::unique_ptr<Init> MakeWalletInit(int argc, char* argv[], int& exit_status)\n{\n    return std::make_unique<Init>();\n}\n} // namespace interfaces\n",
      "#include <external_signer.h>\n#include <wallet/external_signer_scriptpubkeyman.h>\n\n#include <iostream>\n#include <memory>\n#include <stdexcept>\n#include <string>\n#include <utility>\n#include <vector>\n\nnamespace wallet {\nbool ExternalSignerScriptPubKeyMan::SetupDescriptor(std::unique_ptr<Descriptor> desc)\n{\n    LOCK(cs_desc_man);\n    assert(m_storage.IsWalletFlagSet(WALLET_FLAG_DESCRIPTORS));\n    assert(m_storage.IsWalletFlagSet(WALLET_FLAG_EXTERNAL_SIGNER));\n\n    int64_t creation_time = GetTime();\n\n    // Make the descriptor\n    WalletDescriptor w_desc(std::move(desc), creation_time, 0, 0, 0);\n    m_wallet_descriptor = w_desc;\n\n    // Store the descriptor\n    WalletBatch batch(m_storage.GetDatabase());\n    if (!batch.WriteDescriptor(GetID(), m_wallet_descriptor)) {\n        throw std::runtime_error(std::string(__func__) + \": writing descriptor failed\");\n    }\n\n    // TopUp\n    TopUp();\n\n    m_storage.UnsetBlankWalletFlag(batch);\n    return true;\n}\n\nExternalSigner ExternalSignerScriptPubKeyMan::GetExternalSigner() {\n    const std::string command = gArgs.GetArg(\"-signer\", \"\");\n    if (command == \"\") throw std::runtime_error(std::string(__func__) + \": restart bitcoind with -signer=<cmd>\");\n    std::vector<ExternalSigner> signers;\n    ExternalSigner::Enumerate(command, signers, Params().GetChainTypeString());\n    if (signers.empty()) throw std::runtime_error(std::string(__func__) + \": No external signers found\");\n    // TODO: add fingerprint argument instead of failing in case of multiple signers.\n    if (signers.size() > 1) throw std::runtime_error(std::string(__func__) + \": More than one external signer found. Please connect only one at a time.\");\n    return signers[0];\n}\n\nbool ExternalSignerScriptPubKeyMan::DisplayAddress(const CScript scriptPubKey, const ExternalSigner &signer) const\n{\n    // TODO: avoid the need to infer a descriptor from inside a descriptor wallet\n    auto provider = GetSolvingProvider(scriptPubKey);\n    auto descriptor = InferDescriptor(scriptPubKey, *provider);\n\n    signer.DisplayAddress(descriptor->ToString());\n    // TODO inspect result\n    return true;\n}\n\n// If sign is true, transaction must previously have been filled\nTransactionError ExternalSignerScriptPubKeyMan::FillPSBT(PartiallySignedTransaction& psbt, const PrecomputedTransactionData& txdata, int sighash_type, bool sign, bool bip32derivs, int* n_signed, bool finalize) const\n{\n    if (!sign) {\n        return DescriptorScriptPubKeyMan::FillPSBT(psbt, txdata, sighash_type, false, bip32derivs, n_signed, finalize);\n    }",
      "        // Check after just a few txs that combining buckets works as expected\n        if (blocknum == 3) {\n            // At this point we should need to combine 3 buckets to get enough data points\n            // So estimateFee(1) should fail and estimateFee(2) should return somewhere around\n            // 9*baserate.  estimateFee(2) %'s are 100,100,90 = average 97%\n            BOOST_CHECK(feeEst.estimateFee(1) == CFeeRate(0));\n            BOOST_CHECK(feeEst.estimateFee(2).GetFeePerK() < 9*baseRate.GetFeePerK() + deltaFee);\n            BOOST_CHECK(feeEst.estimateFee(2).GetFeePerK() > 9*baseRate.GetFeePerK() - deltaFee);\n        }\n    }\n\n    std::vector<CAmount> origFeeEst;\n    // Highest feerate is 10*baseRate and gets in all blocks,\n    // second highest feerate is 9*baseRate and gets in 9/10 blocks = 90%,\n    // third highest feerate is 8*base rate, and gets in 8/10 blocks = 80%,\n    // so estimateFee(1) would return 10*baseRate but is hardcoded to return failure\n    // Second highest feerate has 100% chance of being included by 2 blocks,\n    // so estimateFee(2) should return 9*baseRate etc...\n    for (int i = 1; i < 10;i++) {\n        origFeeEst.push_back(feeEst.estimateFee(i).GetFeePerK());\n        if (i > 2) { // Fee estimates should be monotonically decreasing\n            BOOST_CHECK(origFeeEst[i-1] <= origFeeEst[i-2]);\n        }\n        int mult = 11-i;\n        if (i % 2 == 0) { //At scale 2, test logic is only correct for even targets\n            BOOST_CHECK(origFeeEst[i-1] < mult*baseRate.GetFeePerK() + deltaFee);\n            BOOST_CHECK(origFeeEst[i-1] > mult*baseRate.GetFeePerK() - deltaFee);\n        }\n    }\n    // Fill out rest of the original estimates\n    for (int i = 10; i <= 48; i++) {\n        origFeeEst.push_back(feeEst.estimateFee(i).GetFeePerK());\n    }\n\n    // Mine 50 more blocks with no transactions happening, estimates shouldn't change\n    // We haven't decayed the moving average enough so we still have enough data points in every bucket\n    while (blocknum < 250)\n        mpool.removeForBlock(block, ++blocknum);\n\n    BOOST_CHECK(feeEst.estimateFee(1) == CFeeRate(0));\n    for (int i = 2; i < 10;i++) {\n        BOOST_CHECK(feeEst.estimateFee(i).GetFeePerK() < origFeeEst[i-1] + deltaFee);\n        BOOST_CHECK(feeEst.estimateFee(i).GetFeePerK() > origFeeEst[i-1] - deltaFee);\n    }\n\n\n    // Mine 15 more blocks with lots of transactions happening and not getting mined\n    // Estimates should go up\n    while (blocknum < 265) {\n        for (int j = 0; j < 10; j++) { // For each fee multiple\n            for (int k = 0; k < 4; k++) { // add 4 fee txs\n                tx.vin[0].prevout.n = 10000*blocknum+100*j+k;\n                uint256 hash = tx.GetHash();\n                mpool.addUnchecked(entry.Fee(feeV[j]).Time(Now<NodeSeconds>()).Height(blocknum).FromTx(tx));\n                txHashes[j].push_back(hash);\n            }\n        }\n        mpool.removeForBlock(block, ++blocknum);\n    }\n\n    for (int i = 1; i < 10;i++) {\n        BOOST_CHECK(feeEst.estimateFee(i) == CFeeRate(0) || feeEst.estimateFee(i).GetFeePerK() > origFeeEst[i-1] - deltaFee);\n    }\n",
      "\n#include <index/txindex.h>\n\n#include <common/args.h>\n#include <index/disktxpos.h>\n#include <logging.h>\n#include <node/blockstorage.h>\n#include <validation.h>\n\nconstexpr uint8_t DB_TXINDEX{'t'};\n\nstd::unique_ptr<TxIndex> g_txindex;\n\n\n/** Access to the txindex database (indexes/txindex/) */\nclass TxIndex::DB : public BaseIndex::DB\n{\npublic:\n    explicit DB(size_t n_cache_size, bool f_memory = false, bool f_wipe = false);\n\n    /// Read the disk location of the transaction data with the given hash. Returns false if the\n    /// transaction hash is not indexed.\n    bool ReadTxPos(const uint256& txid, CDiskTxPos& pos) const;\n\n    /// Write a batch of transaction positions to the DB.\n    bool WriteTxs(const std::vector<std::pair<uint256, CDiskTxPos>>& v_pos);\n};\n\nTxIndex::DB::DB(size_t n_cache_size, bool f_memory, bool f_wipe) :\n    BaseIndex::DB(gArgs.GetDataDirNet() / \"indexes\" / \"txindex\", n_cache_size, f_memory, f_wipe)\n{}\n\nbool TxIndex::DB::ReadTxPos(const uint256 &txid, CDiskTxPos& pos) const\n{\n    return Read(std::make_pair(DB_TXINDEX, txid), pos);\n}\n\nbool TxIndex::DB::WriteTxs(const std::vector<std::pair<uint256, CDiskTxPos>>& v_pos)\n{\n    CDBBatch batch(*this);\n    for (const auto& tuple : v_pos) {\n        batch.Write(std::make_pair(DB_TXINDEX, tuple.first), tuple.second);\n    }\n    return WriteBatch(batch);\n}\n\nTxIndex::TxIndex(std::unique_ptr<interfaces::Chain> chain, size_t n_cache_size, bool f_memory, bool f_wipe)\n    : BaseIndex(std::move(chain), \"txindex\"), m_db(std::make_unique<TxIndex::DB>(n_cache_size, f_memory, f_wipe))\n{}\n\nTxIndex::~TxIndex() = default;\n\nbool TxIndex::CustomAppend(const interfaces::BlockInfo& block)\n{\n    // Exclude genesis block transaction because outputs are not spendable.\n    if (block.height == 0) return true;\n\n    assert(block.data);\n    CDiskTxPos pos({block.file_number, block.data_pos}, GetSizeOfCompactSize(block.data->vtx.size()));\n    std::vector<std::pair<uint256, CDiskTxPos>> vPos;\n    vPos.reserve(block.data->vtx.size());\n    for (const auto& tx : block.data->vtx) {\n        vPos.emplace_back(tx->GetHash(), pos);\n        pos.nTxOffset += ::GetSerializeSize(*tx, CLIENT_VERSION);",
      "        }\n        return false;\n    }\n\n    fDbEnvInit = true;\n    fMockDb = false;\n    return true;\n}\n\n//! Construct an in-memory mock Berkeley environment for testing\nBerkeleyEnvironment::BerkeleyEnvironment() : m_use_shared_memory(false)\n{\n    Reset();\n\n    LogPrint(BCLog::WALLETDB, \"BerkeleyEnvironment::MakeMock\\n\");\n\n    dbenv->set_cachesize(1, 0, 1);\n    dbenv->set_lg_bsize(10485760 * 4);\n    dbenv->set_lg_max(10485760);\n    dbenv->set_lk_max_locks(10000);\n    dbenv->set_lk_max_objects(10000);\n    dbenv->set_flags(DB_AUTO_COMMIT, 1);\n    dbenv->log_set_config(DB_LOG_IN_MEMORY, 1);\n    int ret = dbenv->open(nullptr,\n                         DB_CREATE |\n                             DB_INIT_LOCK |\n                             DB_INIT_LOG |\n                             DB_INIT_MPOOL |\n                             DB_INIT_TXN |\n                             DB_THREAD |\n                             DB_PRIVATE,\n                         S_IRUSR | S_IWUSR);\n    if (ret > 0) {\n        throw std::runtime_error(strprintf(\"BerkeleyEnvironment::MakeMock: Error %d opening database environment.\", ret));\n    }\n\n    fDbEnvInit = true;\n    fMockDb = true;\n}\n\n/** RAII class that automatically cleanses its data on destruction */\nclass SafeDbt final\n{\n    Dbt m_dbt;\n\npublic:\n    // construct Dbt with internally-managed data\n    SafeDbt();\n    // construct Dbt with provided data\n    SafeDbt(void* data, size_t size);\n    ~SafeDbt();\n\n    // delegate to Dbt\n    const void* get_data() const;\n    uint32_t get_size() const;\n\n    // conversion operator to access the underlying Dbt\n    operator Dbt*();\n};\n\nSafeDbt::SafeDbt()\n{\n    m_dbt.set_flags(DB_DBT_MALLOC);\n}",
      "#include <cassert>\n\nstd::string ConnectionTypeAsString(ConnectionType conn_type)\n{\n    switch (conn_type) {\n    case ConnectionType::INBOUND:\n        return \"inbound\";\n    case ConnectionType::MANUAL:\n        return \"manual\";\n    case ConnectionType::FEELER:\n        return \"feeler\";\n    case ConnectionType::OUTBOUND_FULL_RELAY:\n        return \"outbound-full-relay\";\n    case ConnectionType::BLOCK_RELAY:\n        return \"block-relay-only\";\n    case ConnectionType::ADDR_FETCH:\n        return \"addr-fetch\";\n    } // no default case, so the compiler can warn about missing cases\n\n    assert(false);\n}\n",
      "     * if both are the case.\n     * */\n    if (this->IsOverflow()) this->FullReduce();\n    if (c0) this->FullReduce();\n}\n\nvoid Num3072::SetToOne()\n{\n    this->limbs[0] = 1;\n    for (int i = 1; i < LIMBS; ++i) this->limbs[i] = 0;\n}\n\nvoid Num3072::Divide(const Num3072& a)\n{\n    if (this->IsOverflow()) this->FullReduce();\n\n    Num3072 inv{};\n    if (a.IsOverflow()) {\n        Num3072 b = a;\n        b.FullReduce();\n        inv = b.GetInverse();\n    } else {\n        inv = a.GetInverse();\n    }\n\n    this->Multiply(inv);\n    if (this->IsOverflow()) this->FullReduce();\n}\n\nNum3072::Num3072(const unsigned char (&data)[BYTE_SIZE]) {\n    for (int i = 0; i < LIMBS; ++i) {\n        if (sizeof(limb_t) == 4) {\n            this->limbs[i] = ReadLE32(data + 4 * i);\n        } else if (sizeof(limb_t) == 8) {\n            this->limbs[i] = ReadLE64(data + 8 * i);\n        }\n    }\n}\n\nvoid Num3072::ToBytes(unsigned char (&out)[BYTE_SIZE]) {\n    for (int i = 0; i < LIMBS; ++i) {\n        if (sizeof(limb_t) == 4) {\n            WriteLE32(out + i * 4, this->limbs[i]);\n        } else if (sizeof(limb_t) == 8) {\n            WriteLE64(out + i * 8, this->limbs[i]);\n        }\n    }\n}\n\nNum3072 MuHash3072::ToNum3072(Span<const unsigned char> in) {\n    unsigned char tmp[Num3072::BYTE_SIZE];\n\n    uint256 hashed_in{(HashWriter{} << in).GetSHA256()};\n    static_assert(sizeof(tmp) % ChaCha20Aligned::BLOCKLEN == 0);\n    ChaCha20Aligned{MakeByteSpan(hashed_in)}.Keystream(MakeWritableByteSpan(tmp));\n    Num3072 out{tmp};\n\n    return out;\n}\n\nMuHash3072::MuHash3072(Span<const unsigned char> in) noexcept\n{\n    m_numerator = ToNum3072(in);\n}",
      "    // will slow down fuzzing.\n    g_socks5_recv_timeout = (fuzzed_data_provider.ConsumeBool() && std::getenv(\"FUZZED_SOCKET_FAKE_LATENCY\") != nullptr) ? 1ms : default_socks5_recv_timeout;\n    FuzzedSock fuzzed_sock = ConsumeSock(fuzzed_data_provider);\n    // This Socks5(...) fuzzing harness would have caught CVE-2017-18350 within\n    // a few seconds of fuzzing.\n    (void)Socks5(fuzzed_data_provider.ConsumeRandomLengthString(512),\n                 fuzzed_data_provider.ConsumeIntegral<uint16_t>(),\n                 fuzzed_data_provider.ConsumeBool() ? &proxy_credentials : nullptr,\n                 fuzzed_sock);\n}\n",
      "            }\n        }\n\n        // Same for `vvTried`.\n        for (size_t i = 0; i < ADDRMAN_TRIED_BUCKET_COUNT; ++i) {\n            for (size_t j = 0; j < ADDRMAN_BUCKET_SIZE; ++j) {\n                if (!IdsReferToSameAddress(m_impl->vvTried[i][j], other.m_impl->vvTried[i][j])) {\n                    return false;\n                }\n            }\n        }\n\n        return true;\n    }\n};\n\nFUZZ_TARGET(addrman, .init = initialize_addrman)\n{\n    FuzzedDataProvider fuzzed_data_provider(buffer.data(), buffer.size());\n    SetMockTime(ConsumeTime(fuzzed_data_provider));\n    NetGroupManager netgroupman{ConsumeNetGroupManager(fuzzed_data_provider)};\n    auto addr_man_ptr = std::make_unique<AddrManDeterministic>(netgroupman, fuzzed_data_provider);\n    if (fuzzed_data_provider.ConsumeBool()) {\n        const std::vector<uint8_t> serialized_data{ConsumeRandomLengthByteVector(fuzzed_data_provider)};\n        CDataStream ds(serialized_data, SER_DISK, INIT_PROTO_VERSION);\n        const auto ser_version{fuzzed_data_provider.ConsumeIntegral<int32_t>()};\n        ds.SetVersion(ser_version);\n        try {\n            ds >> *addr_man_ptr;\n        } catch (const std::ios_base::failure&) {\n            addr_man_ptr = std::make_unique<AddrManDeterministic>(netgroupman, fuzzed_data_provider);\n        }\n    }\n    AddrManDeterministic& addr_man = *addr_man_ptr;\n    LIMITED_WHILE(fuzzed_data_provider.ConsumeBool(), 10000) {\n        CallOneOf(\n            fuzzed_data_provider,\n            [&] {\n                addr_man.ResolveCollisions();\n            },\n            [&] {\n                (void)addr_man.SelectTriedCollision();\n            },\n            [&] {\n                std::vector<CAddress> addresses;\n                LIMITED_WHILE(fuzzed_data_provider.ConsumeBool(), 10000) {\n                    addresses.push_back(ConsumeAddress(fuzzed_data_provider));\n                }\n                addr_man.Add(addresses, ConsumeNetAddr(fuzzed_data_provider), std::chrono::seconds{ConsumeTime(fuzzed_data_provider, 0, 100000000)});\n            },\n            [&] {\n                addr_man.Good(ConsumeService(fuzzed_data_provider), NodeSeconds{std::chrono::seconds{ConsumeTime(fuzzed_data_provider)}});\n            },\n            [&] {\n                addr_man.Attempt(ConsumeService(fuzzed_data_provider), fuzzed_data_provider.ConsumeBool(), NodeSeconds{std::chrono::seconds{ConsumeTime(fuzzed_data_provider)}});\n            },\n            [&] {\n                addr_man.Connected(ConsumeService(fuzzed_data_provider), NodeSeconds{std::chrono::seconds{ConsumeTime(fuzzed_data_provider)}});\n            },\n            [&] {\n                addr_man.SetServices(ConsumeService(fuzzed_data_provider), ConsumeWeakEnum(fuzzed_data_provider, ALL_SERVICE_FLAGS));\n            });\n    }\n    const AddrMan& const_addr_man{addr_man};"
    ]
  },
  {
    "id": "home-assistant/core",
    "org": "home-assistant",
    "avatarURL": "https://avatars.githubusercontent.com/u/13844975?v=4",
    "name": "home-assistant/core",
    "url": "https://github.com/home-assistant/core",
    "lang": "Python",
    "desc": "Open source home automation that puts local control and privacy first.",
    "star_num": 62641,
    "fork_num": 24536,
    "snippets": [
      "CONF_AVOID_FERRIES = \"avoid_ferries\"\n\nDEFAULT_NAME = \"Waze Travel Time\"\nDEFAULT_REALTIME = True\nDEFAULT_VEHICLE_TYPE = \"car\"\nDEFAULT_AVOID_TOLL_ROADS = False\nDEFAULT_AVOID_SUBSCRIPTION_ROADS = False\nDEFAULT_AVOID_FERRIES = False\n\nIMPERIAL_UNITS = \"imperial\"\nMETRIC_UNITS = \"metric\"\nUNITS = [METRIC_UNITS, IMPERIAL_UNITS]\n\nREGIONS = [\"us\", \"na\", \"eu\", \"il\", \"au\"]\nVEHICLE_TYPES = [\"car\", \"taxi\", \"motorcycle\"]\n\nDEFAULT_OPTIONS: dict[str, str | bool] = {\n    CONF_REALTIME: DEFAULT_REALTIME,\n    CONF_VEHICLE_TYPE: DEFAULT_VEHICLE_TYPE,\n    CONF_UNITS: METRIC_UNITS,\n    CONF_AVOID_FERRIES: DEFAULT_AVOID_FERRIES,\n    CONF_AVOID_SUBSCRIPTION_ROADS: DEFAULT_AVOID_SUBSCRIPTION_ROADS,\n    CONF_AVOID_TOLL_ROADS: DEFAULT_AVOID_TOLL_ROADS,\n}\n",
      "\nasync def test_full_user_flow(\n    hass: HomeAssistant,\n    mock_setup_entry: MagicMock,\n    snapshot: SnapshotAssertion,\n) -> None:\n    \"\"\"Test the full user configuration flow.\"\"\"\n    result = await hass.config_entries.flow.async_init(\n        DOMAIN, context={\"source\": SOURCE_USER}\n    )\n\n    assert result.get(\"type\") == FlowResultType.FORM\n    assert result.get(\"step_id\") == \"user\"\n    assert \"flow_id\" in result\n\n    result2 = await hass.config_entries.flow.async_configure(\n        result[\"flow_id\"],\n        user_input={},\n    )\n\n    assert result2.get(\"type\") == FlowResultType.CREATE_ENTRY\n    assert result2 == snapshot\n\n    assert len(mock_setup_entry.mock_calls) == 1\n",
      "\"\"\"Config flow for OwnTracks.\"\"\"\nimport secrets\n\nfrom homeassistant import config_entries\nfrom homeassistant.components import cloud, webhook\nfrom homeassistant.const import CONF_WEBHOOK_ID\n\nfrom .const import DOMAIN\nfrom .helper import supports_encryption\n\nCONF_SECRET = \"secret\"\nCONF_CLOUDHOOK = \"cloudhook\"\n\n\nclass OwnTracksFlow(config_entries.ConfigFlow, domain=DOMAIN):\n    \"\"\"Set up OwnTracks.\"\"\"\n\n    VERSION = 1\n\n    async def async_step_user(self, user_input=None):\n        \"\"\"Handle a user initiated set up flow to create OwnTracks webhook.\"\"\"\n        if self._async_current_entries():\n            return self.async_abort(reason=\"single_instance_allowed\")\n\n        if user_input is None:\n            return self.async_show_form(step_id=\"user\")\n\n        try:\n            webhook_id, webhook_url, cloudhook = await self._get_webhook_id()\n        except cloud.CloudNotConnected:\n            return self.async_abort(reason=\"cloud_not_connected\")\n\n        secret = secrets.token_hex(16)\n\n        if supports_encryption():\n            secret_desc = (\n                f\"The encryption key is {secret} (on Android under Preferences >\"\n                \" Advanced)\"\n            )\n        else:\n            secret_desc = \"Encryption is not supported because nacl is not installed.\"\n\n        return self.async_create_entry(\n            title=\"OwnTracks\",\n            data={\n                CONF_WEBHOOK_ID: webhook_id,\n                CONF_SECRET: secret,\n                CONF_CLOUDHOOK: cloudhook,\n            },\n            description_placeholders={\n                \"secret\": secret_desc,\n                \"webhook_url\": webhook_url,\n                \"android_url\": \"https://play.google.com/store/apps/details?id=org.owntracks.android\",\n                \"ios_url\": \"https://itunes.apple.com/us/app/owntracks/id692424691?mt=8\",\n                \"docs_url\": \"https://www.home-assistant.io/integrations/owntracks/\",\n            },\n        )\n\n    async def _get_webhook_id(self):\n        \"\"\"Generate webhook ID.\"\"\"\n        webhook_id = webhook.async_generate_id()\n        if cloud.async_active_subscription(self.hass):\n            webhook_url = await cloud.async_create_cloudhook(self.hass, webhook_id)\n            cloudhook = True",
      "                    \"Core config step already done\", HTTPStatus.FORBIDDEN\n                )\n\n            await self._async_mark_done(hass)\n\n            # Integrations to set up when finishing onboarding\n            onboard_integrations = [\n                \"google_translate\",\n                \"met\",\n                \"radio_browser\",\n                \"shopping_list\",\n            ]\n\n            # pylint: disable-next=import-outside-toplevel\n            from homeassistant.components import hassio\n\n            if (\n                hassio.is_hassio(hass)\n                and \"raspberrypi\" in hassio.get_core_info(hass)[\"machine\"]\n            ):\n                onboard_integrations.append(\"rpi_power\")\n\n            # Set up integrations after onboarding\n            await asyncio.gather(\n                *(\n                    hass.config_entries.flow.async_init(\n                        domain, context={\"source\": \"onboarding\"}\n                    )\n                    for domain in onboard_integrations\n                )\n            )\n\n            return self.json({})\n\n\nclass IntegrationOnboardingView(_BaseOnboardingView):\n    \"\"\"View to finish integration onboarding step.\"\"\"\n\n    url = \"/api/onboarding/integration\"\n    name = \"api:onboarding:integration\"\n    step = STEP_INTEGRATION\n\n    @RequestDataValidator(\n        vol.Schema({vol.Required(\"client_id\"): str, vol.Required(\"redirect_uri\"): str})\n    )\n    async def post(self, request, data):\n        \"\"\"Handle token creation.\"\"\"\n        hass = request.app[\"hass\"]\n        refresh_token_id = request[KEY_HASS_REFRESH_TOKEN_ID]\n\n        async with self._lock:\n            if self._async_is_done():\n                return self.json_message(\n                    \"Integration step already done\", HTTPStatus.FORBIDDEN\n                )\n\n            await self._async_mark_done(hass)\n\n            # Validate client ID and redirect uri\n            if not await indieauth.verify_redirect_uri(\n                request.app[\"hass\"], data[\"client_id\"], data[\"redirect_uri\"]\n            ):\n                return self.json_message(\n                    \"invalid client id or redirect uri\", HTTPStatus.BAD_REQUEST",
      "ATTR_UPDATED_AT = \"updated_at\"\n\n# how many days to snooze the reminder for\nATTR_REMINDER_DAYS = \"days\"\nRESET_REMINDER_SCHEMA = {\n    vol.Required(ATTR_REMINDER_DAYS): vol.All(\n        vol.Coerce(int), vol.Range(min=30, max=365)\n    )\n}\nSNOOZE_REMINDER_SCHEMA = {\n    vol.Required(ATTR_REMINDER_DAYS): vol.All(\n        vol.Coerce(int), vol.Range(min=10, max=120)\n    )\n}\n\n\nasync def async_setup_entry(\n    hass: HomeAssistant, entry: ConfigEntry, async_add_entities: AddEntitiesCallback\n) -> None:\n    \"\"\"Set up binary sensor entities for the binary sensors in the tub.\"\"\"\n\n    controller = hass.data[DOMAIN][entry.entry_id][SMARTTUB_CONTROLLER]\n\n    entities: list[BinarySensorEntity] = []\n    for spa in controller.spas:\n        entities.append(SmartTubOnline(controller.coordinator, spa))\n        entities.append(SmartTubError(controller.coordinator, spa))\n        entities.extend(\n            SmartTubReminder(controller.coordinator, spa, reminder)\n            for reminder in controller.coordinator.data[spa.id][ATTR_REMINDERS].values()\n        )\n\n    async_add_entities(entities)\n\n    platform = entity_platform.async_get_current_platform()\n\n    platform.async_register_entity_service(\n        \"snooze_reminder\",\n        SNOOZE_REMINDER_SCHEMA,\n        \"async_snooze\",\n    )\n    platform.async_register_entity_service(\n        \"reset_reminder\",\n        RESET_REMINDER_SCHEMA,\n        \"async_reset\",\n    )\n\n\nclass SmartTubOnline(SmartTubSensorBase, BinarySensorEntity):\n    \"\"\"A binary sensor indicating whether the spa is currently online (connected to the cloud).\"\"\"\n\n    _attr_device_class = BinarySensorDeviceClass.CONNECTIVITY\n\n    def __init__(self, coordinator, spa):\n        \"\"\"Initialize the entity.\"\"\"\n        super().__init__(coordinator, spa, \"Online\", \"online\")\n\n    @property\n    def entity_registry_enabled_default(self) -> bool:\n        \"\"\"Return if the entity should be enabled when first added to the entity registry.\n\n        This seems to be very noisy and not generally useful, so disable by default.\n        \"\"\"\n        return False",
      "    @abstractmethod\n    def _process_data(self) -> None:\n        \"\"\"Update and validate the data from the thermostat.\"\"\"\n\n    @callback\n    def _handle_coordinator_update(self) -> None:\n        self._process_data()\n        return super()._handle_coordinator_update()\n",
      "        self._domain, self._object_id = split_entity_id(self.entity_id)\n        state = self.hass.states.get(self.entity_id)\n\n        self.activate_only = self.is_activate(self.hass.states.get(self.entity_id))\n\n        serv_switch = self.add_preload_service(SERV_SWITCH)\n        self.char_on = serv_switch.configure_char(\n            CHAR_ON, value=False, setter_callback=self.set_state\n        )\n        # Set the state so it is in sync on initial\n        # GET to avoid an event storm after homekit startup\n        self.async_update_state(state)\n\n    def is_activate(self, state):\n        \"\"\"Check if entity is activate only.\"\"\"\n        return self._domain in ACTIVATE_ONLY_SWITCH_DOMAINS\n\n    def reset_switch(self, *args):\n        \"\"\"Reset switch to emulate activate click.\"\"\"\n        _LOGGER.debug(\"%s: Reset switch to off\", self.entity_id)\n        self.char_on.set_value(False)\n\n    def set_state(self, value):\n        \"\"\"Move switch state to value if call came from HomeKit.\"\"\"\n        _LOGGER.debug(\"%s: Set switch state to %s\", self.entity_id, value)\n        if self.activate_only and not value:\n            _LOGGER.debug(\"%s: Ignoring turn_off call\", self.entity_id)\n            return\n\n        params = {ATTR_ENTITY_ID: self.entity_id}\n        if self._domain == \"script\":\n            service = self._object_id\n            params = {}\n        elif self._domain == button.DOMAIN:\n            service = button.SERVICE_PRESS\n        elif self._domain == input_button.DOMAIN:\n            service = input_button.SERVICE_PRESS\n        else:\n            service = SERVICE_TURN_ON if value else SERVICE_TURN_OFF\n\n        self.async_call_service(self._domain, service, params)\n\n        if self.activate_only:\n            async_call_later(self.hass, ACTIVATE_ONLY_RESET_SECONDS, self.reset_switch)\n\n    @callback\n    def async_update_state(self, new_state):\n        \"\"\"Update switch state after state changed.\"\"\"\n        self.activate_only = self.is_activate(new_state)\n        if self.activate_only:\n            _LOGGER.debug(\n                \"%s: Ignore state change, entity is activate only\", self.entity_id\n            )\n            return\n\n        current_state = new_state.state == STATE_ON\n        _LOGGER.debug(\"%s: Set current state to %s\", self.entity_id, current_state)\n        self.char_on.set_value(current_state)\n\n\n@TYPES.register(\"Vacuum\")\nclass Vacuum(Switch):\n    \"\"\"Generate a Switch accessory.\"\"\"\n",
      "\n        if preset_mode == PRESET_MANUAL:\n            await self.executor.async_execute_command(\n                OverkizCommand.SET_VENTILATION_CONFIGURATION_MODE,\n                OverkizCommandParam.STANDARD,\n            )\n            await self._set_ventilation_mode(prog=OverkizCommandParam.OFF)\n\n        await self.executor.async_execute_command(\n            OverkizCommand.REFRESH_VENTILATION_STATE,\n        )\n        await self.executor.async_execute_command(\n            OverkizCommand.REFRESH_VENTILATION_CONFIGURATION_MODE,\n        )\n\n    @property\n    def fan_mode(self) -> str | None:\n        \"\"\"Return the fan setting.\"\"\"\n        ventilation_mode = cast(\n            dict, self.executor.select_state(OverkizState.IO_VENTILATION_MODE)\n        )\n        cooling = ventilation_mode.get(OverkizCommandParam.COOLING)\n\n        if cooling == OverkizCommandParam.ON:\n            return FAN_BYPASS\n\n        return OVERKIZ_TO_FAN_MODES[\n            cast(str, self.executor.select_state(OverkizState.IO_AIR_DEMAND_MODE))\n        ]\n\n    async def async_set_fan_mode(self, fan_mode: str) -> None:\n        \"\"\"Set new target fan mode.\"\"\"\n        if fan_mode == FAN_BYPASS:\n            await self.executor.async_execute_command(\n                OverkizCommand.SET_AIR_DEMAND_MODE, OverkizCommandParam.AUTO\n            )\n            await self._set_ventilation_mode(cooling=OverkizCommandParam.ON)\n        else:\n            await self._set_ventilation_mode(cooling=OverkizCommandParam.OFF)\n            await self.executor.async_execute_command(\n                OverkizCommand.SET_AIR_DEMAND_MODE, FAN_MODES_TO_OVERKIZ[fan_mode]\n            )\n\n        await self.executor.async_execute_command(\n            OverkizCommand.REFRESH_VENTILATION_STATE,\n        )\n\n    async def _set_ventilation_mode(\n        self,\n        cooling: str | None = None,\n        prog: str | None = None,\n    ) -> None:\n        \"\"\"Execute ventilation mode command with all parameters.\"\"\"\n        ventilation_mode = cast(\n            dict, self.executor.select_state(OverkizState.IO_VENTILATION_MODE)\n        )\n\n        if cooling:\n            ventilation_mode[OverkizCommandParam.COOLING] = cooling\n\n        if prog:\n            ventilation_mode[OverkizCommandParam.PROG] = prog\n\n        await self.executor.async_execute_command(",
      "        if \"application token is invalid\" in str(err):\n            errors[CONF_API_KEY] = \"invalid_api_key\"\n        elif \"user key is invalid\" in str(err):\n            errors[CONF_USER_KEY] = \"invalid_user_key\"\n        else:\n            errors[\"base\"] = \"cannot_connect\"\n    return errors\n\n\nclass PushBulletConfigFlow(config_entries.ConfigFlow, domain=DOMAIN):\n    \"\"\"Handle a config flow for pushover integration.\"\"\"\n\n    _reauth_entry: config_entries.ConfigEntry | None\n\n    async def async_step_reauth(self, entry_data: Mapping[str, Any]) -> FlowResult:\n        \"\"\"Perform reauth upon an API authentication error.\"\"\"\n        self._reauth_entry = self.hass.config_entries.async_get_entry(\n            self.context[\"entry_id\"]\n        )\n        return await self.async_step_reauth_confirm()\n\n    async def async_step_reauth_confirm(\n        self, user_input: dict[str, str] | None = None\n    ) -> FlowResult:\n        \"\"\"Confirm reauth dialog.\"\"\"\n        errors = {}\n        if user_input is not None and self._reauth_entry:\n            user_input = {**self._reauth_entry.data, **user_input}\n            self._async_abort_entries_match(\n                {\n                    CONF_USER_KEY: user_input[CONF_USER_KEY],\n                    CONF_API_KEY: user_input[CONF_API_KEY],\n                }\n            )\n            errors = await validate_input(self.hass, user_input)\n            if not errors:\n                self.hass.config_entries.async_update_entry(\n                    self._reauth_entry, data=user_input\n                )\n                await self.hass.config_entries.async_reload(self._reauth_entry.entry_id)\n                return self.async_abort(reason=\"reauth_successful\")\n\n        return self.async_show_form(\n            step_id=\"reauth_confirm\",\n            data_schema=vol.Schema(\n                {\n                    vol.Required(CONF_API_KEY): str,\n                }\n            ),\n            errors=errors,\n        )\n\n    async def async_step_user(\n        self, user_input: dict[str, Any] | None = None\n    ) -> FlowResult:\n        \"\"\"Handle the initial step.\"\"\"\n        errors = {}\n\n        if user_input is not None:\n            self._async_abort_entries_match(\n                {\n                    CONF_USER_KEY: user_input[CONF_USER_KEY],\n                    CONF_API_KEY: user_input[CONF_API_KEY],\n                }",
      "\"\"\"Config flow for Traccar.\"\"\"\nfrom homeassistant.helpers import config_entry_flow\n\nfrom .const import DOMAIN\n\nconfig_entry_flow.register_webhook_flow(\n    DOMAIN,\n    \"Traccar Webhook\",\n    {\"docs_url\": \"https://www.home-assistant.io/integrations/traccar/\"},\n)\n"
    ]
  },
  {
    "id": "hashicorp/terraform",
    "org": "hashicorp",
    "avatarURL": "https://avatars.githubusercontent.com/u/761456?v=4",
    "name": "hashicorp/terraform",
    "url": "https://github.com/hashicorp/terraform",
    "lang": "Go",
    "desc": "Terraform enables you to safely and predictably create, change, and improve infrastructure.",
    "star_num": 38740,
    "fork_num": 9015,
    "snippets": [
      "}\n\nfunc (me *LogRoundTripper) RoundTrip(request *http.Request) (response *http.Response, errRet error) {\n\n\tvar inBytes, outBytes []byte\n\n\tvar start = time.Now()\n\n\tdefer func() { me.log(inBytes, outBytes, errRet, start) }()\n\n\tbodyReader, errRet := request.GetBody()\n\tif errRet != nil {\n\t\treturn\n\t}\n\tvar headName = \"X-TC-Action\"\n\n\tif envReqClient := os.Getenv(REQUEST_CLIENT); envReqClient != \"\" {\n\t\tReqClient = envReqClient\n\t}\n\n\trequest.Header.Set(\"X-TC-RequestClient\", ReqClient)\n\tinBytes = []byte(fmt.Sprintf(\"%s, request: \", request.Header[headName]))\n\trequestBody, errRet := ioutil.ReadAll(bodyReader)\n\tif errRet != nil {\n\t\treturn\n\t}\n\tinBytes = append(inBytes, requestBody...)\n\n\theadName = \"X-TC-Region\"\n\tappendMessage := []byte(fmt.Sprintf(\n\t\t\", (host %+v, region:%+v)\",\n\t\trequest.Header[\"Host\"],\n\t\trequest.Header[headName],\n\t))\n\n\tinBytes = append(inBytes, appendMessage...)\n\n\tresponse, errRet = http.DefaultTransport.RoundTrip(request)\n\tif errRet != nil {\n\t\treturn\n\t}\n\toutBytes, errRet = ioutil.ReadAll(response.Body)\n\tif errRet != nil {\n\t\treturn\n\t}\n\tresponse.Body = ioutil.NopCloser(bytes.NewBuffer(outBytes))\n\treturn\n}\n\nfunc (me *LogRoundTripper) log(in []byte, out []byte, err error, start time.Time) {\n\tvar buf bytes.Buffer\n\tbuf.WriteString(\"######\")\n\ttag := \"[DEBUG]\"\n\tif err != nil {\n\t\ttag = \"[CRITICAL]\"\n\t}\n\tbuf.WriteString(tag)\n\tif len(in) > 0 {\n\t\tbuf.WriteString(\"tencentcloud-sdk-go: \")\n\t\tbuf.Write(in)\n\t}\n\tif len(out) > 0 {\n\t\tbuf.WriteString(\"; response:\")\n\t\terr := json.Compact(&buf, out)",
      "\tsort.Strings(names) // Stable result for tests\n\n\tret := make([]addrs.Provider, len(names))\n\tfor i, name := range names {\n\t\tret[i] = m[name]\n\t}\n\n\treturn ret\n}\n",
      "\tif r.Actions.IsCancelable {\n\t\t// Only ask if the remote operation should be canceled\n\t\t// if the auto approve flag is not set.\n\t\tif !op.AutoApprove {\n\t\t\tv, err := op.UIIn.Input(cancelCtx, &terraform.InputOpts{\n\t\t\t\tId:          \"cancel\",\n\t\t\t\tQuery:       \"\\nDo you want to cancel the remote operation?\",\n\t\t\t\tDescription: \"Only 'yes' will be accepted to cancel.\",\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn generalError(\"Failed asking to cancel\", err)\n\t\t\t}\n\t\t\tif v != \"yes\" {\n\t\t\t\tif b.CLI != nil {\n\t\t\t\t\tb.CLI.Output(b.Colorize().Color(strings.TrimSpace(operationNotCanceled)))\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t}\n\t\t} else {\n\t\t\tif b.CLI != nil {\n\t\t\t\t// Insert a blank line to separate the ouputs.\n\t\t\t\tb.CLI.Output(\"\")\n\t\t\t}\n\t\t}\n\n\t\t// Try to cancel the remote operation.\n\t\terr := b.client.Runs.Cancel(cancelCtx, r.ID, tfe.RunCancelOptions{})\n\t\tif err != nil {\n\t\t\treturn generalError(\"Failed to cancel run\", err)\n\t\t}\n\t\tif b.CLI != nil {\n\t\t\tb.CLI.Output(b.Colorize().Color(strings.TrimSpace(operationCanceled)))\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// IgnoreVersionConflict allows commands to disable the fall-back check that\n// the local Terraform version matches the remote workspace's configured\n// Terraform version. This should be called by commands where this check is\n// unnecessary, such as those performing remote operations, or read-only\n// operations. It will also be called if the user uses a command-line flag to\n// override this check.\nfunc (b *Cloud) IgnoreVersionConflict() {\n\tb.ignoreVersionConflict = true\n}\n\n// VerifyWorkspaceTerraformVersion compares the local Terraform version against\n// the workspace's configured Terraform version. If they are compatible, this\n// means that there are no state compatibility concerns, so it returns no\n// diagnostics.\n//\n// If the versions aren't compatible, it returns an error (or, if\n// b.ignoreVersionConflict is set, a warning).\nfunc (b *Cloud) VerifyWorkspaceTerraformVersion(workspaceName string) tfdiags.Diagnostics {\n\tvar diags tfdiags.Diagnostics\n\n\tworkspace, err := b.getRemoteWorkspace(context.Background(), workspaceName)\n\tif err != nil {\n\t\t// If the workspace doesn't exist, there can be no compatibility\n\t\t// problem, so we can return. This is most likely to happen when\n\t\t// migrating state from a local backend to a new workspace.\n\t\tif err == tfe.ErrResourceNotFound {",
      "\t\tlog.Printf(\"[ERROR] UpgradeResourceState: stripRemovedStateAttributes: %s\", err)\n\t\treturn state\n\t}\n\n\t// if no changes were made, we return the original state to ensure nothing\n\t// was altered in the marshaling process.\n\tif !removeRemovedAttrs(jsonMap, ty) {\n\t\treturn state\n\t}\n\n\tjs, err := json.Marshal(jsonMap)\n\tif err != nil {\n\t\t// if the json map was somehow mangled enough to not marhsal, something\n\t\t// went horribly wrong\n\t\tpanic(err)\n\t}\n\n\treturn js\n}\n\n// strip out the actual missing attributes, and return a bool indicating if any\n// changes were made.\nfunc removeRemovedAttrs(v interface{}, ty cty.Type) bool {\n\tmodified := false\n\t// we're only concerned with finding maps that correspond to object\n\t// attributes\n\tswitch v := v.(type) {\n\tcase []interface{}:\n\t\tswitch {\n\t\t// If these aren't blocks the next call will be a noop\n\t\tcase ty.IsListType() || ty.IsSetType():\n\t\t\teTy := ty.ElementType()\n\t\t\tfor _, eV := range v {\n\t\t\t\tmodified = removeRemovedAttrs(eV, eTy) || modified\n\t\t\t}\n\t\t}\n\t\treturn modified\n\tcase map[string]interface{}:\n\t\tswitch {\n\t\tcase ty.IsMapType():\n\t\t\t// map blocks aren't yet supported, but handle this just in case\n\t\t\teTy := ty.ElementType()\n\t\t\tfor _, eV := range v {\n\t\t\t\tmodified = removeRemovedAttrs(eV, eTy) || modified\n\t\t\t}\n\t\t\treturn modified\n\n\t\tcase ty == cty.DynamicPseudoType:\n\t\t\tlog.Printf(\"[DEBUG] UpgradeResourceState: ignoring dynamic block: %#v\\n\", v)\n\t\t\treturn false\n\n\t\tcase ty.IsObjectType():\n\t\t\tattrTypes := ty.AttributeTypes()\n\t\t\tfor attr, attrV := range v {\n\t\t\t\tattrTy, ok := attrTypes[attr]\n\t\t\t\tif !ok {\n\t\t\t\t\tlog.Printf(\"[DEBUG] UpgradeResourceState: attribute %q no longer present in schema\", attr)\n\t\t\t\t\tdelete(v, attr)\n\t\t\t\t\tmodified = true\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tmodified = removeRemovedAttrs(attrV, attrTy) || modified\n\t\t\t}",
      "\t\"github.com/hashicorp/terraform/internal/addrs\"\n\t\"github.com/hashicorp/terraform/internal/states\"\n)\n\nfunc TestUpdateStateHook(t *testing.T) {\n\tmockHook := new(MockHook)\n\n\tstate := states.NewState()\n\tstate.Module(addrs.RootModuleInstance).SetLocalValue(\"foo\", cty.StringVal(\"hello\"))\n\n\tctx := new(MockEvalContext)\n\tctx.HookHook = mockHook\n\tctx.StateState = state.SyncWrapper()\n\n\tif err := updateStateHook(ctx); err != nil {\n\t\tt.Fatalf(\"err: %s\", err)\n\t}\n\n\tif !mockHook.PostStateUpdateCalled {\n\t\tt.Fatal(\"should call PostStateUpdate\")\n\t}\n\tif mockHook.PostStateUpdateState.LocalValue(addrs.LocalValue{Name: \"foo\"}.Absolute(addrs.RootModuleInstance)) != cty.StringVal(\"hello\") {\n\t\tt.Fatalf(\"wrong state passed to hook: %s\", spew.Sdump(mockHook.PostStateUpdateState))\n\t}\n}\n",
      "\n// Refresh represents the command-line arguments for the apply command.\ntype Refresh struct {\n\t// State, Operation, and Vars are the common extended flags\n\tState     *State\n\tOperation *Operation\n\tVars      *Vars\n\n\t// InputEnabled is used to disable interactive input for unspecified\n\t// variable and backend config values. Default is true.\n\tInputEnabled bool\n\n\t// ViewType specifies which output format to use\n\tViewType ViewType\n}\n\n// ParseRefresh processes CLI arguments, returning a Refresh value and errors.\n// If errors are encountered, a Refresh value is still returned representing\n// the best effort interpretation of the arguments.\nfunc ParseRefresh(args []string) (*Refresh, tfdiags.Diagnostics) {\n\tvar diags tfdiags.Diagnostics\n\trefresh := &Refresh{\n\t\tState:     &State{},\n\t\tOperation: &Operation{},\n\t\tVars:      &Vars{},\n\t}\n\n\tcmdFlags := extendedFlagSet(\"refresh\", refresh.State, refresh.Operation, refresh.Vars)\n\tcmdFlags.BoolVar(&refresh.InputEnabled, \"input\", true, \"input\")\n\n\tvar json bool\n\tcmdFlags.BoolVar(&json, \"json\", false, \"json\")\n\n\tif err := cmdFlags.Parse(args); err != nil {\n\t\tdiags = diags.Append(tfdiags.Sourceless(\n\t\t\ttfdiags.Error,\n\t\t\t\"Failed to parse command-line flags\",\n\t\t\terr.Error(),\n\t\t))\n\t}\n\n\targs = cmdFlags.Args()\n\tif len(args) > 0 {\n\t\tdiags = diags.Append(tfdiags.Sourceless(\n\t\t\ttfdiags.Error,\n\t\t\t\"Too many command line arguments\",\n\t\t\t\"Expected at most one positional argument.\",\n\t\t))\n\t}\n\n\tdiags = diags.Append(refresh.Operation.Parse())\n\n\t// JSON view currently does not support input, so we disable it here\n\tif json {\n\t\trefresh.InputEnabled = false\n\t}\n\n\tswitch {\n\tcase json:\n\t\trefresh.ViewType = ViewJSON\n\tdefault:\n\t\trefresh.ViewType = ViewHuman\n\t}\n",
      "\t\t\t\"non self reference\",\n\t\t\trAddr,\n\t\t\thcltest.MockExprTraversalSrc(\"aws_instance.bar.id\"),\n\t\t\tfalse,\n\t\t},\n\n\t\t{\n\t\t\t\"self reference\",\n\t\t\trAddr,\n\t\t\thcltest.MockExprTraversalSrc(\"aws_instance.foo.id\"),\n\t\t\ttrue,\n\t\t},\n\n\t\t{\n\t\t\t\"self reference other index\",\n\t\t\trAddr,\n\t\t\thcltest.MockExprTraversalSrc(\"aws_instance.foo[4].id\"),\n\t\t\tfalse,\n\t\t},\n\n\t\t{\n\t\t\t\"self reference same index\",\n\t\t\trAddr.Instance(addrs.IntKey(4)),\n\t\t\thcltest.MockExprTraversalSrc(\"aws_instance.foo[4].id\"),\n\t\t\ttrue,\n\t\t},\n\n\t\t{\n\t\t\t\"self reference whole\",\n\t\t\trAddr.Instance(addrs.IntKey(4)),\n\t\t\thcltest.MockExprTraversalSrc(\"aws_instance.foo\"),\n\t\t\ttrue,\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(fmt.Sprintf(\"%d-%s\", i, test.Name), func(t *testing.T) {\n\t\t\tbody := hcltest.MockBody(&hcl.BodyContent{\n\t\t\t\tAttributes: hcl.Attributes{\n\t\t\t\t\t\"foo\": {\n\t\t\t\t\t\tName: \"foo\",\n\t\t\t\t\t\tExpr: test.Expr,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t})\n\n\t\t\tps := providers.ProviderSchema{\n\t\t\t\tResourceTypes: map[string]providers.Schema{\n\t\t\t\t\t\"aws_instance\": {\n\t\t\t\t\t\tBlock: &configschema.Block{\n\t\t\t\t\t\t\tAttributes: map[string]*configschema.Attribute{\n\t\t\t\t\t\t\t\t\"foo\": {\n\t\t\t\t\t\t\t\t\tType:     cty.String,\n\t\t\t\t\t\t\t\t\tRequired: true,\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t}\n\n\t\t\tdiags := validateSelfRef(test.Addr, body, ps)\n\t\t\tif diags.HasErrors() != test.Err {\n\t\t\t\tif test.Err {",
      "func (h uiModuleInstallHooks) Download(modulePath, packageAddr string, v *version.Version) {\n\tif v != nil {\n\t\th.Ui.Info(fmt.Sprintf(\"Downloading %s %s for %s...\", packageAddr, v, modulePath))\n\t} else {\n\t\th.Ui.Info(fmt.Sprintf(\"Downloading %s for %s...\", packageAddr, modulePath))\n\t}\n}\n\nfunc (h uiModuleInstallHooks) Install(modulePath string, v *version.Version, localDir string) {\n\tif h.ShowLocalPaths {\n\t\th.Ui.Info(fmt.Sprintf(\"- %s in %s\", modulePath, localDir))\n\t} else {\n\t\th.Ui.Info(fmt.Sprintf(\"- %s\", modulePath))\n\t}\n}\n",
      "\n\t\tif connInfo.BastionUser == \"\" {\n\t\t\tconnInfo.BastionUser = connInfo.User\n\t\t}\n\t\tif connInfo.BastionPassword == \"\" {\n\t\t\tconnInfo.BastionPassword = connInfo.Password\n\t\t}\n\t\tif connInfo.BastionPrivateKey == \"\" {\n\t\t\tconnInfo.BastionPrivateKey = connInfo.PrivateKey\n\t\t}\n\t\tif connInfo.BastionCertificate == \"\" {\n\t\t\tconnInfo.BastionCertificate = connInfo.Certificate\n\t\t}\n\t\tif connInfo.BastionPort == 0 {\n\t\t\tconnInfo.BastionPort = connInfo.Port\n\t\t}\n\t}\n\n\treturn connInfo, nil\n}\n\n// safeDuration returns either the parsed duration or a default value\nfunc safeDuration(dur string, defaultDur time.Duration) time.Duration {\n\td, err := time.ParseDuration(dur)\n\tif err != nil {\n\t\tlog.Printf(\"Invalid duration '%s', using default of %s\", dur, defaultDur)\n\t\treturn defaultDur\n\t}\n\treturn d\n}\n\n// prepareSSHConfig is used to turn the *ConnectionInfo provided into a\n// usable *SSHConfig for client initialization.\nfunc prepareSSHConfig(connInfo *connectionInfo) (*sshConfig, error) {\n\tsshAgent, err := connectToAgent(connInfo)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\thost := fmt.Sprintf(\"%s:%d\", connInfo.Host, connInfo.Port)\n\n\tsshConf, err := buildSSHClientConfig(sshClientConfigOpts{\n\t\tuser:        connInfo.User,\n\t\thost:        host,\n\t\tprivateKey:  connInfo.PrivateKey,\n\t\tpassword:    connInfo.Password,\n\t\thostKey:     connInfo.HostKey,\n\t\tcertificate: connInfo.Certificate,\n\t\tsshAgent:    sshAgent,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar p *proxyInfo\n\n\tif connInfo.ProxyHost != \"\" {\n\t\tp = newProxyInfo(\n\t\t\tfmt.Sprintf(\"%s:%d\", connInfo.ProxyHost, connInfo.ProxyPort),\n\t\t\tconnInfo.ProxyScheme,\n\t\t\tconnInfo.ProxyUserName,\n\t\t\tconnInfo.ProxyUserPassword,\n\t\t)\n\t}",
      "\tinfo := &statemgr.LockInfo{}\n\tif err := json.Unmarshal(data, info); err != nil {\n\t\treturn nil, err\n\t}\n\n\tinfo.ID = checksum\n\n\treturn info, nil\n}\n\n// getObject get remote object\nfunc (c *remoteClient) getObject(cosFile string) (exists bool, data []byte, checksum string, err error) {\n\trsp, err := c.cosClient.Object.Get(c.cosContext, cosFile, nil)\n\tif rsp == nil {\n\t\tlog.Printf(\"[DEBUG] getObject %s: error: %v\", cosFile, err)\n\t\terr = fmt.Errorf(\"failed to open file at %v: %v\", cosFile, err)\n\t\treturn\n\t}\n\tdefer rsp.Body.Close()\n\n\tlog.Printf(\"[DEBUG] getObject %s: code: %d, error: %v\", cosFile, rsp.StatusCode, err)\n\tif err != nil {\n\t\tif rsp.StatusCode == 404 {\n\t\t\terr = nil\n\t\t} else {\n\t\t\terr = fmt.Errorf(\"failed to open file at %v: %v\", cosFile, err)\n\t\t}\n\t\treturn\n\t}\n\n\tchecksum = rsp.Header.Get(\"X-Cos-Meta-Md5\")\n\tlog.Printf(\"[DEBUG] getObject %s: checksum: %s\", cosFile, checksum)\n\tif len(checksum) != 32 {\n\t\terr = fmt.Errorf(\"failed to open file at %v: checksum %s invalid\", cosFile, checksum)\n\t\treturn\n\t}\n\n\texists = true\n\tdata, err = ioutil.ReadAll(rsp.Body)\n\tlog.Printf(\"[DEBUG] getObject %s: data length: %d\", cosFile, len(data))\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to open file at %v: %v\", cosFile, err)\n\t\treturn\n\t}\n\n\tcheck := fmt.Sprintf(\"%x\", md5.Sum(data))\n\tlog.Printf(\"[DEBUG] getObject %s: check: %s\", cosFile, check)\n\tif check != checksum {\n\t\terr = fmt.Errorf(\"failed to open file at %v: checksum mismatch, %s != %s\", cosFile, check, checksum)\n\t\treturn\n\t}\n\n\treturn\n}\n\n// putObject put object to remote\nfunc (c *remoteClient) putObject(cosFile string, data []byte) error {\n\topt := &cos.ObjectPutOptions{\n\t\tObjectPutHeaderOptions: &cos.ObjectPutHeaderOptions{\n\t\t\tXCosMetaXXX: &http.Header{\n\t\t\t\t\"X-Cos-Meta-Md5\": []string{fmt.Sprintf(\"%x\", md5.Sum(data))},\n\t\t\t},\n\t\t},\n\t\tACLHeaderOptions: &cos.ACLHeaderOptions{"
    ]
  },
  {
    "id": "jekyll/jekyll",
    "org": "jekyll",
    "avatarURL": "https://avatars.githubusercontent.com/u/3083652?v=4",
    "name": "jekyll/jekyll",
    "url": "https://github.com/jekyll/jekyll",
    "lang": "Ruby",
    "desc": "A simple, blog-aware static site generator.",
    "star_num": 47189,
    "fork_num": 10186,
    "snippets": [
      "      end\n\n      # Clear all caches\n      def clear\n        delete_cache_files\n        base_cache.each_value(&:clear)\n      end\n\n      # Compare the current config to the cached config\n      # If they are different, clear all caches\n      #\n      # Returns nothing.\n      def clear_if_config_changed(config)\n        config = config.inspect\n        cache = Jekyll::Cache.new \"Jekyll::Cache\"\n        return if cache.key?(\"config\") && cache[\"config\"] == config\n\n        clear\n        cache = Jekyll::Cache.new \"Jekyll::Cache\"\n        cache[\"config\"] = config\n        nil\n      end\n\n      private\n\n      # Delete all cached items from all caches\n      #\n      # Returns nothing.\n      def delete_cache_files\n        FileUtils.rm_rf(@cache_dir) if disk_cache_enabled\n      end\n    end\n\n    #\n\n    # Get an existing named cache, or create a new one if none exists\n    #\n    # name - name of the cache\n    #\n    # Returns nothing.\n    def initialize(name)\n      @cache = Jekyll::Cache.base_cache[name] ||= {}\n      @name = name.gsub(%r![^\\w\\s-]!, \"-\")\n    end\n\n    # Clear this particular cache\n    def clear\n      delete_cache_files\n      @cache.clear\n    end\n\n    # Retrieve a cached item\n    # Raises if key does not exist in cache\n    #\n    # Returns cached value\n    def [](key)\n      return @cache[key] if @cache.key?(key)\n\n      path = path_to(hash(key))\n      if disk_cache_enabled? && File.file?(path) && File.readable?(path)\n        @cache[key] = load(path)\n      else\n        raise\n      end",
      "        # Check if the current entry is explicitly included and cache the result\n        included = included?(e)\n\n        # Reject current entry if it is excluded but not explicitly included as well.\n        next true if excluded?(e) && !included\n\n        # Reject current entry if it is a symlink.\n        next true if symlink?(e)\n\n        # Do not reject current entry if it is explicitly included.\n        next false if included\n\n        # Reject current entry if it is special or a backup file.\n        special?(e) || backup?(e)\n      end\n    end\n\n    def included?(entry)\n      glob_include?(site.include, entry) ||\n        glob_include?(site.include, File.basename(entry))\n    end\n\n    def special?(entry)\n      SPECIAL_LEADING_CHAR_REGEX.match?(entry) ||\n        SPECIAL_LEADING_CHAR_REGEX.match?(File.basename(entry))\n    end\n\n    def backup?(entry)\n      entry.end_with?(\"~\")\n    end\n\n    def excluded?(entry)\n      glob_include?(site.exclude - site.include, relative_to_source(entry)).tap do |excluded|\n        if excluded\n          Jekyll.logger.debug(\n            \"EntryFilter:\",\n            \"excluded #{relative_to_source(entry)}\"\n          )\n        end\n      end\n    end\n\n    # --\n    # Check if a file is a symlink.\n    # NOTE: This can be converted to allowing even in safe,\n    #   since we use Pathutil#in_path? now.\n    # --\n    def symlink?(entry)\n      site.safe && File.symlink?(entry) && symlink_outside_site_source?(entry)\n    end\n\n    # --\n    # Check if given path is outside of current site's configured source directory.\n    # --\n    def symlink_outside_site_source?(entry)\n      !File.realpath(entry).start_with?(site.in_source_dir)\n    end\n\n    # Check if an entry matches a specific pattern.\n    # Returns true if path matches against any glob pattern, else false.\n    def glob_include?(enumerator, entry)\n      entry_with_source = PathManager.join(site.source, entry)\n      entry_is_directory = File.directory?(entry_with_source)\n",
      "\nrequire \"helper\"\n\nclass TestURL < JekyllUnitTest\n  context \"The URL class\" do\n    should \"throw an exception if neither permalink or template is specified\" do\n      assert_raises ArgumentError do\n        URL.new(:placeholders => {})\n      end\n    end\n\n    should \"replace placeholders in templates\" do\n      assert_equal \"/foo/bar\", URL.new(\n        :template     => \"/:x/:y\",\n        :placeholders => { :x => \"foo\", :y => \"bar\" }\n      ).to_s\n    end\n\n    should \"handle multiple of the same key in the template\" do\n      assert_equal \"/foo/bar/foo/\", URL.new(\n        :template     => \"/:x/:y/:x/\",\n        :placeholders => { :x => \"foo\", :y => \"bar\" }\n      ).to_s\n    end\n\n    should \"use permalink if given\" do\n      assert_equal \"/le/perma/link\", URL.new(\n        :template     => \"/:x/:y\",\n        :placeholders => { :x => \"foo\", :y => \"bar\" },\n        :permalink    => \"/le/perma/link\"\n      ).to_s\n    end\n\n    should \"replace placeholders in permalinks\" do\n      assert_equal \"/foo/bar\", URL.new(\n        :template     => \"/baz\",\n        :permalink    => \"/:x/:y\",\n        :placeholders => { :x => \"foo\", :y => \"bar\" }\n      ).to_s\n    end\n\n    should \"handle multiple of the same key in the permalink\" do\n      assert_equal \"/foo/bar/foo/\", URL.new(\n        :template     => \"/baz\",\n        :permalink    => \"/:x/:y/:x/\",\n        :placeholders => { :x => \"foo\", :y => \"bar\" }\n      ).to_s\n    end\n\n    should \"handle nil values for keys in the template\" do\n      assert_equal \"/foo/bar/\", URL.new(\n        :template     => \"/:x/:y/:z/\",\n        :placeholders => { :x => \"foo\", :y => \"bar\", :z => nil }\n      ).to_s\n    end\n\n    should \"handle UrlDrop as a placeholder in addition to a hash\" do\n      _, matching_doc = fixture_document(\"_methods/escape-+ #%20[].md\")\n      assert_equal \"/methods/escape-+-20/escape-20.html\", URL.new(\n        :template     => \"/methods/:title/:name:output_ext\",\n        :placeholders => matching_doc.url_placeholders\n      ).to_s\n    end\n",
      "# frozen_string_literal: true\n\nrequire \"helper\"\n\nclass TestAnsi < JekyllUnitTest\n  context nil do\n    setup do\n      @subject = Jekyll::Utils::Ansi\n    end\n\n    Jekyll::Utils::Ansi::COLORS.each_key do |color|\n      should \"respond_to? #{color}\" do\n        assert_respond_to(@subject, color)\n      end\n    end\n\n    should \"be able to strip colors\" do\n      assert_equal \"hello\", @subject.strip(@subject.yellow(@subject.red(\"hello\")))\n    end\n\n    should \"be able to detect colors\" do\n      assert @subject.has?(@subject.yellow(\"hello\"))\n    end\n  end\nend\n",
      "    end\n\n    should \"not parse if the front matter is not at the start of the file\" do\n      ret = @convertible.read_yaml(@base, \"broken_front_matter1.erb\")\n      assert_equal({}, ret)\n    end\n\n    should \"not parse if there is syntax error in front matter\" do\n      name = \"broken_front_matter2.erb\"\n      out = capture_stderr do\n        ret = @convertible.read_yaml(@base, name)\n        assert_equal({}, ret)\n      end\n      assert_match(%r!YAML Exception!, out)\n      assert_match(%r!#{Regexp.escape(File.join(@base, name))}!, out)\n    end\n\n    should \"raise for broken front matter with `strict_front_matter` set\" do\n      name = \"broken_front_matter2.erb\"\n      @convertible.site.config[\"strict_front_matter\"] = true\n      assert_raises(Psych::SyntaxError) do\n        @convertible.read_yaml(@base, name)\n      end\n    end\n\n    should \"not allow ruby objects in YAML\" do\n      out = capture_stderr do\n        @convertible.read_yaml(@base, \"exploit_front_matter.erb\")\n      end\n      refute_match(%r!undefined class/module DoesNotExist!, out)\n    end\n\n    should \"not parse if there is encoding error in file\" do\n      name = \"broken_front_matter3.erb\"\n      out = capture_stderr do\n        ret = @convertible.read_yaml(@base, name, :encoding => \"utf-8\")\n        assert_equal({}, ret)\n      end\n      assert_match(%r!invalid byte sequence in UTF-8!, out)\n      assert_match(%r!#{Regexp.escape(File.join(@base, name))}!, out)\n    end\n\n    should \"parse the front matter but show an error if permalink is empty\" do\n      name = \"empty_permalink.erb\"\n      assert_raises(Errors::InvalidPermalinkError) do\n        @convertible.read_yaml(@base, name)\n      end\n    end\n\n    should \"parse the front matter correctly without permalink\" do\n      out = capture_stderr do\n        @convertible.read_yaml(@base, \"front_matter.erb\")\n      end\n      refute_match(%r!Invalid permalink!, out)\n    end\n\n    should \"not parse Liquid if disabled in front matter\" do\n      name = \"no_liquid.erb\"\n      @convertible.read_yaml(@base, name)\n      ret = @convertible.content.strip\n      assert_equal(\"{% raw %}{% endraw %}\", ret)\n    end\n  end\nend",
      "# frozen_string_literal: true\n\nrequire \"simplecov\"\n\nSimpleCov.profiles.define \"gem\" do\n  add_filter \"/test/\"\n  add_filter \"/features/\"\n  add_filter \"/autotest/\"\n\n  add_group \"Binaries\", \"/bin/\"\n  add_group \"Libraries\", \"/lib/\"\nend\n",
      "  end\n\n  def processed_documents\n    @post_reader.read_publishable(@dir, @magic_dir, @matcher)\n  end\nend\n",
      "  end\nend\n",
      "    then FileUtils.mkdir_p(path)\n  end\nend\n\n#\n\nGiven(%r!^I do not have a \"(.*)\" directory$!) do |path|\n  Paths.test_dir.join(path).directory?\nend\n\n#\n\nGiven(%r!^I have an? \"(.*)\" page(?: with (.*) \"(.*)\")? that contains \"(.*)\"$!) do |file, key, value, text|\n  File.write(file, <<~DATA)\n    ---\n    #{key || \"layout\"}: #{value || \"none\"}\n    ---\n\n    #{text}\n  DATA\nend\n\n#\n\nGiven(%r!^I have an? \"(.*)\" file that contains \"(.*)\"$!) do |file, text|\n  File.write(file, text)\nend\n\n#\n\nGiven(%r!^I have an? (.*) (layout|theme) that contains \"(.*)\"$!) do |name, type, text|\n  folder = type == \"layout\" ? \"_layouts\" : \"_theme\"\n\n  destination_file = Pathname.new(File.join(folder, \"#{name}.html\"))\n  FileUtils.mkdir_p(destination_file.parent) unless destination_file.parent.directory?\n  File.write(destination_file, text)\nend\n\n#\n\nGiven(%r!^I have an? \"(.*)\" file with content:$!) do |file, text|\n  File.write(file, text)\nend\n\n#\n\nGiven(%r!^I have an? \"(.*)\" page with content:$!) do |file, text|\n  File.write(file, <<~DATA)\n    ---\n    ---\n\n    #{text}\n  DATA\nend\n\n#\n\nGiven(%r!^I have an? (.*) directory$!) do |dir|\n  unless File.directory?(dir)\n    then FileUtils.mkdir_p(dir)\n  end\nend\n\n#",
      "        }).call(this);\n      JS\n    end\n\n    should \"write a JS file in place\" do\n      assert_exist @test_coffeescript_file\n    end\n\n    should \"produce JS\" do\n      assert_equal @js_output, File.read(@test_coffeescript_file)\n    end\n  end\nend\n"
    ]
  },
  {
    "id": "freeCodeCamp/freeCodeCamp",
    "org": "freeCodeCamp",
    "avatarURL": "https://avatars.githubusercontent.com/u/9892522?v=4",
    "name": "freeCodeCamp/freeCodeCamp",
    "url": "https://github.com/freeCodeCamp/freeCodeCamp",
    "lang": "JavaScript",
    "desc": "The https://www.freecodecamp.org open source codebase and curriculum. Learn to code for free.",
    "star_num": 373056,
    "fork_num": 33431,
    "snippets": [
      "import { csrfOptions } from './csurf.js';\n\nexport default function csrfErrorHandler() {\n  return function (err, req, res, next) {\n    if (err.code === 'EBADCSRFTOKEN' && req.csrfToken) {\n      // use the middleware to generate a token. The client sends this back via\n      // a header\n      res.cookie('csrf_token', req.csrfToken(), csrfOptions);\n    }\n    next(err);\n  };\n}\n",
      "  return { ...req, ...opts };\n};\n\nexport const mockRes = opts => {\n  const res = {};\n  res.status = jest.fn().mockReturnValue(res);\n  res.json = jest.fn().mockReturnValue(res);\n  res.redirect = jest.fn().mockReturnValue(res);\n  res.set = jest.fn().mockReturnValue(res);\n  res.clearCookie = jest.fn().mockReturnValue(res);\n  res.cookie = jest.fn().mockReturnValue(res);\n  return { ...res, ...opts };\n};\n\ndescribe('boot/settings', () => {\n  describe('updateMySocials', () => {\n    it('does not allow non-github domain in GitHub social', () => {\n      const req = mockReq({\n        user: {},\n        body: {\n          githubProfile: 'https://www.almost-github.com'\n        }\n      });\n      const res = mockRes();\n      const next = jest.fn();\n      updateMySocials(req, res, next);\n      expect(res.status).toHaveBeenCalledWith(403);\n    });\n\n    it('does not allow non-linkedin domain in LinkedIn social', () => {\n      const req = mockReq({\n        user: {},\n        body: {\n          linkedin: 'https://www.freecodecamp.org'\n        }\n      });\n      const res = mockRes();\n      const next = jest.fn();\n      updateMySocials(req, res, next);\n      expect(res.status).toHaveBeenCalledWith(403);\n    });\n\n    it('does not allow non-twitter domain in Twitter social', () => {\n      const req = mockReq({\n        user: {},\n        body: {\n          twitter: 'https://www.freecodecamp.org'\n        }\n      });\n      const res = mockRes();\n      const next = jest.fn();\n      updateMySocials(req, res, next);\n      expect(res.status).toHaveBeenCalledWith(403);\n    });\n\n    it('allows empty string in any social', () => {\n      const req = mockReq({\n        user: {\n          updateAttributes: (_, cb) => cb()\n        },\n        body: {\n          twitter: '',\n          linkedin: '',\n          githubProfile: '',",
      "var globalConfig = require('../common/config.global');\n\nmodule.exports = {\n  restApiRoot: globalConfig.restApi,\n  sessionSecret: process.env.SESSION_SECRET,\n\n  github: {\n    clientID: process.env.GITHUB_ID,\n    clientSecret: process.env.GITHUB_SECRET\n  }\n};\n",
      "    return 0;\n  });\n  replaceHeadComponents(headComponents);\n};\n",
      "  return send$(notifyUser).map(() => true);\n}\n\nfunction getUserIsCertMap(user) {\n  const {\n    isRespWebDesignCert = false,\n    isJsAlgoDataStructCert = false,\n    isFrontEndLibsCert = false,\n    is2018DataVisCert = false,\n    isApisMicroservicesCert = false,\n    isInfosecQaCert = false,\n    isQaCertV7 = false,\n    isInfosecCertV7 = false,\n    isFrontEndCert = false,\n    isBackEndCert = false,\n    isDataVisCert = false,\n    isFullStackCert = false,\n    isSciCompPyCertV7 = false,\n    isDataAnalysisPyCertV7 = false,\n    isMachineLearningPyCertV7 = false,\n    isRelationalDatabaseCertV8 = false,\n    isCollegeAlgebraPyCertV8 = false,\n    isFoundationalCSharpCertV8 = false\n  } = user;\n\n  return {\n    isRespWebDesignCert,\n    isJsAlgoDataStructCert,\n    isFrontEndLibsCert,\n    is2018DataVisCert,\n    isApisMicroservicesCert,\n    isInfosecQaCert,\n    isQaCertV7,\n    isInfosecCertV7,\n    isFrontEndCert,\n    isBackEndCert,\n    isDataVisCert,\n    isFullStackCert,\n    isSciCompPyCertV7,\n    isDataAnalysisPyCertV7,\n    isMachineLearningPyCertV7,\n    isRelationalDatabaseCertV8,\n    isCollegeAlgebraPyCertV8,\n    isFoundationalCSharpCertV8\n  };\n}\n\nfunction createVerifyCert(certTypeIds, app) {\n  const { Email } = app.models;\n  return function verifyCert(req, res, next) {\n    const {\n      body: { certSlug },\n      user\n    } = req;\n    log(certSlug);\n    let certType = certSlugTypeMap[certSlug];\n    log(certType);\n    return Observable.of(certTypeIds[certType])\n      .flatMap(challenge => {\n        const certName = certTypeTitleMap[certType];\n        if (user[certType]) {\n          return Observable.just({\n            type: 'info',\n            message: 'flash.already-claimed',",
      "import i18next from 'i18next';\nimport { ofType } from 'redux-observable';\nimport { mapTo, tap } from 'rxjs/operators';\n\nimport envData from '../../../../../config/env.json';\nimport { transformEditorLink } from '../utils';\nimport { actionTypes } from './action-types';\nimport { closeModal } from './actions';\nimport {\n  challengeFilesSelector,\n  challengeMetaSelector,\n  projectFormValuesSelector\n} from './selectors';\n\nconst { forumLocation } = envData;\n\nfunction filesToMarkdown(challengeFiles = []) {\n  const moreThanOneFile = challengeFiles?.length > 1;\n  return challengeFiles.reduce((fileString, challengeFile) => {\n    if (!challengeFile) {\n      return fileString;\n    }\n\n    const fileExtension = challengeFile.ext;\n    const fileName = challengeFile.name;\n    const fileType = fileExtension === 'js' ? 'javascript' : fileExtension;\n    let fileDescription;\n\n    if (!moreThanOneFile) {\n      fileDescription = '';\n    } else if (fileExtension === 'html') {\n      fileDescription = `<!-- file: ${fileName}.${fileExtension} -->\\n`;\n    } else {\n      fileDescription = `/* file: ${fileName}.${fileExtension} */\\n`;\n    }\n\n    return `${fileString}\\`\\`\\`${fileType}\\n${fileDescription}${challengeFile.contents}\\n\\`\\`\\`\\n\\n`;\n  }, '\\n');\n}\n\nexport function insertEditableRegions(challengeFiles = []) {\n  if (challengeFiles?.some(file => file.editableRegionBoundaries?.length > 0)) {\n    const editableRegionStrings = fileExtension => {\n      const startComment = fileExtension === 'html' ? '<!--' : '/*';\n      const endComment = fileExtension === 'html' ? '-->' : '*/';\n      return `\\n${startComment} User Editable Region ${endComment}\\n`;\n    };\n\n    const filesWithEditableRegions = challengeFiles.map(file => {\n      const { contents, editableRegionBoundaries, ext } = file;\n      if (editableRegionBoundaries.length > 0) {\n        const comment = editableRegionStrings(ext);\n        const [start, end] = editableRegionBoundaries;\n        const lines = contents.split('\\n');\n        lines.splice(start, 0, comment);\n        lines.splice(end, 0, comment);\n        return { ...file, contents: lines.join('\\n') };\n      }\n      return file;\n    });\n    return filesWithEditableRegions;\n  }\n  return challengeFiles;\n}",
      "    return res.redirect(origin);\n  };\n}\n\nexport function ifNoUserSend(sendThis) {\n  return function (req, res, next) {\n    if (req.user) {\n      return next();\n    }\n    return res.status(200).send(sendThis);\n  };\n}\n\nexport function ifNoUser401(req, res, next) {\n  if (req.user) {\n    return next();\n  }\n  return res.status(401).end();\n}\n\nexport function ifNotVerifiedRedirectToUpdateEmail(req, res, next) {\n  const { user } = req;\n  if (!user) {\n    return next();\n  }\n  if (!user.emailVerified) {\n    req.flash(\n      'danger',\n      dedent`\n        We do not have your verified email address on record,\n        please add it in the settings to continue with your request.\n      `\n    );\n    return res.redirect('/settings');\n  }\n  return next();\n}\n\nexport function ifUserRedirectTo(status) {\n  status = status === 301 ? 301 : 302;\n  return (req, res, next) => {\n    const { accessToken } = getAccessTokenFromRequest(req);\n    const { returnTo } = getRedirectParams(req);\n    if (req.user && accessToken) {\n      return res.status(status).redirect(returnTo);\n    }\n    if (req.user && !accessToken) {\n      // This request has an active auth session\n      // but there is no accessToken attached to the request\n      // perhaps the user cleared cookies?\n      // we need to remove the zombie auth session\n      removeCookies(req, res);\n      delete req.session.passport;\n    }\n    return next();\n  };\n}\n\nexport function ifNotMobileRedirect() {\n  return (req, res, next) => {\n    //\n    // Todo: Use the below check once we have done more research on usage\n    //\n    // const isMobile = /(iPhone|iPad|Android)/.test(req.headers['user-agent']);",
      "        query: payload\n      };\n    }\n  },\n  initialState\n);\n",
      "    id: 4,\n    emails: []\n  };\n  expect(ensureLowerCaseEmail(profile)).toBe('');\n});\n\ntest('returns empty string when emails is undefined', () => {\n  const profile = {\n    id: 5\n  };\n  expect(ensureLowerCaseEmail(profile)).toBe('');\n});\n\ntest('returns empty string when profile is undefined', () => {\n  let profile;\n  expect(ensureLowerCaseEmail(profile)).toBe('');\n});\n",
      "  const moreThanOneFile = challengeFiles?.length > 1;\n  return challengeFiles.reduce((fileString, challengeFile) => {\n    if (!challengeFile) {\n      return fileString;\n    }\n\n    const fileExtension = challengeFile.ext;\n    const fileName = challengeFile.name;\n    const fileType = fileExtension === 'js' ? 'javascript' : fileExtension;\n    let fileDescription;\n\n    if (!moreThanOneFile) {\n      fileDescription = '';\n    } else if (fileExtension === 'html') {\n      fileDescription = `<!-- file: ${fileName}.${fileExtension} -->\\n`;\n    } else {\n      fileDescription = `/* file: ${fileName}.${fileExtension} */\\n`;\n    }\n\n    return `${fileString}\\`\\`\\`${fileType}\\n${fileDescription}${challengeFile.contents}\\n\\`\\`\\`\\n\\n`;\n  }, '\\n');\n}\n\nexport function insertEditableRegions(challengeFiles = []) {\n  if (challengeFiles?.some(file => file.editableRegionBoundaries?.length > 0)) {\n    const editableRegionStrings = fileExtension => {\n      const startComment = fileExtension === 'html' ? '<!--' : '/*';\n      const endComment = fileExtension === 'html' ? '-->' : '*/';\n      return `\\n${startComment} User Editable Region ${endComment}\\n`;\n    };\n\n    const filesWithEditableRegions = challengeFiles.map(file => {\n      const { contents, editableRegionBoundaries, ext } = file;\n      if (editableRegionBoundaries.length > 0) {\n        const comment = editableRegionStrings(ext);\n        const [start, end] = editableRegionBoundaries;\n        const lines = contents.split('\\n');\n        lines.splice(start, 0, comment);\n        lines.splice(end, 0, comment);\n        return { ...file, contents: lines.join('\\n') };\n      }\n      return file;\n    });\n    return filesWithEditableRegions;\n  }\n  return challengeFiles;\n}\n\nfunction createQuestionEpic(action$, state$, { window }) {\n  return action$.pipe(\n    ofType(actionTypes.createQuestion),\n    tap(() => {\n      const state = state$.value;\n      let challengeFiles = challengeFilesSelector(state);\n      const {\n        title: challengeTitle,\n        superBlock,\n        block,\n        helpCategory\n      } = challengeMetaSelector(state);\n\n      challengeFiles = insertEditableRegions(challengeFiles);\n\n      const {"
    ]
  },
  {
    "id": "ansible/ansible",
    "org": "ansible",
    "avatarURL": "https://avatars.githubusercontent.com/u/1507452?v=4",
    "name": "ansible/ansible",
    "url": "https://github.com/ansible/ansible",
    "lang": "Python",
    "desc": "Ansible is a radically simple IT automation platform.",
    "star_num": 58469,
    "fork_num": 23643,
    "snippets": [
      "import os\n\nfrom . import (\n    SanityMultipleVersion,\n    SanityMessage,\n    SanityFailure,\n    SanitySuccess,\n    SanityTargets,\n    SanitySkipped,\n    TARGET_SANITY_ROOT,\n)\n\nfrom ...test import (\n    TestResult,\n)\n\nfrom ...target import (\n    TestTarget,\n)\n\nfrom ...util import (\n    SubprocessError,\n    display,\n    parse_to_list_of_dict,\n    is_subdir,\n)\n\nfrom ...util_common import (\n    run_command,\n)\n\nfrom ...config import (\n    SanityConfig,\n)\n\nfrom ...host_configs import (\n    PythonConfig,\n)\n\n\nclass CompileTest(SanityMultipleVersion):\n    \"\"\"Sanity test for proper python syntax.\"\"\"\n\n    def filter_targets(self, targets: list[TestTarget]) -> list[TestTarget]:\n        \"\"\"Return the given list of test targets, filtered to include only those relevant for the test.\"\"\"\n        return [target for target in targets if os.path.splitext(target.path)[1] == '.py' or is_subdir(target.path, 'bin')]\n\n    def test(self, args: SanityConfig, targets: SanityTargets, python: PythonConfig) -> TestResult:\n        if args.prime_venvs:\n            return SanitySkipped(self.name, python_version=python.version)\n\n        settings = self.load_processor(args, python.version)\n\n        paths = [target.path for target in targets.include]\n\n        cmd = [python.path, os.path.join(TARGET_SANITY_ROOT, 'compile', 'compile.py')]\n\n        data = '\\n'.join(paths)\n\n        display.info(data, verbosity=4)\n\n        try:\n            stdout, stderr = run_command(args, cmd, data=data, capture=True)\n            status = 0",
      "# Copyright (c) 2017 Ansible Project\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\n\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nDOCUMENTATION = r'''\n    name: host_list\n    version_added: \"2.4\"\n    short_description: Parses a 'host list' string\n    description:\n        - Parses a host list string as a comma separated values of hosts\n        - This plugin only applies to inventory strings that are not paths and contain a comma.\n'''\n\nEXAMPLES = r'''\n    # define 2 hosts in command line\n    # ansible -i '10.10.2.6, 10.10.2.4' -m ping all\n\n    # DNS resolvable names\n    # ansible -i 'host1.example.com, host2' -m user -a 'name=me state=absent' all\n\n    # just use localhost\n    # ansible-playbook -i 'localhost,' play.yml -c local\n'''\n\nimport os\n\nfrom ansible.errors import AnsibleError, AnsibleParserError\nfrom ansible.module_utils.common.text.converters import to_bytes, to_native, to_text\nfrom ansible.parsing.utils.addresses import parse_address\nfrom ansible.plugins.inventory import BaseInventoryPlugin\n\n\nclass InventoryModule(BaseInventoryPlugin):\n\n    NAME = 'host_list'\n\n    def verify_file(self, host_list):\n\n        valid = False\n        b_path = to_bytes(host_list, errors='surrogate_or_strict')\n        if not os.path.exists(b_path) and ',' in host_list:\n            valid = True\n        return valid\n\n    def parse(self, inventory, loader, host_list, cache=True):\n        ''' parses the inventory file '''\n\n        super(InventoryModule, self).parse(inventory, loader, host_list)\n\n        try:\n            for h in host_list.split(','):\n                h = h.strip()\n                if h:\n                    try:\n                        (host, port) = parse_address(h, allow_ranges=False)\n                    except AnsibleError as e:\n                        self.display.vvv(\"Unable to parse address from hostname, leaving unchanged: %s\" % to_text(e))\n                        host = h\n                        port = None\n\n                    if host not in self.inventory.hosts:\n                        self.inventory.add_host(host, group='ungrouped', port=port)",
      "        @classmethod\n        def fromtimestamp(cls, timestamp, tz=None):\n            if tz == UTC:\n                return UTC_DT.replace(tzinfo=tz)\n            return DT.replace(tzinfo=tz)\n\n    def _time():\n        return EPOCH_TS\n\n    monkeypatch.setattr(date_time.datetime, 'datetime', FakeNow)\n    monkeypatch.setattr(time, 'time', _time)\n\n\n@pytest.fixture\ndef fake_date_facts(fake_now):\n    \"\"\"Return a predictable instance of collected date_time facts.\"\"\"\n\n    collector = date_time.DateTimeFactCollector()\n    data = collector.collect()\n\n    return data\n\n\n@pytest.mark.parametrize(\n    ('fact_name', 'fact_value'),\n    (\n        ('year', '2020'),\n        ('month', '07'),\n        ('weekday', 'Saturday'),\n        ('weekday_number', '6'),\n        ('weeknumber', '27'),\n        ('day', '11'),\n        ('hour', '12'),\n        ('minute', '34'),\n        ('second', '56'),\n        ('date', '2020-07-11'),\n        ('time', '12:34:56'),\n        ('iso8601_basic', '20200711T123456124356'),\n        ('iso8601_basic_short', '20200711T123456'),\n        ('iso8601_micro', '2020-07-11T02:34:56.124356Z'),\n        ('iso8601', '2020-07-11T02:34:56Z'),\n    ),\n)\ndef test_date_time_facts(fake_date_facts, fact_name, fact_value):\n    assert fake_date_facts['date_time'][fact_name] == fact_value\n\n\ndef test_date_time_epoch(fake_date_facts):\n    \"\"\"Test that format of returned epoch value is correct\"\"\"\n\n    assert fake_date_facts['date_time']['epoch'].isdigit()\n    assert len(fake_date_facts['date_time']['epoch']) == 10  # This length will not change any time soon\n    assert fake_date_facts['date_time']['epoch_int'].isdigit()\n    assert len(fake_date_facts['date_time']['epoch_int']) == 10  # This length will not change any time soon\n\n\n@pytest.mark.parametrize('fact_name', ('tz', 'tz_dst'))\ndef test_date_time_tz(fake_date_facts, fact_name):\n    \"\"\"\n    Test the returned value for timezone consists of only uppercase\n    letters and is the expected length.\n    \"\"\"\n\n    assert fake_date_facts['date_time'][fact_name].isupper()",
      "        lines=expand_indexes(covered_path_lines, covered_targets, format_line),\n    )\n\n    if not args.explain:\n        write_json_file(args.output_file, report, encoder=SortedSetEncoder)\n",
      "\n    facts = inst.get_virtual_facts()\n    expected = {\n        'virtualization_role': 'guest',\n        'virtualization_tech_host': set(),\n        'virtualization_type': 'bhyve',\n        'virtualization_tech_guest': set(['bhyve']),\n    }\n\n    assert facts == expected\n",
      "from ...inventory import (\n    create_controller_inventory,\n    create_posix_inventory,\n)\n\n\ndef command_shell(args: ShellConfig) -> None:\n    \"\"\"Entry point for the `shell` command.\"\"\"\n    if args.raw and isinstance(args.targets[0], ControllerConfig):\n        raise ApplicationError('The --raw option has no effect on the controller.')\n\n    if not args.export and not args.cmd and not sys.stdin.isatty():\n        raise ApplicationError('Standard input must be a TTY to launch a shell.')\n\n    host_state = prepare_profiles(args, skip_setup=args.raw)  # shell\n\n    if args.delegate:\n        raise Delegate(host_state=host_state)\n\n    if args.raw and not isinstance(args.controller, OriginConfig):\n        display.warning('The --raw option will only be applied to the target.')\n\n    target_profile = t.cast(SshTargetHostProfile, host_state.target_profiles[0])\n\n    if isinstance(target_profile, ControllerProfile):\n        # run the shell locally unless a target was requested\n        con: Connection = LocalConnection(args)\n\n        if args.export:\n            display.info('Configuring controller inventory.', verbosity=1)\n            create_controller_inventory(args, args.export, host_state.controller_profile)\n    else:\n        # a target was requested, connect to it over SSH\n        con = target_profile.get_controller_target_connections()[0]\n\n        if args.export:\n            display.info('Configuring target inventory.', verbosity=1)\n            create_posix_inventory(args, args.export, host_state.target_profiles, True)\n\n    if args.export:\n        return\n\n    if args.cmd:\n        # Running a command is assumed to be non-interactive. Only a shell (no command) is interactive.\n        # If we want to support interactive commands in the future, we'll need an `--interactive` command line option.\n        # Command stderr output is allowed to mix with our own output, which is all sent to stderr.\n        con.run(args.cmd, capture=False, interactive=False, output_stream=OutputStream.ORIGINAL)\n        return\n\n    if isinstance(con, SshConnection) and args.raw:\n        cmd: list[str] = []\n    elif isinstance(target_profile, PosixProfile):\n        cmd = []\n\n        if args.raw:\n            shell = 'sh'  # shell required for non-ssh connection\n        else:\n            shell = 'bash'\n\n            python = target_profile.python  # make sure the python interpreter has been initialized before opening a shell\n            display.info(f'Target Python {python.version} is at: {python.path}')\n\n            optional_vars = (\n                'TERM',  # keep backspace working",
      "      - Command to run that returns a unique string indicating the last time the system was booted.\n      - Setting this to a command that has different output each time it is run will cause the task to fail.\n    type: str\n    default: 'cat /proc/sys/kernel/random/boot_id'\n    version_added: '2.10'\n\n  reboot_command:\n    description:\n      - Command to run that reboots the system, including any parameters passed to the command.\n      - Can be an absolute path to the command or just the command name. If an absolute path to the\n        command is not given, O(search_paths) on the target system will be searched to find the absolute path.\n      - This will cause O(pre_reboot_delay), O(post_reboot_delay), and O(msg) to be ignored.\n    type: str\n    default: '[determined based on target OS]'\n    version_added: '2.11'\nextends_documentation_fragment:\n  -  action_common_attributes\n  -  action_common_attributes.flow\nattributes:\n    action:\n        support: full\n    async:\n        support: none\n    bypass_host_loop:\n        support: none\n    check_mode:\n        support: full\n    diff_mode:\n        support: none\n    platform:\n        platforms: posix\nseealso:\n- module: ansible.windows.win_reboot\nauthor:\n    - Matt Davis (@nitzmahone)\n    - Sam Doran (@samdoran)\n'''\n\nEXAMPLES = r'''\n- name: Unconditionally reboot the machine with all defaults\n  ansible.builtin.reboot:\n\n- name: Reboot a slow machine that might have lots of updates to apply\n  ansible.builtin.reboot:\n    reboot_timeout: 3600\n\n- name: Reboot a machine with shutdown command in unusual place\n  ansible.builtin.reboot:\n    search_paths:\n     - '/lib/molly-guard'\n\n- name: Reboot machine using a custom reboot command\n  ansible.builtin.reboot:\n    reboot_command: launchctl reboot userspace\n    boot_time_command: uptime | cut -d ' ' -f 5\n\n- name: Reboot machine and send a message\n  ansible.builtin.reboot:\n    msg: \"Rebooting machine in 5 seconds\"\n\n'''\n\nRETURN = r'''\nrebooted:",
      "    type: bool\n    default: no\n'''\n",
      "\"\"\"Custom entry-point for pip that filters out unwanted logging and warnings.\"\"\"\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nimport logging\nimport os\nimport re\nimport runpy\nimport sys\nimport warnings\n\nBUILTIN_FILTERER_FILTER = logging.Filterer.filter\n\nLOGGING_MESSAGE_FILTER = re.compile(\"^(\"\n                                    \".*Running pip install with root privileges is generally not a good idea.*|\"  # custom Fedora patch [1]\n                                    \".*Running pip as the 'root' user can result in broken permissions .*|\"  # pip 21.1\n                                    \"DEPRECATION: Python 2.7 will reach the end of its life .*|\"  # pip 19.2.3\n                                    \"Ignoring .*: markers .* don't match your environment|\"\n                                    \"Looking in indexes: .*|\"  # pypi-test-container\n                                    \"Requirement already satisfied.*\"\n                                    \")$\")\n\n# [1] https://src.fedoraproject.org/rpms/python-pip/blob/f34/f/emit-a-warning-when-running-with-root-privileges.patch\n\nWARNING_MESSAGE_FILTERS = (\n    # DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained.\n    # pip 21.0 will drop support for Python 2.7 in January 2021.\n    # More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\n    'DEPRECATION: Python 2.7 reached the end of its life ',\n)\n\n\ndef custom_filterer_filter(self, record):\n    \"\"\"Globally omit logging of unwanted messages.\"\"\"\n    if LOGGING_MESSAGE_FILTER.search(record.getMessage()):\n        return 0\n\n    return BUILTIN_FILTERER_FILTER(self, record)\n\n\ndef main():\n    \"\"\"Main program entry point.\"\"\"\n    # Filtering logging output globally avoids having to intercept stdout/stderr.\n    # It also avoids problems with loss of color output and mixing up the order of stdout/stderr messages.\n    logging.Filterer.filter = custom_filterer_filter\n\n    for message_filter in WARNING_MESSAGE_FILTERS:\n        # Setting filterwarnings in code is necessary because of the following:\n        #   Python 2.7 cannot use the -W option to match warning text after a colon. This makes it impossible to match specific warning messages.\n        warnings.filterwarnings('ignore', message_filter)\n\n    get_pip = os.environ.get('GET_PIP')\n\n    try:\n        if get_pip:\n            directory, filename = os.path.split(get_pip)\n            module = os.path.splitext(filename)[0]\n            sys.path.insert(0, directory)\n            runpy.run_module(module, run_name='__main__', alter_sys=True)\n        else:\n            runpy.run_module('pip.__main__', run_name='__main__', alter_sys=True)\n    except ImportError as ex:\n        print('pip is unavailable: %s' % ex)\n        sys.exit(1)",
      "# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nimport os\n\nfrom ansible.module_utils.facts.virtual.base import Virtual, VirtualCollector\nfrom ansible.module_utils.facts.virtual.sysctl import VirtualSysctlDetectionMixin\n\n\nclass FreeBSDVirtual(Virtual, VirtualSysctlDetectionMixin):\n    \"\"\"\n    This is a FreeBSD-specific subclass of Virtual.  It defines\n    - virtualization_type\n    - virtualization_role\n    \"\"\"\n    platform = 'FreeBSD'\n\n    def get_virtual_facts(self):\n        virtual_facts = {}\n        host_tech = set()\n        guest_tech = set()\n\n        # Set empty values as default\n        virtual_facts['virtualization_type'] = ''\n        virtual_facts['virtualization_role'] = ''\n\n        if os.path.exists('/dev/xen/xenstore'):\n            guest_tech.add('xen')\n            virtual_facts['virtualization_type'] = 'xen'\n            virtual_facts['virtualization_role'] = 'guest'\n\n        kern_vm_guest = self.detect_virt_product('kern.vm_guest')\n        guest_tech.update(kern_vm_guest['virtualization_tech_guest'])\n        host_tech.update(kern_vm_guest['virtualization_tech_host'])\n\n        hw_hv_vendor = self.detect_virt_product('hw.hv_vendor')\n        guest_tech.update(hw_hv_vendor['virtualization_tech_guest'])\n        host_tech.update(hw_hv_vendor['virtualization_tech_host'])\n\n        sec_jail_jailed = self.detect_virt_product('security.jail.jailed')\n        guest_tech.update(sec_jail_jailed['virtualization_tech_guest'])\n        host_tech.update(sec_jail_jailed['virtualization_tech_host'])\n\n        if virtual_facts['virtualization_type'] == '':\n            sysctl = kern_vm_guest or hw_hv_vendor or sec_jail_jailed\n            # We call update here, then re-set virtualization_tech_host/guest\n            # later.\n            virtual_facts.update(sysctl)\n"
    ]
  },
  {
    "id": "elastic/elasticsearch",
    "org": "elastic",
    "avatarURL": "https://avatars.githubusercontent.com/u/6764390?v=4",
    "name": "elastic/elasticsearch",
    "url": "https://github.com/elastic/elasticsearch",
    "lang": "Java",
    "desc": "Open Source, Distributed, RESTful Search Engine.",
    "star_num": 64998,
    "fork_num": 23525,
    "snippets": [
      "    @Override\n    public void shardInitialized(ShardRouting unassignedShard, ShardRouting initializedShard) {\n        assert unassignedShard.unassigned() : \"expected unassigned shard \" + unassignedShard;\n        assert initializedShard.initializing() : \"expected initializing shard \" + initializedShard;\n        setChanged();\n    }\n\n    @Override\n    public void shardStarted(ShardRouting initializingShard, ShardRouting startedShard) {\n        assert initializingShard.initializing() : \"expected initializing shard \" + initializingShard;\n        assert startedShard.started() : \"expected started shard \" + startedShard;\n        setChanged();\n    }\n\n    @Override\n    public void relocationStarted(ShardRouting startedShard, ShardRouting targetRelocatingShard) {\n        assert startedShard.started() : \"expected started shard \" + startedShard;\n        assert targetRelocatingShard.isRelocationTarget() : \"expected relocation target shard \" + targetRelocatingShard;\n        setChanged();\n    }\n\n    @Override\n    public void unassignedInfoUpdated(ShardRouting unassignedShard, UnassignedInfo newUnassignedInfo) {\n        assert unassignedShard.unassigned() : \"expected unassigned shard \" + unassignedShard;\n        setChanged();\n    }\n\n    @Override\n    public void relocationFailureInfoUpdated(ShardRouting relocatedShard, RelocationFailureInfo relocationFailureInfo) {\n        assert relocatedShard.active() : \"expected active shard \" + relocatedShard;\n        setChanged();\n    }\n\n    @Override\n    public void shardFailed(ShardRouting failedShard, UnassignedInfo unassignedInfo) {\n        assert failedShard.assignedToNode() : \"expected assigned shard \" + failedShard;\n        setChanged();\n    }\n\n    @Override\n    public void relocationCompleted(ShardRouting removedRelocationSource) {\n        assert removedRelocationSource.relocating() : \"expected relocating shard \" + removedRelocationSource;\n        setChanged();\n    }\n\n    @Override\n    public void relocationSourceRemoved(ShardRouting removedReplicaRelocationSource) {\n        assert removedReplicaRelocationSource.primary() == false && removedReplicaRelocationSource.isRelocationTarget()\n            : \"expected replica relocation target shard \" + removedReplicaRelocationSource;\n        setChanged();\n    }\n\n    @Override\n    public void replicaPromoted(ShardRouting replicaShard) {\n        assert replicaShard.started() && replicaShard.primary() == false : \"expected started replica shard \" + replicaShard;\n        setChanged();\n    }\n\n    @Override\n    public void initializedReplicaReinitialized(ShardRouting oldReplica, ShardRouting reinitializedReplica) {\n        assert oldReplica.initializing() && oldReplica.primary() == false : \"expected initializing replica shard \" + oldReplica;\n        assert reinitializedReplica.initializing() && reinitializedReplica.primary() == false\n            : \"expected reinitialized replica shard \" + reinitializedReplica;\n        assert oldReplica.allocationId().getId().equals(reinitializedReplica.allocationId().getId()) == false",
      "\n    @Override\n    Boolean getResult(PyTorchResult result) {\n        return true;\n    }\n}\n",
      "    public CompositeKeyExtractor(String key, boolean isDateTimeBased) {\n        this.key = key;\n        this.isDateTimeBased = isDateTimeBased;\n    }\n\n    CompositeKeyExtractor(StreamInput in) throws IOException {\n        key = in.readString();\n        isDateTimeBased = in.readBoolean();\n    }\n\n    @Override\n    public void writeTo(StreamOutput out) throws IOException {\n        out.writeString(key);\n        out.writeBoolean(isDateTimeBased);\n    }\n\n    public String key() {\n        return key;\n    }\n\n    @Override\n    public String getWriteableName() {\n        return NAME;\n    }\n\n    @Override\n    public Object extract(Bucket bucket) {\n        // get the composite value\n        Object m = bucket.getKey();\n\n        if ((m instanceof Map) == false) {\n            throw new EqlIllegalArgumentException(\"Unexpected bucket returned: {}\", m);\n        }\n\n        Object object = ((Map<?, ?>) m).get(key);\n\n        if (isDateTimeBased) {\n            if (object == null) {\n                return object;\n            } else if (object instanceof Long) {\n                // object = DateUtils.asDateTimeWithNanos(((Long) object).longValue(), zoneId);\n                return object;\n            } else {\n                throw new EqlIllegalArgumentException(\"Invalid date key returned: {}\", object);\n            }\n        }\n\n        return object;\n    }\n\n    @Override\n    public int hashCode() {\n        return Objects.hash(key, isDateTimeBased);\n    }\n\n    @Override\n    public boolean equals(Object obj) {\n        if (this == obj) {\n            return true;\n        }\n\n        if (obj == null || getClass() != obj.getClass()) {\n            return false;\n        }",
      "        String simpleName,\n        MappedFieldType mappedFieldType,\n        MultiFields multiFields,\n        CopyTo copyTo,\n        Parser<T> parser,\n        OnScriptError onScriptError\n    ) {\n        super(simpleName, mappedFieldType, multiFields, copyTo, true, onScriptError);\n        this.ignoreMalformed = Explicit.EXPLICIT_FALSE;\n        this.ignoreZValue = Explicit.EXPLICIT_FALSE;\n        this.parser = parser;\n    }\n\n    @Override\n    @SuppressWarnings(\"unchecked\")\n    public AbstractGeometryFieldType<T> fieldType() {\n        return (AbstractGeometryFieldType<T>) mappedFieldType;\n    }\n\n    @Override\n    protected void parseCreateField(DocumentParserContext context) throws IOException {\n        throw new UnsupportedOperationException(\"Parsing is implemented in parse(), this method should NEVER be called\");\n    }\n\n    /**\n     * Build an index document using a parsed geometry\n     * @param context   the ParseContext holding the document\n     * @param geometry  the parsed geometry object\n     */\n    protected abstract void index(DocumentParserContext context, T geometry) throws IOException;\n\n    @Override\n    protected boolean supportsParsingObject() {\n        return true;\n    }\n\n    @Override\n    public final void parse(DocumentParserContext context) throws IOException {\n        if (hasScript) {\n            throw new DocumentParsingException(\n                context.parser().getTokenLocation(),\n                \"failed to parse field [\" + fieldType().name() + \"] of type + \" + contentType() + \"]\",\n                new IllegalArgumentException(\"Cannot index data directly into a field with a [script] parameter\")\n            );\n        }\n        parser.parse(context.parser(), v -> index(context, v), e -> {\n            if (ignoreMalformed()) {\n                context.addIgnoredField(fieldType().name());\n            } else {\n                throw new DocumentParsingException(\n                    context.parser().getTokenLocation(),\n                    \"failed to parse field [\" + fieldType().name() + \"] of type [\" + contentType() + \"]\",\n                    e\n                );\n            }\n        });\n    }\n\n    @Override\n    public boolean ignoreMalformed() {\n        return ignoreMalformed.value();\n    }\n\n    public boolean ignoreZValue() {",
      "}\n",
      "\nimport java.nio.file.Path;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Locale;\n\nimport javax.net.ssl.KeyManagerFactory;\nimport javax.net.ssl.TrustManagerFactory;\n\nimport static org.hamcrest.Matchers.containsInAnyOrder;\nimport static org.hamcrest.Matchers.containsString;\nimport static org.hamcrest.Matchers.equalTo;\nimport static org.hamcrest.Matchers.hasItem;\nimport static org.hamcrest.Matchers.instanceOf;\nimport static org.hamcrest.Matchers.is;\nimport static org.hamcrest.Matchers.notNullValue;\n\npublic class SslConfigurationLoaderTests extends ESTestCase {\n\n    private final Path certRoot = getDataPath(\"/certs/ca1/ca.crt\").getParent().getParent();\n\n    private Settings settings;\n    private MockSecureSettings secureSettings = new MockSecureSettings();\n    private SslConfigurationLoader loader = new SslConfigurationLoader(\"test.ssl.\") {\n        @Override\n        protected boolean hasSettings(String prefix) {\n            return true;\n        }\n\n        @Override\n        protected String getSettingAsString(String key) throws Exception {\n            return settings.get(key);\n        }\n\n        @Override\n        protected char[] getSecureSetting(String key) throws Exception {\n            final SecureString secStr = secureSettings.getString(key);\n            return secStr == null ? null : secStr.getChars();\n        }\n\n        @Override\n        protected List<String> getSettingAsList(String key) throws Exception {\n            return settings.getAsList(key);\n        }\n    };\n\n    /**\n     * A test for non-trust, non-key configurations.\n     * These are straight forward and can all be tested together\n     */\n    public void testBasicConfigurationOptions() {\n        final SslVerificationMode verificationMode = randomFrom(SslVerificationMode.values());\n        final SslClientAuthenticationMode clientAuth = randomFrom(SslClientAuthenticationMode.values());\n        final String[] ciphers = generateRandomStringArray(8, 12, false, false);\n        final String[] protocols = generateRandomStringArray(4, 5, false, false);\n        settings = Settings.builder()\n            .put(\"test.ssl.verification_mode\", verificationMode.name().toLowerCase(Locale.ROOT))\n            .put(\"test.ssl.client_authentication\", clientAuth.name().toLowerCase(Locale.ROOT))\n            .putList(\"test.ssl.cipher_suites\", ciphers)\n            .putList(\"test.ssl.supported_protocols\", protocols)\n            .build();\n        final SslConfiguration configuration = loader.load(certRoot);\n        assertThat(configuration.clientAuth(), is(clientAuth));\n        assertThat(configuration.verificationMode(), is(verificationMode));",
      " * This class also provides access to metadata information like checksums for committed files. A committed\n * file is a file that belongs to a segment written by a Lucene commit. Files that have not been committed\n * ie. created during a merge or a shard refresh / NRT reopen are not considered in the MetadataSnapshot.\n * <p>\n * Note: If you use a store its reference count should be increased before using it by calling #incRef and a\n * corresponding #decRef must be called in a try/finally block to release the store again ie.:\n * <pre>\n *      store.incRef();\n *      try {\n *        // use the store...\n *\n *      } finally {\n *          store.decRef();\n *      }\n * </pre>\n */\npublic class Store extends AbstractIndexShardComponent implements Closeable, RefCounted {\n\n    /**\n     * Legacy index setting, kept for 7.x BWC compatibility. This setting has no effect in 8.x. Do not use.\n     * TODO: Remove in 9.0\n     */\n    @Deprecated\n    public static final Setting<Boolean> FORCE_RAM_TERM_DICT = Setting.boolSetting(\n        \"index.force_memory_term_dictionary\",\n        false,\n        Property.IndexScope,\n        Property.IndexSettingDeprecatedInV7AndRemovedInV8\n    );\n\n    static final String CODEC = \"store\";\n    static final int CORRUPTED_MARKER_CODEC_VERSION = 2;\n    // public is for test purposes\n    public static final String CORRUPTED_MARKER_NAME_PREFIX = \"corrupted_\";\n    public static final Setting<TimeValue> INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING = Setting.timeSetting(\n        \"index.store.stats_refresh_interval\",\n        TimeValue.timeValueSeconds(10),\n        Property.IndexScope\n    );\n\n    /**\n     * Specific {@link IOContext} indicating that we will read only the Lucene file footer (containing the file checksum)\n     * See {@link MetadataSnapshot#checksumFromLuceneFile}.\n     */\n    public static final IOContext READONCE_CHECKSUM = new IOContext(IOContext.READONCE, true);\n\n    private final AtomicBoolean isClosed = new AtomicBoolean(false);\n    private final StoreDirectory directory;\n    private final ReentrantReadWriteLock metadataLock = new ReentrantReadWriteLock();\n    private final ShardLock shardLock;\n    private final OnClose onClose;\n\n    private final AbstractRefCounted refCounter = AbstractRefCounted.of(this::closeInternal); // close us once we are done\n\n    public Store(ShardId shardId, IndexSettings indexSettings, Directory directory, ShardLock shardLock) {\n        this(shardId, indexSettings, directory, shardLock, OnClose.EMPTY);\n    }\n\n    public Store(ShardId shardId, IndexSettings indexSettings, Directory directory, ShardLock shardLock, OnClose onClose) {\n        super(shardId, indexSettings);\n        this.directory = new StoreDirectory(\n            byteSizeDirectory(directory, indexSettings, logger),\n            Loggers.getLogger(\"index.store.deletes\", shardId)\n        );",
      "    private final DocCountProvider docCountProvider;\n\n    public NumericRateAggregator(\n        String name,\n        ValuesSourceConfig valuesSourceConfig,\n        Rounding.DateTimeUnit rateUnit,\n        RateMode rateMode,\n        AggregationContext context,\n        Aggregator parent,\n        Map<String, Object> metadata\n    ) throws IOException {\n        super(name, valuesSourceConfig, rateUnit, rateMode, context, parent, metadata);\n        docCountProvider = computeWithDocCount ? new DocCountProvider() : null;\n    }\n\n    @Override\n    public LeafBucketCollector getLeafCollector(AggregationExecutionContext aggCtx, final LeafBucketCollector sub) throws IOException {\n        final CompensatedSum kahanSummation = new CompensatedSum(0, 0);\n        if (computeWithDocCount) {\n            // No field or script has been set at the rate agg. So, rate will be computed based on the doc_counts.\n            // This implementation hard-wires the DocCountProvider and reads the _doc_count fields when available.\n            // A better approach would be to create a DOC_COUNT ValuesSource type and use that as valuesSource\n            // In that case the computeRateOnDocs variable and this branch of the if-statement are not required.\n            docCountProvider.setLeafReaderContext(aggCtx.getLeafReaderContext());\n            return new LeafBucketCollectorBase(sub, null) {\n                @Override\n                public void collect(int doc, long bucket) throws IOException {\n                    sums = bigArrays().grow(sums, bucket + 1);\n                    compensations = bigArrays().grow(compensations, bucket + 1);\n                    // Compute the sum of double values with Kahan summation algorithm which is more\n                    // accurate than naive summation.\n                    double sum = sums.get(bucket);\n                    double compensation = compensations.get(bucket);\n                    kahanSummation.reset(sum, compensation);\n\n                    final int docCount = docCountProvider.getDocCount(doc);\n                    kahanSummation.add(docCount);\n                    compensations.set(bucket, kahanSummation.delta());\n                    sums.set(bucket, kahanSummation.value());\n                }\n            };\n        } else {\n            final SortedNumericDoubleValues values = ((ValuesSource.Numeric) valuesSource).doubleValues(aggCtx.getLeafReaderContext());\n            return new LeafBucketCollectorBase(sub, values) {\n                @Override\n                public void collect(int doc, long bucket) throws IOException {\n                    sums = bigArrays().grow(sums, bucket + 1);\n                    compensations = bigArrays().grow(compensations, bucket + 1);\n\n                    if (values.advanceExact(doc)) {\n                        final int valuesCount = values.docValueCount();\n                        // Compute the sum of double values with Kahan summation algorithm which is more\n                        // accurate than naive summation.\n                        double sum = sums.get(bucket);\n                        double compensation = compensations.get(bucket);\n                        kahanSummation.reset(sum, compensation);\n                        switch (rateMode) {\n                            case SUM:\n                                for (int i = 0; i < valuesCount; i++) {\n                                    kahanSummation.add(values.nextValue());\n                                }\n                                break;\n                            case VALUE_COUNT:\n                                kahanSummation.add(valuesCount);",
      "}\n",
      "    }\n\n    public FieldCapabilitiesRequestBuilder setRuntimeFields(Map<String, Object> runtimeFieldSection) {\n        request().runtimeFields(runtimeFieldSection);\n        return this;\n    }\n}\n"
    ]
  },
  {
    "id": "mongodb/mongo",
    "org": "mongodb",
    "avatarURL": "https://avatars.githubusercontent.com/u/45120?v=4",
    "name": "mongodb/mongo",
    "url": "https://github.com/mongodb/mongo",
    "lang": "C++",
    "desc": "The MongoDB Database.",
    "star_num": 24341,
    "fork_num": 5610,
    "snippets": [
      "            return result;\n        }\n        // Generic format\n        switch (style) {\n        case GENERIC_LOCATION:\n            tzfmt->format(UTZFMT_STYLE_GENERIC_LOCATION, *this, date, result, &timeType);\n            break;\n        case LONG_GENERIC:\n            tzfmt->format(UTZFMT_STYLE_GENERIC_LONG, *this, date, result, &timeType);\n            break;\n        case SHORT_GENERIC:\n            tzfmt->format(UTZFMT_STYLE_GENERIC_SHORT, *this, date, result, &timeType);\n            break;\n        default:\n            U_ASSERT(FALSE);\n        }\n        // Generic format many use Localized GMT as the final fallback.\n        // When Localized GMT format is used, the result might not be\n        // appropriate for the requested daylight value.\n        if ((daylight && timeType == UTZFMT_TIME_TYPE_STANDARD) || (!daylight && timeType == UTZFMT_TIME_TYPE_DAYLIGHT)) {\n            offset = daylight ? getRawOffset() + getDSTSavings() : getRawOffset();\n            if (style == SHORT_GENERIC) {\n                tzfmt->formatOffsetShortLocalizedGMT(offset, result, status);\n            } else {\n                tzfmt->formatOffsetLocalizedGMT(offset, result, status);\n            }\n        }\n    } else if (style == LONG_GMT || style == SHORT_GMT) {\n        LocalPointer<TimeZoneFormat> tzfmt(TimeZoneFormat::createInstance(locale, status));\n        if (U_FAILURE(status)) {\n            result.remove();\n            return result;\n        }\n        offset = daylight && useDaylightTime() ? getRawOffset() + getDSTSavings() : getRawOffset();\n        switch (style) {\n        case LONG_GMT:\n            tzfmt->formatOffsetLocalizedGMT(offset, result, status);\n            break;\n        case SHORT_GMT:\n            tzfmt->formatOffsetISO8601Basic(offset, FALSE, FALSE, FALSE, result, status);\n            break;\n        default:\n            U_ASSERT(FALSE);\n        }\n\n    } else {\n        U_ASSERT(style == LONG || style == SHORT || style == SHORT_COMMONLY_USED);\n        UTimeZoneNameType nameType = UTZNM_UNKNOWN;\n        switch (style) {\n        case LONG:\n            nameType = daylight ? UTZNM_LONG_DAYLIGHT : UTZNM_LONG_STANDARD;\n            break;\n        case SHORT:\n        case SHORT_COMMONLY_USED:\n            nameType = daylight ? UTZNM_SHORT_DAYLIGHT : UTZNM_SHORT_STANDARD;\n            break;\n        default:\n            U_ASSERT(FALSE);\n        }\n        LocalPointer<TimeZoneNames> tznames(TimeZoneNames::createInstance(locale, status));\n        if (U_FAILURE(status)) {\n            result.remove();\n            return result;\n        }",
      " * If this set already any particular character, it has no effect on that character.\n * @param the source string\n * @return the modified set, for chaining\n */\nUnicodeSet& UnicodeSet::removeAll(const UnicodeString& s) {\n    UnicodeSet set;\n    set.addAll(s);\n    removeAll(set);\n    return *this;\n}\n\nUnicodeSet& UnicodeSet::removeAllStrings() {\n    if (!isFrozen() && hasStrings()) {\n        strings->removeAllElements();\n        releasePattern();\n    }\n    return *this;\n}\n\n\n/**\n * Makes a set from a multicharacter string. Thus \"ch\" => {\"ch\"}\n * <br><b>Warning: you cannot add an empty string (\"\") to a UnicodeSet.</b>\n * @param the source string\n * @return a newly created set containing the given string\n */\nUnicodeSet* U_EXPORT2 UnicodeSet::createFrom(const UnicodeString& s) {\n    UnicodeSet *set = new UnicodeSet();\n    if (set != NULL) { // Check for memory allocation error.\n        set->add(s);\n    }\n    return set;\n}\n\n\n/**\n * Makes a set from each of the characters in the string. Thus \"ch\" => {\"c\", \"h\"}\n * @param the source string\n * @return a newly created set containing the given characters\n */\nUnicodeSet* U_EXPORT2 UnicodeSet::createFromAll(const UnicodeString& s) {\n    UnicodeSet *set = new UnicodeSet();\n    if (set != NULL) { // Check for memory allocation error.\n        set->addAll(s);\n    }\n    return set;\n}\n\n/**\n * Retain only the elements in this set that are contained in the\n * specified range.  If <code>end > start</code> then an empty range is\n * retained, leaving the set empty.\n *\n * @param start first character, inclusive, of range to be retained\n * to this set.\n * @param end last character, inclusive, of range to be retained\n * to this set.\n */\nUnicodeSet& UnicodeSet::retain(UChar32 start, UChar32 end) {\n    if (pinCodePoint(start) <= pinCodePoint(end)) {\n        UChar32 range[3] = { start, end+1, UNICODESET_HIGH };\n        retain(range, 2, 0);\n    } else {\n        clear();",
      "  SET_DEFAULT(disableRecoverIns, false);\n\n  // Toggle whether eager scalar replacement is globally disabled.\n  SET_DEFAULT(disableScalarReplacement, false);\n\n  // Toggles whether CacheIR stubs are used.\n  SET_DEFAULT(disableCacheIR, false);\n\n  // Toggles whether sink code motion is globally disabled.\n  SET_DEFAULT(disableSink, true);\n\n  // Toggles whether we verify that we don't recompile with the same CacheIR.\n  SET_DEFAULT(disableBailoutLoopCheck, false);\n\n  // Whether the Baseline Interpreter is enabled.\n  SET_DEFAULT(baselineInterpreter, true);\n\n  // Whether the Baseline JIT is enabled.\n  SET_DEFAULT(baselineJit, true);\n\n  // Whether the IonMonkey JIT is enabled.\n  SET_DEFAULT(ion, true);\n\n  // Warp compile Async functions\n  SET_DEFAULT(warpAsync, true);\n\n  // Warp compile Generator functions\n  SET_DEFAULT(warpGenerator, true);\n\n  // Whether the IonMonkey and Baseline JITs are enabled for Trusted Principals.\n  // (Ignored if ion or baselineJit is set to true.)\n  SET_DEFAULT(jitForTrustedPrincipals, false);\n\n  // Whether the RegExp JIT is enabled.\n  SET_DEFAULT(nativeRegExp, true);\n\n  // Whether Warp should use ICs instead of transpiling Baseline CacheIR.\n  SET_DEFAULT(forceInlineCaches, false);\n\n  // Whether all ICs should be initialized as megamorphic ICs.\n  SET_DEFAULT(forceMegamorphicICs, false);\n\n  // Toggles whether large scripts are rejected.\n  SET_DEFAULT(limitScriptSize, true);\n\n  // Toggles whether functions may be entered at loop headers.\n  SET_DEFAULT(osr, true);\n\n  // Whether to enable extra code to perform dynamic validations.\n  SET_DEFAULT(runExtraChecks, false);\n\n  // How many invocations or loop iterations are needed before functions\n  // enter the Baseline Interpreter.\n  SET_DEFAULT(baselineInterpreterWarmUpThreshold, 10);\n\n  // How many invocations or loop iterations are needed before functions\n  // are compiled with the baseline compiler.\n  // Duplicated in all.js - ensure both match.\n  SET_DEFAULT(baselineJitWarmUpThreshold, 100);\n\n  // How many invocations or loop iterations are needed before functions\n  // are considered for trial inlining.\n  SET_DEFAULT(trialInliningWarmUpThreshold, 500);\n",
      "    return false;\n  }\n\n  // Step 2.\n  RootedObject handler(cx,\n                       RequireObjectArg(cx, \"`handler`\", callerName, args[1]));\n  if (!handler) {\n    return false;\n  }\n\n  // Steps 3-4, 6.\n  RootedValue priv(cx, ObjectValue(*target));\n  JSObject* proxy_ = NewProxyObject(cx, &ScriptedProxyHandler::singleton, priv,\n                                    TaggedProto::LazyProto);\n  if (!proxy_) {\n    return false;\n  }\n\n  // Step 7 (reordered).\n  Rooted<ProxyObject*> proxy(cx, &proxy_->as<ProxyObject>());\n  proxy->setReservedSlot(ScriptedProxyHandler::HANDLER_EXTRA,\n                         ObjectValue(*handler));\n\n  // Step 5.\n  uint32_t callable =\n      target->isCallable() ? ScriptedProxyHandler::IS_CALLABLE : 0;\n  uint32_t constructor =\n      target->isConstructor() ? ScriptedProxyHandler::IS_CONSTRUCTOR : 0;\n  proxy->setReservedSlot(ScriptedProxyHandler::IS_CALLCONSTRUCT_EXTRA,\n                         PrivateUint32Value(callable | constructor));\n\n  // Step 8.\n  args.rval().setObject(*proxy);\n  return true;\n}\n\nbool js::proxy(JSContext* cx, unsigned argc, Value* vp) {\n  CallArgs args = CallArgsFromVp(argc, vp);\n\n  if (!ThrowIfNotConstructing(cx, args, \"Proxy\")) {\n    return false;\n  }\n\n  return ProxyCreate(cx, args, \"Proxy\");\n}\n\nstatic bool RevokeProxy(JSContext* cx, unsigned argc, Value* vp) {\n  CallArgs args = CallArgsFromVp(argc, vp);\n\n  RootedFunction func(cx, &args.callee().as<JSFunction>());\n  RootedObject p(cx, func->getExtendedSlot(ScriptedProxyHandler::REVOKE_SLOT)\n                         .toObjectOrNull());\n\n  if (p) {\n    func->setExtendedSlot(ScriptedProxyHandler::REVOKE_SLOT, NullValue());\n\n    MOZ_ASSERT(p->is<ProxyObject>());\n\n    p->as<ProxyObject>().setSameCompartmentPrivate(NullValue());\n    p->as<ProxyObject>().setReservedSlot(ScriptedProxyHandler::HANDLER_EXTRA,\n                                         NullValue());\n  }\n\n  args.rval().setUndefined();",
      "        if (const auto fillFieldValue = evaluateInput(*doc)) {\n            Value sortFieldValue =\n                _sortBy->evaluate(*doc, &_sortBy->getExpressionContext()->variables);\n            if (!sortFieldValue.nullish()) {\n                _prevX2Y2 = boost::optional<std::pair<Value, Value>>(\n                    std::make_pair(sortFieldValue, *fillFieldValue));\n                return _prevX2Y2;\n            }\n        }\n        index++;\n    }\n    return boost::none;\n}\n\nValue WindowFunctionExecLinearFill::getNext() {\n    const auto currentDoc = *_iter[0];\n    Value fillFieldValue = _input->evaluate(currentDoc, &_input->getExpressionContext()->variables);\n    uassert(ErrorCodes::TypeMismatch,\n            str::stream() << \"Value to be filled must be numeric or nullish, but found \"\n                          << fillFieldValue.getType(),\n            fillFieldValue.numeric() || fillFieldValue.nullish());\n    Value sortFieldValue =\n        _sortBy->evaluate(currentDoc, &_sortBy->getExpressionContext()->variables);\n    uassert(ErrorCodes::TypeMismatch,\n            str::stream() << \"Value of the sortBy field must be numeric or a date, but found \"\n                          << sortFieldValue.getType(),\n            sortFieldValue.numeric() || sortFieldValue.coercibleToDate());\n\n    // We do not allow repeated sort field values. This is because if we have the following\n    // collection that has a repeated sort value and different fill values, eg [(10, 100), (10,\n    // -100), (20, null), (30, 50)], it is unclear if the left value, (X1, Y1), should be (10, 100)\n    // or (10, -100) when we interpolate on the third document.\n\n    uassert(6050106,\n            \"There can be no repeated values in the sort field\",\n            ValueComparator{}.evaluate(sortFieldValue != _lastSeenElement));\n    if (!_lastSeenElement.missing())\n        // Throw an error If the sort value was previously of type numeric, but we've just found a\n        // date (or vice versa).\n        uassert(ErrorCodes::TypeMismatch,\n                str::stream() << \"Conflicting sort value types, previously received type \"\n                              << _lastSeenElement.getType() << \", but found \"\n                              << sortFieldValue.getType(),\n                (sortFieldValue.coercibleToDate() && _lastSeenElement.coercibleToDate()) ||\n                    (sortFieldValue.numeric() && _lastSeenElement.numeric()));\n    _lastSeenElement = sortFieldValue;\n\n    // We have found either (x1, y1) or (x2, y2).\n    if (!fillFieldValue.nullish()) {\n        // We can expire all documents before the current document in the cache. We don't want to\n        // expire the current document, because it may become our *first* non-null, ie (x1, x2), for\n        // the next set of null documents.\n        _iter.manualExpireUpTo(-1);\n        // If (x2, y2) is known, it becomes our (x1, y1) for the next sequence of null documents.\n        // Otherwise the current document becomes (x1, y1).\n        _prevX1Y1 = _prevX2Y2 ? _prevX2Y2\n                              : boost::optional<std::pair<Value, Value>>(\n                                    std::make_pair(sortFieldValue, fillFieldValue));\n        _prevX2Y2 = boost::none;\n        return fillFieldValue;\n    }\n    // Interpolation requires that the documents with null values for the field we are filling,\n    // be bookended with documents with non-null sort and input field numeric values.\n    // To do this, we store the previous known coordinates so that we don't scan the same null",
      " */\n\n#include <memory>\n\n#include <boost/optional/optional.hpp>\n#include <boost/preprocessor/control/iif.hpp>\n\n#include \"mongo/base/error_codes.h\"\n#include \"mongo/db/catalog/collection_catalog.h\"\n#include \"mongo/db/commands/profile_common.h\"\n#include \"mongo/db/commands/profile_gen.h\"\n#include \"mongo/db/commands/set_profiling_filter_globally_cmd.h\"\n#include \"mongo/db/concurrency/locker.h\"\n#include \"mongo/db/database_name.h\"\n#include \"mongo/db/operation_context.h\"\n#include \"mongo/db/profile_filter_impl.h\"\n#include \"mongo/util/assert_util.h\"\n\nnamespace mongo {\nnamespace {\n\nclass ProfileCmd : public ProfileCmdBase {\npublic:\n    ProfileCmd() = default;\n\n    // Although mongoS does not have a system.profile collection, the profile command can change the\n    // per-database profile filter, which applies to slow-query log lines just like on mongoD.\n    bool adminOnly() const final {\n        return false;\n    }\n\nprotected:\n    CollectionCatalog::ProfileSettings _applyProfilingLevel(\n        OperationContext* opCtx,\n        const DatabaseName& dbName,\n        const ProfileCmdRequest& request) const final {\n        // Writing to the CollectionCatalog requires holding the Global lock to avoid concurrent\n        // races with BatchedCollectionCatalogWriter.\n        Lock::GlobalLock lk{opCtx, MODE_IX};\n\n        const auto profilingLevel = request.getCommandParameter();\n\n        // The only valid profiling level for mongoS is 0, because mongoS has no system.profile\n        // collection in which to record the profiling data (because mongoS has no collections\n        // at all).\n        uassert(ErrorCodes::BadValue,\n                \"Profiling is not permitted on mongoS: the 'profile' field should be 0 to change \"\n                \"'slowms', 'sampleRate', or 'filter' settings for logging, or -1 to view current \"\n                \"values\",\n                profilingLevel == -1 || profilingLevel == 0);\n\n        auto oldSettings = CollectionCatalog::get(opCtx)->getDatabaseProfileSettings(dbName);\n\n        if (auto filterOrUnset = request.getFilter()) {\n            auto newSettings = oldSettings;\n            if (auto filter = filterOrUnset->obj) {\n                newSettings.filter = std::make_shared<ProfileFilterImpl>(*filter);\n            } else {\n                newSettings.filter = nullptr;\n            }\n            CollectionCatalog::write(opCtx, [&](CollectionCatalog& catalog) {\n                catalog.setDatabaseProfileSettings(dbName, newSettings);\n            });\n        }",
      "\nvoid PromiseResolver::RejectedCallback(JSContext* aCx,\n                                       JS::Handle<JS::Value> aValue) {\n  mPromise->MaybeRejectWithClone(aCx, aValue);\n}\n\nPromiseResolver::~PromiseResolver() { mPromise = nullptr; }\n\n/**\n * MaybeWrapPromise is a helper method used by Localization\n * API methods to clone the value returned by a promise\n * into a new context.\n *\n * This allows for a promise from a privileged context\n * to be returned into an unprivileged document.\n *\n * This method is only used for promises that carry values.\n */\nalready_AddRefed<Promise> Localization::MaybeWrapPromise(\n    Promise* aInnerPromise) {\n  // For system principal we don't need to wrap the\n  // result promise at all.\n  nsIPrincipal* principal = mGlobal->PrincipalOrNull();\n  if (principal && principal->IsSystemPrincipal()) {\n    return RefPtr<Promise>(aInnerPromise).forget();\n  }\n\n  ErrorResult result;\n  RefPtr<Promise> docPromise = Promise::Create(mGlobal, result);\n  if (NS_WARN_IF(result.Failed())) {\n    return nullptr;\n  }\n\n  RefPtr<PromiseResolver> resolver = new PromiseResolver(docPromise);\n  aInnerPromise->AppendNativeHandler(resolver);\n  return docPromise.forget();\n}\n\nvoid Localization::ConvertL10nArgsToJSValue(\n    JSContext* aCx, const L10nArgs& aArgs, JS::MutableHandle<JS::Value> aRetVal,\n    ErrorResult& aRv) {\n  // This method uses a temporary dictionary to automate\n  // converting an IDL Record to a JS Value via a dictionary.\n  //\n  // Once we get ToJSValue for Record, we'll switch to that.\n  L10nArgsHelperDict helperDict;\n  for (auto& entry : aArgs.Entries()) {\n    L10nArgs::EntryType* newEntry =\n        helperDict.mArgs.Entries().AppendElement(fallible);\n    if (!newEntry) {\n      aRv.Throw(NS_ERROR_OUT_OF_MEMORY);\n      return;\n    }\n    newEntry->mKey = entry.mKey;\n    newEntry->mValue = entry.mValue;\n  }\n  JS::Rooted<JS::Value> jsVal(aCx);\n  if (!ToJSValue(aCx, helperDict, &jsVal)) {\n    aRv.Throw(NS_ERROR_UNEXPECTED);\n    return;\n  }\n  JS::Rooted<JSObject*> jsObj(aCx, &jsVal.toObject());\n  if (!JS_GetProperty(aCx, jsObj, \"args\", aRetVal)) {\n    aRv.Throw(NS_ERROR_UNEXPECTED);",
      " *    This program is free software: you can redistribute it and/or modify\n *    it under the terms of the Server Side Public License, version 1,\n *    as published by MongoDB, Inc.\n *\n *    This program is distributed in the hope that it will be useful,\n *    but WITHOUT ANY WARRANTY; without even the implied warranty of\n *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n *    Server Side Public License for more details.\n *\n *    You should have received a copy of the Server Side Public License\n *    along with this program. If not, see\n *    <http://www.mongodb.com/licensing/server-side-public-license>.\n *\n *    As a special exception, the copyright holders give permission to link the\n *    code of portions of this program with the OpenSSL library under certain\n *    conditions as described in each individual source file and distribute\n *    linked combinations including the program with the OpenSSL library. You\n *    must comply with the Server Side Public License in all respects for\n *    all of the code used other than as permitted herein. If you modify file(s)\n *    with this exception, you may extend this exception to your version of the\n *    file(s), but you are not obligated to do so. If you do not wish to do so,\n *    delete this exception statement from your version. If you delete this\n *    exception statement from all source files in the program, then also delete\n *    it in the license file.\n */\n\n#include \"mongo/db/commands/rename_collection_common.h\"\n\n#include <variant>\n\n#include \"mongo/base/error_codes.h\"\n#include \"mongo/db/auth/action_set.h\"\n#include \"mongo/db/auth/action_type.h\"\n#include \"mongo/db/auth/authorization_session.h\"\n#include \"mongo/db/auth/resource_pattern.h\"\n#include \"mongo/db/client.h\"\n#include \"mongo/db/database_name.h\"\n#include \"mongo/db/namespace_string.h\"\n#include \"mongo/stdx/variant.h\"\n\nnamespace mongo {\nnamespace rename_collection {\n\nStatus checkAuthForRenameCollectionCommand(Client* client, const RenameCollectionCommand& request) {\n    const auto& sourceNS = request.getCommandParameter();\n    const auto& targetNS = request.getTo();\n    const bool dropTarget = [&] {\n        const auto dropTarget = request.getDropTarget();\n        if (stdx::holds_alternative<bool>(dropTarget)) {\n            return stdx::get<bool>(dropTarget);\n        }\n\n        // UUID alternative is \"trueish\"\n        return true;\n    }();\n\n    if (sourceNS.dbName() == targetNS.dbName() && sourceNS.isNormalCollection() &&\n        targetNS.isNormalCollection()) {\n        const bool canRename = AuthorizationSession::get(client)->isAuthorizedForActionsOnResource(\n            ResourcePattern::forDatabaseName(sourceNS.dbName()),\n            ActionType::renameCollectionSameDB);\n\n        bool canDropTargetIfNeeded = true;\n        if (dropTarget) {",
      "    mock().expect(kExampleCmdName, kExampleResponse).times(1);\n\n    RemoteCommandRequestOnAny bsonRequest{\n        makeRequest(BSON(kExampleCmdName << 1 << \"extradata\" << 1))};\n    RemoteCommandRequestOnAny cmdNameRequest{kExampleRequest};\n\n    // Run commands starting with the one for the older (potentially overridden) expectations.\n    // The first request is meant for the older expectation, however it fulfills the matching\n    // requirements for the more recent one. This is a problem as the remaining user expectation\n    // will be unable to match the second request (as it is missing the extra field).\n    ASSERT_OK(startCommand(bsonRequest));\n    // We will not be able to match this request to a user expectation at this point, so it will\n    // have to use the default expectation.\n    ASSERT_OK(startCommand(cmdNameRequest));\n\n    const auto deadline = net().now() + Milliseconds(100);\n    mock().runUntil(deadline);\n\n    // The command matcher superceded the BSON matcher so we have an some unmatched expectation.\n    ASSERT_THROWS_CODE(mock().verifyExpectations(), DBException, (ErrorCodes::Error)5015501);\n    ASSERT(!specificExp.isSatisfied());\n\n    // Fix the tally to meet all expectations.\n    ASSERT_OK(startCommand(bsonRequest));\n\n    mock().runUntilExpectationsSatisfied();\n    evaluateResponses(3 /* numExpected */);\n}\n\nTEST_F(MockNetworkTest, MockFixtureRunUntilReadyRequest) {\n    mock().expect(kExampleCmdName, kExampleResponse);\n\n    RemoteCommandRequestOnAny request{kExampleRequest};\n    ASSERT_OK(startCommand(request));\n\n    TaskExecutor::CallbackHandle cbAlarm;\n    bool alarmFired = false;\n    const auto deadline = net().now() + Milliseconds(100);\n    ASSERT_OK(net().setAlarm(cbAlarm, deadline, [&](Status status) {\n        ASSERT(status.isOK());\n        alarmFired = true;\n    }));\n    ASSERT_FALSE(alarmFired);\n\n    mock().runUntil(deadline);\n    ASSERT_TRUE(alarmFired);\n\n    // We will have run our expected request as it was ready.\n    mock().verifyExpectations();\n    evaluateResponses(1 /* numExpected */);\n}\n\nTEST_F(MockNetworkTest, MockFixtureRunUntilNotAllExpectationsSatisfied) {\n    mock().expect(kExampleCmdName, kExampleResponse);\n    mock().expect(kExampleCmdNameTwo, kExampleResponse);\n\n    RemoteCommandRequestOnAny request{kExampleRequest};\n    ASSERT_OK(startCommand(request));\n\n    TaskExecutor::CallbackHandle cbAlarm;\n    bool alarmFired = false;\n    const auto deadline = net().now() + Milliseconds(100);\n    ASSERT_OK(net().setAlarm(cbAlarm, deadline, [&](Status status) {\n        ASSERT(status.isOK());"
    ]
  },
  {
    "id": "grafana/grafana",
    "org": "grafana",
    "avatarURL": "https://avatars.githubusercontent.com/u/7195757?v=4",
    "name": "grafana/grafana",
    "url": "https://github.com/grafana/grafana",
    "lang": "Go, TypeScript",
    "desc": "Open source platform for monitoring and observability.",
    "star_num": 56981,
    "fork_num": 11235,
    "snippets": [
      "        // This prevents results showing out of order if first request is slower than later ones\n        if (searchTimestamp > this.lastSearchTimestamp) {\n          this.setState({ result, loading: false });\n          this.lastSearchTimestamp = searchTimestamp;\n        }\n      })\n      .catch((error) => {\n        reportSearchFailedQueryInteraction(this.state.eventTrackingNamespace, {\n          ...trackingInfo,\n          error: error?.message,\n        });\n        this.setState({ loading: false });\n      });\n  }\n\n  // This gets the possible tags from within the query results\n  getTagOptions = (): Promise<TermCount[]> => {\n    const query = this.lastQuery ?? {\n      kind: ['dashboard', 'folder'],\n      query: '*',\n    };\n    return getGrafanaSearcher().tags(query);\n  };\n\n  /**\n   * When item is selected clear some filters and report interaction\n   */\n  onSearchItemClicked = (e: React.MouseEvent<HTMLElement>) => {\n    reportSearchResultInteraction(this.state.eventTrackingNamespace, {\n      layout: this.state.layout,\n      starred: this.state.starred,\n      sortValue: this.state.sort,\n      query: this.state.query,\n      tagCount: this.state.tag?.length,\n      includePanels: this.state.includePanels,\n    });\n  };\n\n  /**\n   * Caller should handle debounce\n   */\n  onReportSearchUsage = () => {\n    reportDashboardListViewed(this.state.eventTrackingNamespace, {\n      layout: this.state.layout,\n      starred: this.state.starred,\n      sortValue: this.state.sort,\n      query: this.state.query,\n      tagCount: this.state.tag?.length,\n      includePanels: this.state.includePanels,\n    });\n  };\n}\n\nlet stateManager: SearchStateManager;\n\nexport function getSearchStateManager() {\n  if (!stateManager) {\n    const selectedLayout = localStorage.getItem(SEARCH_SELECTED_LAYOUT) as SearchLayout;\n    const layout = selectedLayout ?? initialState.layout;\n\n    let includePanels = store.getBool(SEARCH_PANELS_LOCAL_STORAGE_KEY, true);\n    if (includePanels) {\n      includePanels = false;\n    }",
      "package tree\n\n// StringToBytes converts string to byte slice without a memory allocation.\nfunc StringToBytes(s string) []byte {\n\treturn []byte(s)\n}\n\n// BytesToString converts byte slice to string without a memory allocation.\nfunc BytesToString(b []byte) string {\n\treturn string(b)\n}\n",
      "\t\"context\"\n\t\"reflect\"\n\t\"time\"\n\n\t\"github.com/grafana/grafana/pkg/infra/log\"\n\t\"github.com/grafana/grafana/pkg/infra/serverlock\"\n\t\"github.com/grafana/grafana/pkg/registry\"\n\t\"github.com/grafana/grafana/pkg/setting\"\n)\n\nvar logger = log.New(\"secret.migration\")\n\nconst actionName = \"secret migration task \"\n\n// SecretMigrationService is used to migrate legacy secrets to new unified secrets.\ntype SecretMigrationService interface {\n\tMigrate(ctx context.Context) error\n}\n\ntype SecretMigrationProvider interface {\n\tregistry.BackgroundService\n\tTriggerPluginMigration(ctx context.Context, toPlugin bool) error\n}\n\ntype SecretMigrationProviderImpl struct {\n\tservices                 []SecretMigrationService\n\tServerLockService        *serverlock.ServerLockService\n\tmigrateToPluginService   *MigrateToPluginService\n\tmigrateFromPluginService *MigrateFromPluginService\n}\n\nfunc ProvideSecretMigrationProvider(\n\tcfg *setting.Cfg,\n\tserverLockService *serverlock.ServerLockService,\n\tdataSourceSecretMigrationService *DataSourceSecretMigrationService,\n\tmigrateToPluginService *MigrateToPluginService,\n\tmigrateFromPluginService *MigrateFromPluginService,\n) *SecretMigrationProviderImpl {\n\tservices := make([]SecretMigrationService, 0)\n\tservices = append(services, dataSourceSecretMigrationService)\n\t// Plugin migration should always be last; should either migrate to or from, not both\n\t// This is because the migrateTo checks for use_plugin = true, in which case we should always\n\t// migrate by default to ensure users don't lose access to secrets. If migration has\n\t// already occurred, the migrateTo function will be called but it won't do anything\n\tif cfg.SectionWithEnvOverrides(\"secrets\").Key(\"migrate_from_plugin\").MustBool(false) {\n\t\tservices = append(services, migrateFromPluginService)\n\t} else {\n\t\tservices = append(services, migrateToPluginService)\n\t}\n\n\treturn &SecretMigrationProviderImpl{\n\t\tServerLockService:        serverLockService,\n\t\tservices:                 services,\n\t\tmigrateToPluginService:   migrateToPluginService,\n\t\tmigrateFromPluginService: migrateFromPluginService,\n\t}\n}\n\nfunc (s *SecretMigrationProviderImpl) Run(ctx context.Context) error {\n\treturn s.Migrate(ctx)\n}\n\n// Migrate Run migration services. This will block until all services have exited.\n// This should only be called once at startup",
      "import { GeomapPanel } from '../GeomapPanel';\nimport { geomapLayerRegistry } from '../layers/registry';\nimport { defaultStyleConfig } from '../style/types';\nimport { GeomapLayerActions, MapLayerState } from '../types';\n\nimport { initLayer } from './layers';\nimport { getNextLayerName } from './utils';\n\nexport const getActions = (panel: GeomapPanel) => {\n  const actions: GeomapLayerActions = {\n    selectLayer: (uid: string) => {\n      const selected = panel.layers.findIndex((v) => v.options.name === uid);\n      if (panel.panelContext && panel.panelContext.onInstanceStateChange) {\n        panel.panelContext.onInstanceStateChange({\n          map: panel.map,\n          layers: panel.layers,\n          selected,\n          actions: panel.actions,\n        });\n      }\n    },\n    canRename: (v: string) => {\n      return !panel.byName.has(v);\n    },\n    deleteLayer: (uid: string) => {\n      const layers: MapLayerState[] = [];\n      for (const lyr of panel.layers) {\n        if (lyr.options.name === uid) {\n          panel.map?.removeLayer(lyr.layer);\n        } else {\n          layers.push(lyr);\n        }\n      }\n      panel.layers = layers;\n      panel.doOptionsUpdate(0);\n    },\n    addlayer: (type: string) => {\n      const item = geomapLayerRegistry.getIfExists(type);\n      if (!item) {\n        return; // ignore empty request\n      }\n      initLayer(\n        panel,\n        panel.map!,\n        {\n          type: item.id,\n          name: getNextLayerName(panel),\n          config: cloneDeep(item.defaultOptions),\n          location: item.showLocation ? { mode: FrameGeometrySourceMode.Auto } : undefined,\n          tooltip: true,\n          ...(!item.hideOpacity && { opacity: defaultStyleConfig.opacity }),\n        },\n        false\n      ).then((lyr) => {\n        panel.layers = panel.layers.slice(0);\n        panel.layers.push(lyr);\n        panel.map?.addLayer(lyr.layer);\n\n        panel.doOptionsUpdate(panel.layers.length - 1);\n      });\n    },\n    reorder: (startIndex: number, endIndex: number) => {\n      const result = Array.from(panel.layers);\n      const [removed] = result.splice(startIndex, 1);",
      "\n\t\"github.com/grafana/grafana/pkg/cuectx\"\n)\n\n// rootrel is the relative path from the grafana repository root to the\n// directory containing the .cue files in which this kind is defined. Necessary\n// for runtime errors related to the definition and/or lineage to provide\n// a real path to the correct .cue file.\nconst rootrel string = \"kinds/librarypanel\"\n\n// TODO standard generated docs\ntype Kind struct {\n\tkindsys.Core\n\tlin    thema.ConvergentLineage[*Resource]\n\tjcodec vmux.Codec\n\tvalmux vmux.ValueMux[*Resource]\n}\n\n// type guard - ensure generated Kind type satisfies the kindsys.Core interface\nvar _ kindsys.Core = &Kind{}\n\n// TODO standard generated docs\nfunc NewKind(rt *thema.Runtime, opts ...thema.BindOption) (*Kind, error) {\n\tdef, err := cuectx.LoadCoreKindDef(rootrel, rt.Context(), nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tk := &Kind{}\n\tk.Core, err = kindsys.BindCore(rt, def, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Get the thema.Schema that the meta says is in the current version (which\n\t// codegen ensures is always the latest)\n\tcursch := thema.SchemaP(k.Core.Lineage(), def.Properties.CurrentVersion)\n\ttsch, err := thema.BindType(cursch, &Resource{})\n\tif err != nil {\n\t\t// Should be unreachable, modulo bugs in the Thema->Go code generator\n\t\treturn nil, err\n\t}\n\n\tk.jcodec = vmux.NewJSONCodec(\"librarypanel.json\")\n\tk.lin = tsch.ConvergentLineage()\n\tk.valmux = vmux.NewValueMux(k.lin.TypedSchema(), k.jcodec)\n\treturn k, nil\n}\n\n// ConvergentLineage returns the same [thema.Lineage] as Lineage, but bound (see [thema.BindType])\n// to the the LibraryPanel [Resource] type generated from the current schema, v0.0.\nfunc (k *Kind) ConvergentLineage() thema.ConvergentLineage[*Resource] {\n\treturn k.lin\n}\n\n// JSONValueMux is a version multiplexer that maps a []byte containing JSON data\n// at any schematized dashboard version to an instance of LibraryPanel [Resource].\n//\n// Validation and translation errors emitted from this func will identify the\n// input bytes as \"dashboard.json\".\n//\n// This is a thin wrapper around Thema's [vmux.ValueMux].\nfunc (k *Kind) JSONValueMux(b []byte) (*Resource, thema.TranslationLacunas, error) {\n\treturn k.valmux(b)\n}",
      "\t// Return ErrInvalidCredentials in case any of the errors was ErrInvalidCredentials (means that the authentication has failed at least once)\n\tfor _, ldapErr := range ldapSilentErrors {\n\t\tif errors.Is(ldapErr, ErrInvalidCredentials) {\n\t\t\treturn nil, ErrInvalidCredentials\n\t\t}\n\t}\n\n\t// Return ErrCouldNotFindUser if all of the configured LDAP servers returned with ErrCouldNotFindUser\n\treturn nil, ErrCouldNotFindUser\n}\n\n// User attempts to find an user by login/username by searching into all of the configured LDAP servers. Then, if the user is found it returns the user alongisde the server it was found.\nfunc (multiples *MultiLDAP) User(login string) (\n\t*login.ExternalUserInfo,\n\tldap.ServerConfig,\n\terror,\n) {\n\tif len(multiples.configs) == 0 {\n\t\treturn nil, ldap.ServerConfig{}, ErrNoLDAPServers\n\t}\n\n\tsearch := []string{login}\n\tfor index, config := range multiples.configs {\n\t\tserver := newLDAP(config, multiples.cfg)\n\n\t\tif err := server.Dial(); err != nil {\n\t\t\tlogDialFailure(err, config)\n\n\t\t\t// Only return an error if it is the last server so we can try next server\n\t\t\tif index == len(multiples.configs)-1 {\n\t\t\t\treturn nil, *config, err\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tdefer server.Close()\n\n\t\tif err := server.Bind(); err != nil {\n\t\t\treturn nil, *config, err\n\t\t}\n\n\t\tusers, err := server.Users(search)\n\t\tif err != nil {\n\t\t\treturn nil, *config, err\n\t\t}\n\n\t\tif len(users) != 0 {\n\t\t\treturn users[0], *config, nil\n\t\t}\n\t}\n\n\treturn nil, ldap.ServerConfig{}, ErrDidNotFindUser\n}\n\n// Users gets users from multiple LDAP servers\nfunc (multiples *MultiLDAP) Users(logins []string) (\n\t[]*login.ExternalUserInfo,\n\terror,\n) {\n\tvar result []*login.ExternalUserInfo\n\n\tif len(multiples.configs) == 0 {\n\t\treturn nil, ErrNoLDAPServers\n\t}",
      "/**\n * UserPermission is a map storing permissions in a form of\n * {\n *   action: true;\n * }\n */\nexport type UserPermission = Record<string, boolean>;\n\n// Permission actions\nexport enum AccessControlAction {\n  UsersRead = 'users:read',\n  UsersWrite = 'users:write',\n  UsersAuthTokenList = 'users.authtoken:read',\n  UsersAuthTokenUpdate = 'users.authtoken:write',\n  UsersPasswordUpdate = 'users.password:write',\n  UsersDelete = 'users:delete',\n  UsersCreate = 'users:create',\n  UsersEnable = 'users:enable',\n  UsersDisable = 'users:disable',\n  UsersPermissionsUpdate = 'users.permissions:write',\n  UsersLogout = 'users:logout',\n  UsersQuotasList = 'users.quotas:read',\n  UsersQuotasUpdate = 'users.quotas:write',\n\n  ServiceAccountsRead = 'serviceaccounts:read',\n  ServiceAccountsCreate = 'serviceaccounts:create',\n  ServiceAccountsWrite = 'serviceaccounts:write',\n  ServiceAccountsDelete = 'serviceaccounts:delete',\n  ServiceAccountsPermissionsRead = 'serviceaccounts.permissions:read',\n  ServiceAccountsPermissionsWrite = 'serviceaccounts.permissions:write',\n\n  OrgsRead = 'orgs:read',\n  OrgsPreferencesRead = 'orgs.preferences:read',\n  OrgsWrite = 'orgs:write',\n  OrgsPreferencesWrite = 'orgs.preferences:write',\n  OrgsCreate = 'orgs:create',\n  OrgsDelete = 'orgs:delete',\n  OrgUsersRead = 'org.users:read',\n  OrgUsersAdd = 'org.users:add',\n  OrgUsersRemove = 'org.users:remove',\n  OrgUsersWrite = 'org.users:write',\n\n  LDAPUsersRead = 'ldap.user:read',\n  LDAPUsersSync = 'ldap.user:sync',\n  LDAPStatusRead = 'ldap.status:read',\n\n  DataSourcesExplore = 'datasources:explore',\n  DataSourcesRead = 'datasources:read',\n  DataSourcesCreate = 'datasources:create',\n  DataSourcesWrite = 'datasources:write',\n  DataSourcesDelete = 'datasources:delete',\n  DataSourcesPermissionsRead = 'datasources.permissions:read',\n  DataSourcesCachingRead = 'datasources.caching:read',\n  DataSourcesInsightsRead = 'datasources.insights:read',\n\n  ActionServerStatsRead = 'server.stats:read',\n\n  ActionTeamsCreate = 'teams:create',\n  ActionTeamsDelete = 'teams:delete',\n  ActionTeamsRead = 'teams:read',\n  ActionTeamsWrite = 'teams:write',\n  ActionTeamsPermissionsRead = 'teams.permissions:read',\n  ActionTeamsPermissionsWrite = 'teams.permissions:write',\n",
      "  data: DataFrame[] = [];\n\n  constructor(instanceSettings: DataSourceInstanceSettings<InputOptions>) {\n    super(instanceSettings);\n\n    if (instanceSettings.jsonData.data) {\n      this.data = instanceSettings.jsonData.data.map((v) => toDataFrame(v));\n    }\n  }\n\n  /**\n   * Convert a query to a simple text string\n   */\n  getQueryDisplayText(query: InputQuery): string {\n    if (query.data) {\n      return 'Panel Data: ' + describeDataFrame(query.data);\n    }\n    return `Shared Data From: ${this.name} (${describeDataFrame(this.data)})`;\n  }\n\n  metricFindQuery(query: string, options?: any): Promise<MetricFindValue[]> {\n    return new Promise((resolve, reject) => {\n      const names = [];\n      for (const series of this.data) {\n        for (const field of series.fields) {\n          // TODO, match query/options?\n          names.push({\n            text: field.name,\n          });\n        }\n      }\n      resolve(names);\n    });\n  }\n\n  query(options: DataQueryRequest<InputQuery>): Promise<DataQueryResponse> {\n    const results: DataFrame[] = [];\n    for (const query of options.targets) {\n      if (query.hide) {\n        continue;\n      }\n      let data = this.data;\n      if (query.data) {\n        data = query.data.map((v) => toDataFrame(v));\n      }\n      for (let i = 0; i < data.length; i++) {\n        results.push({\n          ...data[i],\n          refId: query.refId,\n        });\n      }\n    }\n    return Promise.resolve({ data: results });\n  }\n\n  testDatasource(): Promise<TestDataSourceResponse> {\n    return new Promise((resolve, reject) => {\n      let rowCount = 0;\n      let info = `${this.data.length} Series:`;\n      for (const series of this.data) {\n        const length = series.length;\n        info += ` [${series.fields.length} Fields, ${length} Rows]`;\n        rowCount += length;\n      }",
      "package starimpl\n\nimport (\n\t\"testing\"\n\n\t\"github.com/grafana/grafana/pkg/infra/db\"\n)\n\nfunc TestIntegrationXormUserStarsDataAccess(t *testing.T) {\n\tif testing.Short() {\n\t\tt.Skip(\"skipping integration test\")\n\t}\n\ttestIntegrationUserStarsDataAccess(t, func(ss db.DB) store {\n\t\treturn &sqlStore{db: ss}\n\t})\n}\n",
      "\n      expect(contextQuery.query.expr).toEqual(`{bar=\"baz\"} | logfmt | line_format \"foo\" | label_format a=\"baz\"`);\n    });\n\n    it('should not apply additional parsers', async () => {\n      window.localStorage.setItem(SHOULD_INCLUDE_PIPELINE_OPERATIONS, 'true');\n      logContextProvider.appliedContextFilters = [{ value: 'baz', enabled: true, fromParser: false, label: 'bar' }];\n      const contextQuery = await logContextProvider.prepareLogRowContextQueryTarget(\n        defaultLogRow,\n        10,\n        LogRowContextQueryDirection.Backward,\n        {\n          expr: '{bar=\"baz\"} | logfmt | line_format \"foo\" | json | label_format a=\"baz\"',\n        } as unknown as LokiQuery\n      );\n\n      expect(contextQuery.query.expr).toEqual(`{bar=\"baz\"}`);\n    });\n  });\n\n  describe('getInitContextFiltersFromLabels', () => {\n    describe('query with no parser', () => {\n      const queryWithoutParser = {\n        expr: '{bar=\"baz\"}',\n      } as LokiQuery;\n\n      it('should correctly create contextFilters', async () => {\n        const filters = await logContextProvider.getInitContextFilters(defaultLogRow.labels, queryWithoutParser);\n        expect(filters).toEqual([\n          { enabled: true, fromParser: false, label: 'bar', value: 'baz' },\n          { enabled: false, fromParser: true, label: 'foo', value: 'uniqueParsedLabel' },\n          { enabled: true, fromParser: false, label: 'xyz', value: 'abc' },\n        ]);\n      });\n\n      it('should return empty contextFilters if no query', async () => {\n        const filters = await logContextProvider.getInitContextFilters(defaultLogRow.labels, undefined);\n        expect(filters).toEqual([]);\n      });\n\n      it('should return empty contextFilters if no labels', async () => {\n        const filters = await logContextProvider.getInitContextFilters({}, queryWithoutParser);\n        expect(filters).toEqual([]);\n      });\n    });\n\n    describe('query with parser', () => {\n      const queryWithParser = {\n        expr: '{bar=\"baz\"} | logfmt',\n      } as LokiQuery;\n\n      it('should correctly create contextFilters', async () => {\n        const filters = await logContextProvider.getInitContextFilters(defaultLogRow.labels, queryWithParser);\n        expect(filters).toEqual([\n          { enabled: true, fromParser: false, label: 'bar', value: 'baz' },\n          { enabled: false, fromParser: true, label: 'foo', value: 'uniqueParsedLabel' },\n          { enabled: true, fromParser: false, label: 'xyz', value: 'abc' },\n        ]);\n      });\n\n      it('should return empty contextFilters if no query', async () => {\n        const filters = await logContextProvider.getInitContextFilters(defaultLogRow.labels, undefined);\n        expect(filters).toEqual([]);\n      });"
    ]
  },
  {
    "id": "prometheus/prometheus",
    "org": "prometheus",
    "avatarURL": "https://avatars.githubusercontent.com/u/3380462?v=4",
    "name": "prometheus/prometheus",
    "url": "https://github.com/prometheus/prometheus",
    "lang": "Go",
    "desc": "The Prometheus monitoring system and time series database.",
    "star_num": 49748,
    "fork_num": 8460,
    "snippets": [
      "\npackage tsdb\n\nimport (\n\t\"encoding/json\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\n\t\"github.com/go-kit/log\"\n\t\"github.com/go-kit/log/level\"\n\t\"github.com/pkg/errors\"\n\n\ttsdb_errors \"github.com/prometheus/prometheus/tsdb/errors\"\n\t\"github.com/prometheus/prometheus/tsdb/fileutil\"\n)\n\n// repairBadIndexVersion repairs an issue in index and meta.json persistence introduced in\n// commit 129773b41a565fde5156301e37f9a87158030443.\nfunc repairBadIndexVersion(logger log.Logger, dir string) error {\n\t// All blocks written by Prometheus 2.1 with a meta.json version of 2 are affected.\n\t// We must actually set the index file version to 2 and revert the meta.json version back to 1.\n\tdirs, err := blockDirs(dir)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"list block dirs in %q\", dir)\n\t}\n\n\ttmpFiles := make([]string, 0, len(dirs))\n\tdefer func() {\n\t\tfor _, tmp := range tmpFiles {\n\t\t\tif err := os.RemoveAll(tmp); err != nil {\n\t\t\t\tlevel.Error(logger).Log(\"msg\", \"remove tmp file\", \"err\", err.Error())\n\t\t\t}\n\t\t}\n\t}()\n\n\tfor _, d := range dirs {\n\t\tmeta, err := readBogusMetaFile(d)\n\t\tif err != nil {\n\t\t\tlevel.Error(logger).Log(\"msg\", \"failed to read meta.json for a block during repair process; skipping\", \"dir\", d, \"err\", err)\n\t\t\tcontinue\n\t\t}\n\t\tif meta.Version == metaVersion1 {\n\t\t\tlevel.Info(logger).Log(\n\t\t\t\t\"msg\", \"Found healthy block\",\n\t\t\t\t\"mint\", meta.MinTime,\n\t\t\t\t\"maxt\", meta.MaxTime,\n\t\t\t\t\"ulid\", meta.ULID,\n\t\t\t)\n\t\t\tcontinue\n\t\t}\n\t\tlevel.Info(logger).Log(\n\t\t\t\"msg\", \"Fixing broken block\",\n\t\t\t\"mint\", meta.MinTime,\n\t\t\t\"maxt\", meta.MaxTime,\n\t\t\t\"ulid\", meta.ULID,\n\t\t)\n\n\t\trepl, err := os.Create(filepath.Join(d, \"index.repaired\"))\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, \"create index.repaired for block dir: %v\", d)\n\t\t}\n\t\ttmpFiles = append(tmpFiles, repl.Name())\n",
      "// See the License for the specific language governing permissions and\n// limitations under the License.\n//\n// Package zeropool provides a zero-allocation type-safe alternative for sync.Pool, used to workaround staticheck SA6002.\n// The contents of this package are brought from https://github.com/colega/zeropool because \"little copying is better than little dependency\".\n\npackage zeropool\n\nimport \"sync\"\n\n// Pool is a type-safe pool of items that does not allocate pointers to items.\n// That is not entirely true, it does allocate sometimes, but not most of the time,\n// just like the usual sync.Pool pools items most of the time, except when they're evicted.\n// It does that by storing the allocated pointers in a secondary pool instead of letting them go,\n// so they can be used later to store the items again.\n//\n// Zero value of Pool[T] is valid, and it will return zero values of T if nothing is pooled.\ntype Pool[T any] struct {\n\t// items holds pointers to the pooled items, which are valid to be used.\n\titems sync.Pool\n\t// pointers holds just pointers to the pooled item types.\n\t// The values referenced by pointers are not valid to be used (as they're used by some other caller)\n\t// and it is safe to overwrite these pointers.\n\tpointers sync.Pool\n}\n\n// New creates a new Pool[T] with the given function to create new items.\n// A Pool must not be copied after first use.\nfunc New[T any](item func() T) Pool[T] {\n\treturn Pool[T]{\n\t\titems: sync.Pool{\n\t\t\tNew: func() interface{} {\n\t\t\t\tval := item()\n\t\t\t\treturn &val\n\t\t\t},\n\t\t},\n\t}\n}\n\n// Get returns an item from the pool, creating a new one if necessary.\n// Get may be called concurrently from multiple goroutines.\nfunc (p *Pool[T]) Get() T {\n\tpooled := p.items.Get()\n\tif pooled == nil {\n\t\t// The only way this can happen is when someone is using the zero-value of zeropool.Pool, and items pool is empty.\n\t\t// We don't have a pointer to store in p.pointers, so just return the empty value.\n\t\tvar zero T\n\t\treturn zero\n\t}\n\n\tptr := pooled.(*T)\n\titem := *ptr // ptr still holds a reference to a copy of item, but nobody will use it.\n\tp.pointers.Put(ptr)\n\treturn item\n}\n\n// Put adds an item to the pool.\nfunc (p *Pool[T]) Put(item T) {\n\tvar ptr *T\n\tif pooled := p.pointers.Get(); pooled != nil {\n\t\tptr = pooled.(*T)\n\t} else {\n\t\tptr = new(T)\n\t}",
      "\t\tmodel.MetricNameLabel, metric,\n\t)\n\n\tsig := timeSeriesSignature(\n\t\tpmetric.MetricTypeExponentialHistogram.String(),\n\t\t&labels,\n\t)\n\tts, ok := series[sig]\n\tif !ok {\n\t\tts = &prompb.TimeSeries{\n\t\t\tLabels: labels,\n\t\t}\n\t\tseries[sig] = ts\n\t}\n\n\thistogram, err := exponentialToNativeHistogram(pt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tts.Histograms = append(ts.Histograms, histogram)\n\n\texemplars := getPromExemplars[pmetric.ExponentialHistogramDataPoint](pt)\n\tts.Exemplars = append(ts.Exemplars, exemplars...)\n\n\treturn nil\n}\n\n// exponentialToNativeHistogram  translates OTel Exponential Histogram data point\n// to Prometheus Native Histogram.\nfunc exponentialToNativeHistogram(p pmetric.ExponentialHistogramDataPoint) (prompb.Histogram, error) {\n\tscale := p.Scale()\n\tif scale < -4 || scale > 8 {\n\t\treturn prompb.Histogram{},\n\t\t\tfmt.Errorf(\"cannot convert exponential to native histogram.\"+\n\t\t\t\t\" Scale must be <= 8 and >= -4, was %d\", scale)\n\t\t// TODO: downscale to 8 if scale > 8\n\t}\n\n\tpSpans, pDeltas := convertBucketsLayout(p.Positive())\n\tnSpans, nDeltas := convertBucketsLayout(p.Negative())\n\n\th := prompb.Histogram{\n\t\tSchema: scale,\n\n\t\tZeroCount: &prompb.Histogram_ZeroCountInt{ZeroCountInt: p.ZeroCount()},\n\t\t// TODO use zero_threshold, if set, see\n\t\t// https://github.com/open-telemetry/opentelemetry-proto/pull/441\n\t\tZeroThreshold: defaultZeroThreshold,\n\n\t\tPositiveSpans:  pSpans,\n\t\tPositiveDeltas: pDeltas,\n\t\tNegativeSpans:  nSpans,\n\t\tNegativeDeltas: nDeltas,\n\n\t\tTimestamp: convertTimeStamp(p.Timestamp()),\n\t}\n\n\tif p.Flags().NoRecordedValue() {\n\t\th.Sum = math.Float64frombits(value.StaleNaN)\n\t\th.Count = &prompb.Histogram_CountInt{CountInt: value.StaleNaN}\n\t} else {\n\t\tif p.HasSum() {\n\t\t\th.Sum = p.Sum()\n\t\t}",
      "\treturn promql.NewOriginContext(ctx, map[string]interface{}{\n\t\t\"httpRequest\": map[string]string{\n\t\t\t\"clientIP\": ip,\n\t\t\t\"method\":   r.Method,\n\t\t\t\"path\":     path,\n\t\t},\n\t})\n}\n",
      "package testutil\n\nimport (\n\t\"net\"\n\t\"testing\"\n)\n\n// RandomUnprivilegedPort returns valid unprivileged random port number which can be used for testing.\nfunc RandomUnprivilegedPort(t *testing.T) int {\n\tt.Helper()\n\n\tlistener, err := net.Listen(\"tcp\", \":0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Listening on random port: %v\", err)\n\t}\n\n\tif err := listener.Close(); err != nil {\n\t\tt.Fatalf(\"Closing listener: %v\", err)\n\t}\n\n\treturn listener.Addr().(*net.TCPAddr).Port\n}\n",
      "}\n",
      "\n\t\"github.com/stretchr/testify/require\"\n\t\"golang.org/x/sync/errgroup\"\n\n\t\"github.com/prometheus/prometheus/util/teststorage\"\n)\n\nfunc newTestEngine() *Engine {\n\treturn NewEngine(EngineOpts{\n\t\tLogger:                   nil,\n\t\tReg:                      nil,\n\t\tMaxSamples:               10000,\n\t\tTimeout:                  100 * time.Second,\n\t\tNoStepSubqueryIntervalFn: func(int64) int64 { return durationMilliseconds(1 * time.Minute) },\n\t\tEnableAtModifier:         true,\n\t\tEnableNegativeOffset:     true,\n\t\tEnablePerStepStats:       true,\n\t})\n}\n\nfunc TestEvaluations(t *testing.T) {\n\tRunBuiltinTests(t, newTestEngine())\n}\n\n// Run a lot of queries at the same time, to check for race conditions.\nfunc TestConcurrentRangeQueries(t *testing.T) {\n\tstor := teststorage.New(t)\n\tdefer stor.Close()\n\topts := EngineOpts{\n\t\tLogger:     nil,\n\t\tReg:        nil,\n\t\tMaxSamples: 50000000,\n\t\tTimeout:    100 * time.Second,\n\t}\n\tengine := NewEngine(opts)\n\n\tconst interval = 10000 // 10s interval.\n\t// A day of data plus 10k steps.\n\tnumIntervals := 8640 + 10000\n\terr := setupRangeQueryTestData(stor, engine, interval, numIntervals)\n\trequire.NoError(t, err)\n\n\tcases := rangeQueryCases()\n\n\t// Limit the number of queries running at the same time.\n\tconst numConcurrent = 4\n\tsem := make(chan struct{}, numConcurrent)\n\tfor i := 0; i < numConcurrent; i++ {\n\t\tsem <- struct{}{}\n\t}\n\tvar g errgroup.Group\n\tfor _, c := range cases {\n\t\tc := c\n\t\tif strings.Contains(c.expr, \"count_values\") && c.steps > 10 {\n\t\t\tcontinue // This test is too big to run with -race.\n\t\t}\n\t\tif strings.Contains(c.expr, \"[1d]\") && c.steps > 100 {\n\t\t\tcontinue // This test is too slow.\n\t\t}\n\t\t<-sem\n\t\tg.Go(func() error {\n\t\t\tdefer func() {\n\t\t\t\tsem <- struct{}{}\n\t\t\t}()",
      "\trequire.NotEqual(t, err0, err1, \"Error nodes should not be the same\")\n}\n\nfunc TestError(t *testing.T) {\n\ttests := []struct {\n\t\tname  string\n\t\terror *Error\n\t\twant  string\n\t}{\n\t\t{\n\t\t\tname: \"with alternative node provided in WrappedError\",\n\t\t\terror: &Error{\n\t\t\t\tGroup:    \"some group\",\n\t\t\t\tRule:     1,\n\t\t\t\tRuleName: \"some rule name\",\n\t\t\t\tErr: WrappedError{\n\t\t\t\t\terr: errors.New(\"some error\"),\n\t\t\t\t\tnode: &yaml.Node{\n\t\t\t\t\t\tLine:   10,\n\t\t\t\t\t\tColumn: 20,\n\t\t\t\t\t},\n\t\t\t\t\tnodeAlt: &yaml.Node{\n\t\t\t\t\t\tLine:   11,\n\t\t\t\t\t\tColumn: 21,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\twant: `10:20: 11:21: group \"some group\", rule 1, \"some rule name\": some error`,\n\t\t},\n\t\t{\n\t\t\tname: \"with node provided in WrappedError\",\n\t\t\terror: &Error{\n\t\t\t\tGroup:    \"some group\",\n\t\t\t\tRule:     1,\n\t\t\t\tRuleName: \"some rule name\",\n\t\t\t\tErr: WrappedError{\n\t\t\t\t\terr: errors.New(\"some error\"),\n\t\t\t\t\tnode: &yaml.Node{\n\t\t\t\t\t\tLine:   10,\n\t\t\t\t\t\tColumn: 20,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\twant: `10:20: group \"some group\", rule 1, \"some rule name\": some error`,\n\t\t},\n\t\t{\n\t\t\tname: \"with only err provided in WrappedError\",\n\t\t\terror: &Error{\n\t\t\t\tGroup:    \"some group\",\n\t\t\t\tRule:     1,\n\t\t\t\tRuleName: \"some rule name\",\n\t\t\t\tErr: WrappedError{\n\t\t\t\t\terr: errors.New(\"some error\"),\n\t\t\t\t},\n\t\t\t},\n\t\t\twant: `group \"some group\", rule 1, \"some rule name\": some error`,\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tgot := tt.error.Error()\n\t\t\trequire.Equal(t, tt.want, got)\n\t\t})",
      "\tjsonutil.MarshalTimestamp(p.T, stream)\n\tstream.WriteMore()\n\tjsonutil.MarshalHistogram(p.H, stream)\n\tstream.WriteArrayEnd()\n}\n\nfunc marshalPointJSONIsEmpty(unsafe.Pointer) bool {\n\treturn false\n}\n\n// marshalExemplarJSON writes.\n//\n//\t{\n//\t   labels: <labels>,\n//\t   value: \"<string>\",\n//\t   timestamp: <float>\n//\t}\nfunc marshalExemplarJSON(ptr unsafe.Pointer, stream *jsoniter.Stream) {\n\tp := *((*exemplar.Exemplar)(ptr))\n\tstream.WriteObjectStart()\n\n\t// \"labels\" key.\n\tstream.WriteObjectField(`labels`)\n\tmarshalLabelsJSON(p.Labels, stream)\n\n\t// \"value\" key.\n\tstream.WriteMore()\n\tstream.WriteObjectField(`value`)\n\tjsonutil.MarshalFloat(p.Value, stream)\n\n\t// \"timestamp\" key.\n\tstream.WriteMore()\n\tstream.WriteObjectField(`timestamp`)\n\tjsonutil.MarshalTimestamp(p.Ts, stream)\n\n\tstream.WriteObjectEnd()\n}\n\nfunc marshalExemplarJSONEmpty(unsafe.Pointer) bool {\n\treturn false\n}\n\nfunc unsafeMarshalLabelsJSON(ptr unsafe.Pointer, stream *jsoniter.Stream) {\n\tlabelsPtr := (*labels.Labels)(ptr)\n\tmarshalLabelsJSON(*labelsPtr, stream)\n}\n\nfunc marshalLabelsJSON(lbls labels.Labels, stream *jsoniter.Stream) {\n\tstream.WriteObjectStart()\n\ti := 0\n\tlbls.Range(func(v labels.Label) {\n\t\tif i != 0 {\n\t\t\tstream.WriteMore()\n\t\t}\n\t\ti++\n\t\tstream.WriteString(v.Name)\n\t\tstream.WriteRaw(`:`)\n\t\tstream.WriteString(v.Value)\n\t})\n\tstream.WriteObjectEnd()\n}\n\nfunc labelsIsEmpty(ptr unsafe.Pointer) bool {\n\tlabelsPtr := (*labels.Labels)(ptr)",
      "\n\tswitch {\n\tcase m.Gauge != nil:\n\t\ttoTimeseries(wr, labels, timestamp, m.GetGauge().GetValue())\n\tcase m.Counter != nil:\n\t\ttoTimeseries(wr, labels, timestamp, m.GetCounter().GetValue())\n\tcase m.Summary != nil:\n\t\tmetricName := labels[model.MetricNameLabel]\n\t\t// Preserve metric name order with first quantile labels timeseries then sum suffix timeserie and finally count suffix timeserie\n\t\t// Add Summary quantile timeseries\n\t\tquantileLabels := make(map[string]string, len(labels)+1)\n\t\tfor key, value := range labels {\n\t\t\tquantileLabels[key] = value\n\t\t}\n\n\t\tfor _, q := range m.GetSummary().Quantile {\n\t\t\tquantileLabels[model.QuantileLabel] = fmt.Sprint(q.GetQuantile())\n\t\t\ttoTimeseries(wr, quantileLabels, timestamp, q.GetValue())\n\t\t}\n\t\t// Overwrite label model.MetricNameLabel for count and sum metrics\n\t\t// Add Summary sum timeserie\n\t\tlabels[model.MetricNameLabel] = metricName + sumStr\n\t\ttoTimeseries(wr, labels, timestamp, m.GetSummary().GetSampleSum())\n\t\t// Add Summary count timeserie\n\t\tlabels[model.MetricNameLabel] = metricName + countStr\n\t\ttoTimeseries(wr, labels, timestamp, float64(m.GetSummary().GetSampleCount()))\n\n\tcase m.Histogram != nil:\n\t\tmetricName := labels[model.MetricNameLabel]\n\t\t// Preserve metric name order with first bucket suffix timeseries then sum suffix timeserie and finally count suffix timeserie\n\t\t// Add Histogram bucket timeseries\n\t\tbucketLabels := make(map[string]string, len(labels)+1)\n\t\tfor key, value := range labels {\n\t\t\tbucketLabels[key] = value\n\t\t}\n\t\tfor _, b := range m.GetHistogram().Bucket {\n\t\t\tbucketLabels[model.MetricNameLabel] = metricName + bucketStr\n\t\t\tbucketLabels[model.BucketLabel] = fmt.Sprint(b.GetUpperBound())\n\t\t\ttoTimeseries(wr, bucketLabels, timestamp, float64(b.GetCumulativeCount()))\n\t\t}\n\t\t// Overwrite label model.MetricNameLabel for count and sum metrics\n\t\t// Add Histogram sum timeserie\n\t\tlabels[model.MetricNameLabel] = metricName + sumStr\n\t\ttoTimeseries(wr, labels, timestamp, m.GetHistogram().GetSampleSum())\n\t\t// Add Histogram count timeserie\n\t\tlabels[model.MetricNameLabel] = metricName + countStr\n\t\ttoTimeseries(wr, labels, timestamp, float64(m.GetHistogram().GetSampleCount()))\n\n\tcase m.Untyped != nil:\n\t\ttoTimeseries(wr, labels, timestamp, m.GetUntyped().GetValue())\n\tdefault:\n\t\terr = errors.New(\"unsupported metric type\")\n\t}\n\treturn err\n}\n\nfunc makeLabels(labelsMap map[string]string) []prompb.Label {\n\t// build labels name list\n\tsortedLabelNames := make([]string, 0, len(labelsMap))\n\tfor label := range labelsMap {\n\t\tsortedLabelNames = append(sortedLabelNames, label)\n\t}\n\t// sort labels name in lexicographical order\n\tsort.Strings(sortedLabelNames)"
    ]
  },
  {
    "id": "netdata/netdata",
    "org": "netdata",
    "avatarURL": "https://avatars.githubusercontent.com/u/43390781?v=4",
    "name": "netdata/netdata",
    "url": "https://github.com/netdata/netdata",
    "lang": "C",
    "desc": "Real-time performance monitoring, done right!",
    "star_num": 64954,
    "fork_num": 5655,
    "snippets": [
      "\tchar **packets = malloc(sizeof(char *) * metrics);\n\tsize_t i, *lengths = malloc(sizeof(size_t) * metrics);\n\tsize_t t;\n\n\tfor(i = 0, t = 0; i < metrics ;i++, t++) {\n\t\tif(!types[t]) t = 0;\n\t\tchar *type = types[t];\n\n\t\tlengths[i] = sprintf(packet, \"stress.%s.t%zu.m%zu:%zu|%s\", type, data->id, i, myrand(metrics), type);\n\t\tpackets[i] = strdup(packet);\n\t\t// printf(\"packet %zu, of length %zu: '%s'\\n\", i, lengths[i], packets[i]);\n\t}\n\t//printf(\"\\n\");\n\n\tfor (;;) {\n\t\tfor(i = 0; i < metrics ;i++) {\n\t\t\tif (sendto(s, packets[i], lengths[i], 0, (void *)data->si_other, data->slen) < 0) {\n\t\t\t\tprintf(\"C ==> DROPPED\\n\");\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t\tdata->counter++;\n\t\t}\n\t}\n\n\tfree(packets);\n\tfree(lengths);\n\tclose(s);\n\treturn NULL;\n}\n\nint main(int argc, char *argv[])\n{\n\tif (argc != 5) {\n\t\tfprintf(stderr, \"Usage: '%s THREADS METRICS IP PORT'\\n\", argv[0]);\n\t\texit(-1);\n\t}\n\n\trun_threads = atoi(argv[1]);\n\tmetrics = atoi(argv[2]);\n\tchar *ip = argv[3];\n\tint port = atoi(argv[4]);\n\n\tstruct thread_data data[run_threads];\n\tstruct sockaddr_in si_other;\n\tpthread_t threads[run_threads], report;\n\tsize_t i;\n\n\tsrand(time(NULL));\n\n\tmemset(&si_other, 0, sizeof(si_other));\n\tsi_other.sin_family = AF_INET;\n\tsi_other.sin_port = htons(port);\n\tif (inet_aton(ip, &si_other.sin_addr)==0) {\n\t\tfprintf(stderr, \"inet_aton() of ip '%s' failed\\n\", ip);\n\t\texit(1);\n\t}\n\n\tfor (i = 0; i < run_threads; ++i) {\n\t\tdata[i].id       = i;\n\t\tdata[i].si_other = &si_other;\n\t\tdata[i].slen     = sizeof(si_other);\n\t\tdata[i].counter  = 0;\n\t\tpthread_create(&threads[i], NULL, spam_thread, &data[i]);\n\t}",
      "// SPDX-License-Identifier: GPL-3.0-or-later\n\n#include \"pipename.h\"\n\n#include <stdlib.h>\n\nconst char *daemon_pipename(void) {\n    const char *pipename = getenv(\"NETDATA_PIPENAME\");\n    if (pipename)\n        return pipename;\n\n#ifdef _WIN32\n    return \"\\\\\\\\?\\\\pipe\\\\netdata-cli\";\n#else\n    return \"/tmp/netdata-ipc\";\n#endif\n}\n",
      "    query->data.bin_payload.msg_name = \"ContextsUpdated\";\n    QUEUE_IF_PAYLOAD_PRESENT(query);\n}\n\nvoid aclk_update_node_collectors(struct update_node_collectors *collectors)\n{\n    aclk_query_t query = aclk_query_new(UPDATE_NODE_COLLECTORS);\n    query->data.bin_payload.topic = ACLK_TOPICID_NODE_COLLECTORS;\n    query->data.bin_payload.payload = generate_update_node_collectors_message(&query->data.bin_payload.size, collectors);\n    query->data.bin_payload.msg_name = \"UpdateNodeCollectors\";\n    QUEUE_IF_PAYLOAD_PRESENT(query);\n}\n\nvoid aclk_update_node_info(struct update_node_info *info)\n{\n    aclk_query_t query = aclk_query_new(UPDATE_NODE_INFO);\n    query->data.bin_payload.topic = ACLK_TOPICID_NODE_INFO;\n    query->data.bin_payload.payload = generate_update_node_info_message(&query->data.bin_payload.size, info);\n    query->data.bin_payload.msg_name = \"UpdateNodeInfo\";\n    QUEUE_IF_PAYLOAD_PRESENT(query);\n}\n",
      "    freez(cg->io_service_bytes.filename);\n    freez(cg->io_serviced.filename);\n\n    freez(cg->throttle_io_service_bytes.filename);\n    freez(cg->throttle_io_serviced.filename);\n\n    freez(cg->io_merged.filename);\n    freez(cg->io_queued.filename);\n\n    free_pressure(&cg->cpu_pressure);\n    free_pressure(&cg->io_pressure);\n    free_pressure(&cg->memory_pressure);\n    free_pressure(&cg->irq_pressure);\n\n    freez(cg->id);\n    freez(cg->intermediate_id);\n    freez(cg->chart_id);\n    freez(cg->chart_title);\n\n    rrdlabels_destroy(cg->chart_labels);\n\n    freez(cg);\n\n    cgroup_root_count--;\n}\n\n// ----------------------------------------------------------------------------\n\nstatic inline void discovery_rename_cgroup(struct cgroup *cg) {\n    if (!cg->pending_renames) {\n        return;\n    }\n    cg->pending_renames--;\n\n    netdata_log_debug(D_CGROUP, \"looking for the name of cgroup '%s' with chart id '%s' and title '%s'\", cg->id, cg->chart_id, cg->chart_title);\n    netdata_log_debug(D_CGROUP, \"executing command %s \\\"%s\\\" for cgroup '%s'\", cgroups_rename_script, cg->intermediate_id, cg->chart_id);\n    pid_t cgroup_pid;\n\n    FILE *fp_child_input, *fp_child_output;\n    (void)netdata_popen_raw_default_flags_and_environment(&cgroup_pid, &fp_child_input, &fp_child_output, cgroups_rename_script, cg->id, cg->intermediate_id);\n    if (!fp_child_output) {\n        collector_error(\"CGROUP: cannot popen(%s \\\"%s\\\", \\\"r\\\").\", cgroups_rename_script, cg->intermediate_id);\n        cg->pending_renames = 0;\n        cg->processed = 1;\n        return;\n    }\n\n    char buffer[CGROUP_CHARTID_LINE_MAX + 1];\n    char *new_name = fgets(buffer, CGROUP_CHARTID_LINE_MAX, fp_child_output);\n    int exit_code = netdata_pclose(fp_child_input, fp_child_output, cgroup_pid);\n\n    switch (exit_code) {\n        case 0:\n            cg->pending_renames = 0;\n            break;\n\n        case 3:\n            cg->pending_renames = 0;\n            cg->processed = 1;\n            break;\n    }\n\n    if (cg->pending_renames || cg->processed)\n        return;",
      "            if(!rc->calculation)\n                netdata_log_error(\"Health alarm '%s.%s': failed to parse calculation expression '%s'\", rrdset_id(st), rrdcalctemplate_name(rt), rt->calculation->source);\n        }\n        if(rt->warning) {\n            rc->warning = expression_parse(rt->warning->source, NULL, NULL);\n            if(!rc->warning)\n                netdata_log_error(\"Health alarm '%s.%s': failed to re-parse warning expression '%s'\", rrdset_id(st), rrdcalctemplate_name(rt), rt->warning->source);\n        }\n        if(rt->critical) {\n            rc->critical = expression_parse(rt->critical->source, NULL, NULL);\n            if(!rc->critical)\n                netdata_log_error(\"Health alarm '%s.%s': failed to re-parse critical expression '%s'\", rrdset_id(st), rrdcalctemplate_name(rt), rt->critical->source);\n        }\n    }\n    else if(ctr->from_config) {\n        // dictionary has already copied all the members values and pointers\n        // no need for additional work in this case\n        ;\n    }\n\n    rc->id = rrdcalc_get_unique_id(host, rc->chart, rc->name, &rc->next_event_id, &rc->config_hash_id);\n\n    if(rc->calculation) {\n        rc->calculation->status = &rc->status;\n        rc->calculation->myself = &rc->value;\n        rc->calculation->after = &rc->db_after;\n        rc->calculation->before = &rc->db_before;\n        rc->calculation->rrdcalc = rc;\n    }\n\n    if(rc->warning) {\n        rc->warning->status = &rc->status;\n        rc->warning->myself = &rc->value;\n        rc->warning->after = &rc->db_after;\n        rc->warning->before = &rc->db_before;\n        rc->warning->rrdcalc = rc;\n    }\n\n    if(rc->critical) {\n        rc->critical->status = &rc->status;\n        rc->critical->myself = &rc->value;\n        rc->critical->after = &rc->db_after;\n        rc->critical->before = &rc->db_before;\n        rc->critical->rrdcalc = rc;\n    }\n\n    netdata_log_debug(D_HEALTH, \"Health added alarm '%s.%s': exec '%s', recipient '%s', green \" NETDATA_DOUBLE_FORMAT_AUTO\n                    \", red \" NETDATA_DOUBLE_FORMAT_AUTO\n                    \", lookup: group %d, after %d, before %d, options %u, dimensions '%s', for each dimension '%s', update every %d, calculation '%s', warning '%s', critical '%s', source '%s', delay up %d, delay down %d, delay max %d, delay_multiplier %f, warn_repeat_every %u, crit_repeat_every %u\",\n          rrdcalc_chart_name(rc),\n          rrdcalc_name(rc),\n          (rc->exec)?rrdcalc_exec(rc):\"DEFAULT\",\n          (rc->recipient)?rrdcalc_recipient(rc):\"DEFAULT\",\n          rc->green,\n          rc->red,\n          (int)rc->group,\n          rc->after,\n          rc->before,\n          rc->options,\n          (rc->dimensions)?rrdcalc_dimensions(rc):\"NONE\",\n          (rc->foreach_dimension)?rrdcalc_foreachdim(rc):\"NONE\",\n          rc->update_every,\n          (rc->calculation)?rc->calculation->parsed_as:\"NONE\",\n          (rc->warning)?rc->warning->parsed_as:\"NONE\",",
      "    [44] = ',', // , keep\n    [45] = '-', // - keep\n    [46] = '.', // . keep\n    [47] = '/', // / keep\n    [48] = '0', // 0 keep\n    [49] = '1', // 1 keep\n    [50] = '2', // 2 keep\n    [51] = '3', // 3 keep\n    [52] = '4', // 4 keep\n    [53] = '5', // 5 keep\n    [54] = '6', // 6 keep\n    [55] = '7', // 7 keep\n    [56] = '8', // 8 keep\n    [57] = '9', // 9 keep\n    [58] = ':', // : keep\n    [59] = ':', // ; convert ; to :\n    [60] = '_', // <\n    [61] = ':', // = convert = to :\n    [62] = '_', // >\n    [63] = '_', // ?\n    [64] = '_', // @\n    [65] = 'A', // A keep\n    [66] = 'B', // B keep\n    [67] = 'C', // C keep\n    [68] = 'D', // D keep\n    [69] = 'E', // E keep\n    [70] = 'F', // F keep\n    [71] = 'G', // G keep\n    [72] = 'H', // H keep\n    [73] = 'I', // I keep\n    [74] = 'J', // J keep\n    [75] = 'K', // K keep\n    [76] = 'L', // L keep\n    [77] = 'M', // M keep\n    [78] = 'N', // N keep\n    [79] = 'O', // O keep\n    [80] = 'P', // P keep\n    [81] = 'Q', // Q keep\n    [82] = 'R', // R keep\n    [83] = 'S', // S keep\n    [84] = 'T', // T keep\n    [85] = 'U', // U keep\n    [86] = 'V', // V keep\n    [87] = 'W', // W keep\n    [88] = 'X', // X keep\n    [89] = 'Y', // Y keep\n    [90] = 'Z', // Z keep\n    [91] = '_', // [\n    [92] = '/', // backslash convert \\ to /\n    [93] = '_', // ]\n    [94] = '_', // ^\n    [95] = '_', // _ keep\n    [96] = '_', // `\n    [97] = 'a', // a keep\n    [98] = 'b', // b keep\n    [99] = 'c', // c keep\n    [100] = 'd', // d keep\n    [101] = 'e', // e keep\n    [102] = 'f', // f keep\n    [103] = 'g', // g keep\n    [104] = 'h', // h keep\n    [105] = 'i', // i keep\n    [106] = 'j', // j keep\n    [107] = 'k', // k keep",
      "const struct netdata_static_thread static_threads_freebsd[] = {\n    {NULL, NULL, NULL, 0, NULL, NULL, NULL, NULL, NULL}\n};\n\nconst struct netdata_static_thread static_threads_linux[] = {\n    {NULL, NULL, NULL, 0, NULL, NULL, NULL, NULL, NULL}\n};\n\nstruct netdata_static_thread *static_threads_get() {\n    return static_threads_concat(static_threads_common, static_threads_macos);\n}\n",
      "        tier = str2ul(tier_str);\n        if(tier < storage_tiers)\n            options |= RRDR_OPTION_SELECTED_TIER;\n        else\n            tier = 0;\n    }\n\n    time_t    before = (before_str && *before_str)?str2l(before_str):0;\n    time_t    after  = (after_str  && *after_str) ?str2l(after_str):-600;\n    size_t    points = (points_str && *points_str)?str2u(points_str):0;\n    int       timeout = (timeout_str && *timeout_str)?str2i(timeout_str): 0;\n    time_t    resampling_time = (resampling_time_str && *resampling_time_str) ? str2l(resampling_time_str) : 0;\n\n    QUERY_TARGET_REQUEST qtr = {\n            .version = 2,\n            .scope_nodes = scope_nodes,\n            .scope_contexts = scope_contexts,\n            .after = after,\n            .before = before,\n            .host = NULL,\n            .st = NULL,\n            .nodes = nodes,\n            .contexts = contexts,\n            .instances = instances,\n            .dimensions = dimensions,\n            .alerts = alerts,\n            .timeout_ms = timeout,\n            .points = points,\n            .format = format,\n            .options = options,\n            .time_group_method = time_group,\n            .time_group_options = time_group_options,\n            .resampling_time = resampling_time,\n            .tier = tier,\n            .chart_label_key = NULL,\n            .labels = labels,\n            .query_source = QUERY_SOURCE_API_DATA,\n            .priority = STORAGE_PRIORITY_NORMAL,\n            .received_ut = received_ut,\n\n            .interrupt_callback = web_client_interrupt_callback,\n            .interrupt_callback_data = w,\n    };\n\n    for(size_t g = 0; g < MAX_QUERY_GROUP_BY_PASSES ;g++)\n        qtr.group_by[g] = group_by[g];\n\n    QUERY_TARGET *qt = query_target_create(&qtr);\n    ONEWAYALLOC *owa = NULL;\n\n    if(!qt) {\n        buffer_sprintf(w->response.data, \"Failed to prepare the query.\");\n        ret = HTTP_RESP_INTERNAL_SERVER_ERROR;\n        goto cleanup;\n    }\n\n    web_client_timeout_checkpoint_set(w, timeout);\n    if(web_client_timeout_checkpoint_and_check(w, NULL)) {\n        ret = w->response.code;\n        goto cleanup;\n    }\n\n    if(outFileName && *outFileName) {\n        buffer_sprintf(w->response.header, \"Content-Disposition: attachment; filename=\\\"%s\\\"\\r\\n\", outFileName);",
      "    rrdinstance_rrdset_is_freed(st);\n}\n\nvoid rrdcontext_updated_retention_rrdset(RRDSET *st) {\n    rrdinstance_rrdset_has_updated_retention(st);\n}\n\nvoid rrdcontext_updated_rrdset_name(RRDSET *st) {\n    rrdinstance_updated_rrdset_name(st);\n}\n\nvoid rrdcontext_updated_rrdset_flags(RRDSET *st) {\n    rrdinstance_updated_rrdset_flags(st);\n}\n\nvoid rrdcontext_collected_rrdset(RRDSET *st) {\n    rrdinstance_collected_rrdset(st);\n}\n\nvoid rrdcontext_host_child_connected(RRDHOST *host) {\n    (void)host;\n\n    // no need to do anything here\n    ;\n}\n\nusec_t rrdcontext_next_db_rotation_ut = 0;\nvoid rrdcontext_db_rotation(void) {\n    // called when the db rotates its database\n    rrdcontext_next_db_rotation_ut = now_realtime_usec() + FULL_RETENTION_SCAN_DELAY_AFTER_DB_ROTATION_SECS * USEC_PER_SEC;\n}\n\nint rrdcontext_find_dimension_uuid(RRDSET *st, const char *id, uuid_t *store_uuid) {\n    if(!st->rrdhost) return 1;\n    if(!st->context) return 2;\n\n    RRDCONTEXT_ACQUIRED *rca = (RRDCONTEXT_ACQUIRED *)dictionary_get_and_acquire_item(st->rrdhost->rrdctx.contexts, string2str(st->context));\n    if(!rca) return 3;\n\n    RRDCONTEXT *rc = rrdcontext_acquired_value(rca);\n\n    RRDINSTANCE_ACQUIRED *ria = (RRDINSTANCE_ACQUIRED *)dictionary_get_and_acquire_item(rc->rrdinstances, string2str(st->id));\n    if(!ria) {\n        rrdcontext_release(rca);\n        return 4;\n    }\n\n    RRDINSTANCE *ri = rrdinstance_acquired_value(ria);\n\n    RRDMETRIC_ACQUIRED *rma = (RRDMETRIC_ACQUIRED *)dictionary_get_and_acquire_item(ri->rrdmetrics, id);\n    if(!rma) {\n        rrdinstance_release(ria);\n        rrdcontext_release(rca);\n        return 5;\n    }\n\n    RRDMETRIC *rm = rrdmetric_acquired_value(rma);\n\n    uuid_copy(*store_uuid, rm->uuid);\n\n    rrdmetric_release(rma);\n    rrdinstance_release(ria);\n    rrdcontext_release(rca);\n    return 0;",
      "    if (!engine)\n        return;\n\n    uint8_t count = 0;\n\n    for (struct instance *instance = engine->instance_root; instance; instance = instance->next) {\n        if (count)\n            buffer_strcat(b, \"|\");\n\n        switch (instance->config.type) {\n            case EXPORTING_CONNECTOR_TYPE_GRAPHITE:\n                buffer_strcat(b, \"Graphite\");\n                break;\n            case EXPORTING_CONNECTOR_TYPE_GRAPHITE_HTTP:\n                buffer_strcat(b, \"GraphiteHTTP\");\n                break;\n            case EXPORTING_CONNECTOR_TYPE_JSON:\n                buffer_strcat(b, \"JSON\");\n                break;\n            case EXPORTING_CONNECTOR_TYPE_JSON_HTTP:\n                buffer_strcat(b, \"JSONHTTP\");\n                break;\n            case EXPORTING_CONNECTOR_TYPE_OPENTSDB:\n                buffer_strcat(b, \"OpenTSDB\");\n                break;\n            case EXPORTING_CONNECTOR_TYPE_OPENTSDB_HTTP:\n                buffer_strcat(b, \"OpenTSDBHTTP\");\n                break;\n            case EXPORTING_CONNECTOR_TYPE_PROMETHEUS_REMOTE_WRITE:\n#if ENABLE_PROMETHEUS_REMOTE_WRITE\n                buffer_strcat(b, \"PrometheusRemoteWrite\");\n#endif\n                break;\n            case EXPORTING_CONNECTOR_TYPE_KINESIS:\n#if HAVE_KINESIS\n                buffer_strcat(b, \"Kinesis\");\n#endif\n                break;\n            case EXPORTING_CONNECTOR_TYPE_PUBSUB:\n#if ENABLE_EXPORTING_PUBSUB\n                buffer_strcat(b, \"Pubsub\");\n#endif\n                break;\n            case EXPORTING_CONNECTOR_TYPE_MONGODB:\n#if HAVE_MONGOC\n                buffer_strcat(b, \"MongoDB\");\n#endif\n                break;\n            default:\n                buffer_strcat(b, \"Unknown\");\n        }\n\n        count++;\n    }\n}\n\n/**\n * Exporting Clean Engine\n *\n * Clean all variables allocated inside engine structure\n *\n * @param en a pointer to the structure that will be cleaned.\n */\nstatic void exporting_clean_engine()"
    ]
  },
  {
    "id": "JetBrains/kotlin",
    "org": "JetBrains",
    "avatarURL": "https://avatars.githubusercontent.com/u/878437?v=4",
    "name": "JetBrains/kotlin",
    "url": "https://github.com/JetBrains/kotlin",
    "lang": "Kotlin",
    "desc": "The Kotlin Programming Language.",
    "star_num": 45671,
    "fork_num": 5649,
    "snippets": [
      "// !DIAGNOSTICS: -UNUSED_PARAMETER\n\nobject Delegate {\n    operator fun getValue(x: Any?, y: Any?): String = \"\"\n}\n\nfun <T> delegateFactory(p: Any) = Delegate\n\nclass C(p: Any, val v: Any) {\n\n    val test1 get() = <!UNRESOLVED_REFERENCE!>p<!>\n\n    val test2 get() = v\n\n    // NB here we can use both 'T' (property type parameter) and 'p' (primary constructor parameter)\n    val <T> List<T>.test3 by delegateFactory<T>(p)\n\n    val test4 get() { return <!UNRESOLVED_REFERENCE!>p<!> }\n\n    var test5\n        get() { return <!UNRESOLVED_REFERENCE!>p<!> }\n        set(nv) { <!UNRESOLVED_REFERENCE!>p<!>.let {} }\n}\n",
      "// LOOK_UP_FOR_ELEMENT_OF_TYPE: org.jetbrains.kotlin.psi.KtParameter\n\ndata class X(val a: Int, val b: Int)\n\nfun x(action: (X, Int) -> Unit) {}\n\nfun main() {\n    x { <expr>(a, b)</expr>, i ->\n\n    }\n}",
      "}",
      "fun interface MyRunnable {\n    fun foo(x: Int): Boolean\n}\n\nfun foo(m: MyRunnable) {}\n\nprivate fun interface PrivateRunnable {\n    fun bar(x: String): Boolean\n}\n\nprivate fun bar(pr: PrivateRunnable) {}\n\nfun main() {\n    foo(MyRunnable { x ->\n        x > 1\n    })\n\n    foo(MyRunnable({ it > 1 }))\n\n    val x = { x: Int -> x > 1 }\n\n    foo(MyRunnable(x))\n\n    bar(PrivateRunnable { s -> s.length > 0 })\n}\n",
      "    val isExpect: Boolean get() = IrFlags.IS_EXPECT_FUNCTION.get(flags.toInt())\n    val isFakeOverride: Boolean get() = kind() == CallableMemberDescriptor.Kind.FAKE_OVERRIDE\n\n    val isPrimary: Boolean get() = IrFlags.IS_PRIMARY.get(flags.toInt())\n\n    private fun kind(): CallableMemberDescriptor.Kind = ProtoEnumFlags.memberKind(IrFlags.MEMBER_KIND.get(flags.toInt()))\n\n    companion object {\n        fun encode(function: IrSimpleFunction): Long {\n            function.run {\n                val hasAnnotation = annotations.isNotEmpty()\n                val visibility = ProtoEnumFlags.descriptorVisibility(visibility.normalize())\n                val modality = ProtoEnumFlags.modality(modality)\n                val kind = if (isFakeOverride) ProtoBuf.MemberKind.FAKE_OVERRIDE else ProtoBuf.MemberKind.DECLARATION\n\n                val flags = IrFlags.getFunctionFlags(\n                    hasAnnotation, visibility, modality, kind,\n                    isOperator, isInfix, isInline, isTailrec, isExternal, isSuspend, isExpect,\n                    true // hasStableParameterNames does not make sense for Ir, just pass the default value\n                )\n\n                return flags.toLong()\n            }\n        }\n\n        fun encode(constructor: IrConstructor): Long {\n            constructor.run {\n                val hasAnnotation = annotations.isNotEmpty()\n                val visibility = ProtoEnumFlags.descriptorVisibility(visibility.normalize())\n                val flags = IrFlags.getConstructorFlags(hasAnnotation, visibility, isInline, isExternal, isExpect, isPrimary)\n\n                return flags.toLong()\n            }\n        }\n\n        fun decode(code: Long) = FunctionFlags(code)\n    }\n}\n\n@JvmInline\nvalue class PropertyFlags(val flags: Long) {\n\n    val modality: Modality get() = ProtoEnumFlags.modality(IrFlags.MODALITY.get(flags.toInt()))\n    val visibility: DescriptorVisibility get() = ProtoEnumFlags.descriptorVisibility(IrFlags.VISIBILITY.get(flags.toInt()))\n\n    val isVar: Boolean get() = IrFlags.IS_VAR.get(flags.toInt())\n    val isConst: Boolean get() = IrFlags.IS_CONST.get(flags.toInt())\n    val isLateinit: Boolean get() = IrFlags.IS_LATEINIT.get(flags.toInt())\n    val isExternal: Boolean get() = IrFlags.IS_EXTERNAL_PROPERTY.get(flags.toInt())\n    val isDelegated: Boolean get() = IrFlags.IS_DELEGATED.get(flags.toInt())\n    val isExpect: Boolean get() = IrFlags.IS_EXPECT_PROPERTY.get(flags.toInt())\n    val isFakeOverride: Boolean get() = kind() == CallableMemberDescriptor.Kind.FAKE_OVERRIDE\n\n    private fun kind(): CallableMemberDescriptor.Kind = ProtoEnumFlags.memberKind(IrFlags.MEMBER_KIND.get(flags.toInt()))\n\n    companion object {\n        fun encode(property: IrProperty): Long {\n            return property.run {\n                val hasAnnotation = annotations.isNotEmpty()\n                val visibility = ProtoEnumFlags.descriptorVisibility(visibility.normalize())\n                val modality = ProtoEnumFlags.modality(modality)\n                val kind = if (isFakeOverride) ProtoBuf.MemberKind.FAKE_OVERRIDE else ProtoBuf.MemberKind.DECLARATION\n                val hasGetter = getter != null\n                val hasSetter = setter != null",
      "}",
      " * Use of this source code is governed by the Apache 2.0 license that can be found in the license/LICENSE.txt file.\n */\n\npackage org.jetbrains.kotlin.fir.java.scopes\n\nimport org.jetbrains.kotlin.fir.FirSession\nimport org.jetbrains.kotlin.fir.declarations.FirCallableDeclaration\nimport org.jetbrains.kotlin.fir.declarations.FirDeclarationOrigin\nimport org.jetbrains.kotlin.fir.declarations.FirSimpleFunction\nimport org.jetbrains.kotlin.fir.initialSignatureAttr\nimport org.jetbrains.kotlin.fir.java.enhancement.FirSignatureEnhancement\nimport org.jetbrains.kotlin.fir.resolve.substitution.ConeSubstitutor\nimport org.jetbrains.kotlin.fir.scopes.FirTypeScope\nimport org.jetbrains.kotlin.fir.scopes.ProcessorAction\nimport org.jetbrains.kotlin.fir.scopes.getDirectOverriddenMembers\nimport org.jetbrains.kotlin.fir.scopes.getDirectOverriddenProperties\nimport org.jetbrains.kotlin.fir.symbols.impl.*\nimport org.jetbrains.kotlin.name.Name\n\nclass JavaClassMembersEnhancementScope(\n    session: FirSession,\n    private val owner: FirRegularClassSymbol,\n    private val useSiteMemberScope: JavaClassUseSiteMemberScope,\n) : FirTypeScope() {\n    private val enhancedToOriginalFunctions = mutableMapOf<FirNamedFunctionSymbol, FirNamedFunctionSymbol>()\n    private val enhancedToOriginalProperties = mutableMapOf<FirPropertySymbol, FirPropertySymbol>()\n\n    private val signatureEnhancement = FirSignatureEnhancement(owner.fir, session) {\n        overriddenMembers()\n    }\n\n    override fun processPropertiesByName(name: Name, processor: (FirVariableSymbol<*>) -> Unit) {\n        useSiteMemberScope.processPropertiesByName(name) process@{ original ->\n            val enhancedPropertySymbol = signatureEnhancement.enhancedProperty(original, name)\n            if (original is FirPropertySymbol && enhancedPropertySymbol is FirPropertySymbol) {\n                enhancedToOriginalProperties[enhancedPropertySymbol] = original\n            }\n\n            processor(enhancedPropertySymbol)\n        }\n\n        return super.processPropertiesByName(name, processor)\n    }\n\n    override fun processFunctionsByName(name: Name, processor: (FirNamedFunctionSymbol) -> Unit) {\n        useSiteMemberScope.processFunctionsByName(name) process@{ original ->\n            val symbol = signatureEnhancement.enhancedFunction(original, name)\n            val enhancedFunction = (symbol.fir as? FirSimpleFunction)\n            val enhancedFunctionSymbol = enhancedFunction?.symbol ?: symbol\n\n            if (enhancedFunctionSymbol is FirNamedFunctionSymbol) {\n                enhancedToOriginalFunctions[enhancedFunctionSymbol] = original\n                processor(enhancedFunctionSymbol)\n            }\n        }\n\n        return super.processFunctionsByName(name, processor)\n    }\n\n    private fun FirCallableDeclaration.overriddenMembers(): List<FirCallableDeclaration> {\n        return when (val symbol = this.symbol) {\n            is FirNamedFunctionSymbol -> useSiteMemberScope.getDirectOverriddenMembers(symbol)\n            is FirPropertySymbol -> useSiteMemberScope.getDirectOverriddenProperties(symbol)\n            else -> emptyList()",
      "        fun run() {\n            <!UNRESOLVED_REFERENCE!>p<!>.<!DEBUG_INFO_MISSING_UNRESOLVED, VARIABLE_EXPECTED!>x<!> = 43\n        }\n    }\n}",
      "// FIR_IDENTICAL\nopen class C {\n}\n\nfun C.foo() {}\n\nopen class X {\n    companion object : C() {}\n}\n\nopen class Y {\n    companion object : C() {}\n}\n\nfun bar() {\n    val x = X\n    x.foo()\n    X.foo()\n    (X as C).foo()\n    ((if (1<2) X else Y) <!USELESS_CAST!>as C<!>).foo()\n}\n",
      "// FIR_IDENTICAL\n// ISSUE: KT-57858\n@file:Suppress(\"INVISIBLE_MEMBER\", \"INVISIBLE_REFERENCE\")\n\nimport kotlin.internal.PlatformDependent\n\ninterface I {\n    @PlatformDependent\n    fun f() {}\n}\n\nclass C : I {\n    fun <!VIRTUAL_MEMBER_HIDDEN!>f<!>() {}\n}"
    ]
  },
  {
    "id": "coreos/etcd",
    "org": "coreos",
    "avatarURL": "https://avatars.githubusercontent.com/u/41972792?v=4",
    "name": "coreos/etcd",
    "url": "https://github.com/etcd-io/etcd",
    "lang": "Go",
    "desc": "Distributed reliable key-value store for the most critical data of a distributed system.",
    "star_num": 44400,
    "fork_num": 9470,
    "snippets": [
      "\t}\n\n\tlimit := int(ro.Limit)\n\tif limit <= 0 || limit > len(revpairs) {\n\t\tlimit = len(revpairs)\n\t}\n\n\tkvs := make([]mvccpb.KeyValue, limit)\n\trevBytes := newRevBytes()\n\tfor i, revpair := range revpairs[:len(kvs)] {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn nil, fmt.Errorf(\"rangeKeys: context cancelled: %w\", ctx.Err())\n\t\tdefault:\n\t\t}\n\t\trevToBytes(revpair, revBytes)\n\t\t_, vs := tr.tx.UnsafeRange(schema.Key, revBytes, nil, 0)\n\t\tif len(vs) != 1 {\n\t\t\ttr.s.lg.Fatal(\n\t\t\t\t\"range failed to find revision pair\",\n\t\t\t\tzap.Int64(\"revision-main\", revpair.main),\n\t\t\t\tzap.Int64(\"revision-sub\", revpair.sub),\n\t\t\t\tzap.Int64(\"revision-current\", curRev),\n\t\t\t\tzap.Int64(\"range-option-rev\", ro.Rev),\n\t\t\t\tzap.Int64(\"range-option-limit\", ro.Limit),\n\t\t\t\tzap.Binary(\"key\", key),\n\t\t\t\tzap.Binary(\"end\", end),\n\t\t\t\tzap.Int(\"len-revpairs\", len(revpairs)),\n\t\t\t\tzap.Int(\"len-values\", len(vs)),\n\t\t\t)\n\t\t}\n\t\tif err := kvs[i].Unmarshal(vs[0]); err != nil {\n\t\t\ttr.s.lg.Fatal(\n\t\t\t\t\"failed to unmarshal mvccpb.KeyValue\",\n\t\t\t\tzap.Error(err),\n\t\t\t)\n\t\t}\n\t}\n\ttr.trace.Step(\"range keys from bolt db\")\n\treturn &RangeResult{KVs: kvs, Count: total, Rev: curRev}, nil\n}\n\nfunc (tr *storeTxnRead) End() {\n\ttr.tx.RUnlock() // RUnlock signals the end of concurrentReadTx.\n\ttr.s.mu.RUnlock()\n}\n\ntype storeTxnWrite struct {\n\tstoreTxnCommon\n\ttx backend.BatchTx\n\t// beginRev is the revision where the txn begins; it will write to the next revision.\n\tbeginRev int64\n\tchanges  []mvccpb.KeyValue\n}\n\nfunc (s *store) Write(trace *traceutil.Trace) TxnWrite {\n\ts.mu.RLock()\n\ttx := s.b.BatchTx()\n\ttx.LockInsideApply()\n\ttw := &storeTxnWrite{\n\t\tstoreTxnCommon: storeTxnCommon{s, tx, 0, 0, trace},\n\t\ttx:             tx,\n\t\tbeginRev:       s.currentRev,\n\t\tchanges:        make([]mvccpb.KeyValue, 0, 4),",
      "\t\t\t\t\treturn ErrIntOverflowMembership\n\t\t\t\t}\n\t\t\t\tif iNdEx >= l {\n\t\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t\t}\n\t\t\t\tb := dAtA[iNdEx]\n\t\t\t\tiNdEx++\n\t\t\t\tstringLen |= uint64(b&0x7F) << shift\n\t\t\t\tif b < 0x80 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tintStringLen := int(stringLen)\n\t\t\tif intStringLen < 0 {\n\t\t\t\treturn ErrInvalidLengthMembership\n\t\t\t}\n\t\t\tpostIndex := iNdEx + intStringLen\n\t\t\tif postIndex < 0 {\n\t\t\t\treturn ErrInvalidLengthMembership\n\t\t\t}\n\t\t\tif postIndex > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tm.Ver = string(dAtA[iNdEx:postIndex])\n\t\t\tiNdEx = postIndex\n\t\tdefault:\n\t\t\tiNdEx = preIndex\n\t\t\tskippy, err := skipMembership(dAtA[iNdEx:])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif (skippy < 0) || (iNdEx+skippy) < 0 {\n\t\t\t\treturn ErrInvalidLengthMembership\n\t\t\t}\n\t\t\tif (iNdEx + skippy) > l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tm.XXX_unrecognized = append(m.XXX_unrecognized, dAtA[iNdEx:iNdEx+skippy]...)\n\t\t\tiNdEx += skippy\n\t\t}\n\t}\n\n\tif iNdEx > l {\n\t\treturn io.ErrUnexpectedEOF\n\t}\n\treturn nil\n}\nfunc (m *ClusterMemberAttrSetRequest) Unmarshal(dAtA []byte) error {\n\tl := len(dAtA)\n\tiNdEx := 0\n\tfor iNdEx < l {\n\t\tpreIndex := iNdEx\n\t\tvar wire uint64\n\t\tfor shift := uint(0); ; shift += 7 {\n\t\t\tif shift >= 64 {\n\t\t\t\treturn ErrIntOverflowMembership\n\t\t\t}\n\t\t\tif iNdEx >= l {\n\t\t\t\treturn io.ErrUnexpectedEOF\n\t\t\t}\n\t\t\tb := dAtA[iNdEx]\n\t\t\tiNdEx++\n\t\t\twire |= uint64(b&0x7F) << shift\n\t\t\tif b < 0x80 {",
      "\t\t\tif _, err = cli.UserAdd(context.TODO(), \"root\", \"123\"); err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\t\t\tif _, err = cli.UserGrantRole(context.TODO(), \"root\", \"root\"); err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\n\t\t\tif _, err = cli.RoleAdd(context.TODO(), \"r\"); err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\n\t\t\tif _, err = cli.RoleGrantPermission(\n\t\t\t\tcontext.TODO(),\n\t\t\t\t\"r\",   // role name\n\t\t\t\t\"foo\", // key\n\t\t\t\t\"zoo\", // range end\n\t\t\t\tclientv3.PermissionType(clientv3.PermReadWrite),\n\t\t\t); err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\t\t\tif _, err = cli.UserAdd(context.TODO(), \"u\", \"123\"); err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\t\t\tif _, err = cli.UserGrantRole(context.TODO(), \"u\", \"r\"); err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\t\t\tif _, err = cli.AuthEnable(context.TODO()); err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\n\t\t\tcliAuth, err := clientv3.New(clientv3.Config{\n\t\t\t\tEndpoints:   exampleEndpoints(),\n\t\t\t\tDialTimeout: dialTimeout,\n\t\t\t\tUsername:    \"u\",\n\t\t\t\tPassword:    \"123\",\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\t\t\tdefer cliAuth.Close()\n\n\t\t\tif _, err = cliAuth.Put(context.TODO(), \"foo1\", \"bar\"); err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\n\t\t\t_, err = cliAuth.Txn(context.TODO()).\n\t\t\t\tIf(clientv3.Compare(clientv3.Value(\"zoo1\"), \">\", \"abc\")).\n\t\t\t\tThen(clientv3.OpPut(\"zoo1\", \"XYZ\")).\n\t\t\t\tElse(clientv3.OpPut(\"zoo1\", \"ABC\")).\n\t\t\t\tCommit()\n\t\t\tfmt.Println(err)\n\n\t\t\t// now check the permission with the root account\n\t\t\trootCli, err := clientv3.New(clientv3.Config{\n\t\t\t\tEndpoints:   exampleEndpoints(),\n\t\t\t\tDialTimeout: dialTimeout,\n\t\t\t\tUsername:    \"root\",\n\t\t\t\tPassword:    \"123\",\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\t\t\tdefer rootCli.Close()\n",
      "\t\"time\"\n\n\t\"github.com/anishathalye/porcupine\"\n\t\"github.com/google/go-cmp/cmp\"\n\t\"go.uber.org/zap\"\n\n\t\"go.etcd.io/etcd/tests/v3/robustness/model\"\n)\n\nfunc validateOperationsAndVisualize(t *testing.T, lg *zap.Logger, operations []porcupine.Operation, eventHistory []model.WatchEvent) (visualize func(basepath string) error) {\n\tconst timeout = 5 * time.Minute\n\tlg.Info(\"Validating linearizable operations\", zap.Duration(\"timeout\", timeout))\n\tresult, visualize := validateLinearizableOperationAndVisualize(lg, operations, timeout)\n\tswitch result {\n\tcase porcupine.Illegal:\n\t\tt.Error(\"Linearization failed\")\n\t\treturn\n\tcase porcupine.Unknown:\n\t\tt.Error(\"Linearization has timed out\")\n\t\treturn\n\tcase porcupine.Ok:\n\t\tt.Log(\"Linearization success\")\n\tdefault:\n\t\tt.Fatalf(\"Unknown Linearization\")\n\t}\n\tlg.Info(\"Validating serializable operations\")\n\tvalidateSerializableOperations(t, operations, eventHistory)\n\treturn visualize\n}\n\nfunc validateLinearizableOperationAndVisualize(lg *zap.Logger, operations []porcupine.Operation, timeout time.Duration) (result porcupine.CheckResult, visualize func(basepath string) error) {\n\tlinearizable, info := porcupine.CheckOperationsVerbose(model.NonDeterministicModel, operations, timeout)\n\treturn linearizable, func(path string) error {\n\t\tlg.Info(\"Saving visualization\", zap.String(\"path\", path))\n\t\terr := porcupine.VisualizePath(model.NonDeterministicModel, info, path)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to visualize, err: %v\", err)\n\t\t}\n\t\treturn nil\n\t}\n}\n\nfunc validateSerializableOperations(t *testing.T, operations []porcupine.Operation, totalEventHistory []model.WatchEvent) {\n\tstaleReads := filterSerializableReads(operations)\n\tif len(staleReads) == 0 {\n\t\treturn\n\t}\n\tsort.Slice(staleReads, func(i, j int) bool {\n\t\treturn staleReads[i].Input.(model.EtcdRequest).Range.Revision < staleReads[j].Input.(model.EtcdRequest).Range.Revision\n\t})\n\treplay := model.NewReplay(totalEventHistory)\n\tfor _, read := range staleReads {\n\t\trequest := read.Input.(model.EtcdRequest)\n\t\tresponse := read.Output.(model.MaybeEtcdResponse)\n\t\tvalidateSerializableOperation(t, replay, request, response)\n\t}\n}\n\nfunc filterSerializableReads(operations []porcupine.Operation) []porcupine.Operation {\n\tresp := []porcupine.Operation{}\n\tfor _, op := range operations {\n\t\trequest := op.Input.(model.EtcdRequest)\n\t\tif request.Type == model.Range && request.Range.Revision != 0 {\n\t\t\tresp = append(resp, op)",
      "// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage adapter\n\nimport (\n\t\"context\"\n\n\t\"go.etcd.io/etcd/server/v3/etcdserver/api/v3election/v3electionpb\"\n\n\t\"google.golang.org/grpc\"\n)\n\ntype es2ec struct{ es v3electionpb.ElectionServer }\n\nfunc ElectionServerToElectionClient(es v3electionpb.ElectionServer) v3electionpb.ElectionClient {\n\treturn &es2ec{es}\n}\n\nfunc (s *es2ec) Campaign(ctx context.Context, r *v3electionpb.CampaignRequest, opts ...grpc.CallOption) (*v3electionpb.CampaignResponse, error) {\n\treturn s.es.Campaign(ctx, r)\n}\n\nfunc (s *es2ec) Proclaim(ctx context.Context, r *v3electionpb.ProclaimRequest, opts ...grpc.CallOption) (*v3electionpb.ProclaimResponse, error) {\n\treturn s.es.Proclaim(ctx, r)\n}\n\nfunc (s *es2ec) Leader(ctx context.Context, r *v3electionpb.LeaderRequest, opts ...grpc.CallOption) (*v3electionpb.LeaderResponse, error) {\n\treturn s.es.Leader(ctx, r)\n}\n\nfunc (s *es2ec) Resign(ctx context.Context, r *v3electionpb.ResignRequest, opts ...grpc.CallOption) (*v3electionpb.ResignResponse, error) {\n\treturn s.es.Resign(ctx, r)\n}\n\nfunc (s *es2ec) Observe(ctx context.Context, in *v3electionpb.LeaderRequest, opts ...grpc.CallOption) (v3electionpb.Election_ObserveClient, error) {\n\tcs := newPipeStream(ctx, func(ss chanServerStream) error {\n\t\treturn s.es.Observe(in, &es2ecServerStream{ss})\n\t})\n\treturn &es2ecClientStream{cs}, nil\n}\n\n// es2ecClientStream implements Election_ObserveClient\ntype es2ecClientStream struct{ chanClientStream }\n\n// es2ecServerStream implements Election_ObserveServer\ntype es2ecServerStream struct{ chanServerStream }\n\nfunc (s *es2ecClientStream) Send(rr *v3electionpb.LeaderRequest) error {\n\treturn s.SendMsg(rr)\n}\nfunc (s *es2ecClientStream) Recv() (*v3electionpb.LeaderResponse, error) {\n\tvar v interface{}\n\tif err := s.RecvMsg(&v); err != nil {\n\t\treturn nil, err\n\t}",
      "\tExitServerError       = 4\n\tExitClusterNotHealthy = 5\n)\n\nfunc ExitWithError(code int, err error) {\n\tfmt.Fprintln(os.Stderr, \"Error:\", err)\n\tos.Exit(code)\n}\n",
      "\titemSet map[LeaseItem]struct{}\n\trevokec chan struct{}\n}\n\nfunc NewLease(id LeaseID, ttl int64) *Lease {\n\treturn &Lease{\n\t\tID:      id,\n\t\tttl:     ttl,\n\t\titemSet: make(map[LeaseItem]struct{}),\n\t\trevokec: make(chan struct{}),\n\t}\n}\n\nfunc (l *Lease) expired() bool {\n\treturn l.Remaining() <= 0\n}\n\nfunc (l *Lease) persistTo(b backend.Backend) {\n\tlpb := leasepb.Lease{ID: int64(l.ID), TTL: l.ttl, RemainingTTL: l.remainingTTL}\n\ttx := b.BatchTx()\n\ttx.LockInsideApply()\n\tdefer tx.Unlock()\n\tschema.MustUnsafePutLease(tx, &lpb)\n}\n\n// TTL returns the TTL of the Lease.\nfunc (l *Lease) TTL() int64 {\n\treturn l.ttl\n}\n\n// SetLeaseItem sets the given lease item, this func is thread-safe\nfunc (l *Lease) SetLeaseItem(item LeaseItem) {\n\tl.mu.Lock()\n\tdefer l.mu.Unlock()\n\tl.itemSet[item] = struct{}{}\n}\n\n// getRemainingTTL returns the last checkpointed remaining TTL of the lease.\nfunc (l *Lease) getRemainingTTL() int64 {\n\tif l.remainingTTL > 0 {\n\t\treturn l.remainingTTL\n\t}\n\treturn l.ttl\n}\n\n// refresh refreshes the expiry of the lease.\nfunc (l *Lease) refresh(extend time.Duration) {\n\tnewExpiry := time.Now().Add(extend + time.Duration(l.getRemainingTTL())*time.Second)\n\tl.expiryMu.Lock()\n\tdefer l.expiryMu.Unlock()\n\tl.expiry = newExpiry\n}\n\n// forever sets the expiry of lease to be forever.\nfunc (l *Lease) forever() {\n\tl.expiryMu.Lock()\n\tdefer l.expiryMu.Unlock()\n\tl.expiry = forever\n}\n\n// Keys returns all the keys attached to the lease.\nfunc (l *Lease) Keys() []string {\n\tl.mu.RLock()\n\tkeys := make([]string, 0, len(l.itemSet))",
      "\nfunc setReusePort(network, address string, c syscall.RawConn) error {\n\treturn errors.New(\"port reuse is not supported on Windows\")\n}\n\n// Windows supports SO_REUSEADDR, but it may cause undefined behavior, as\n// there is no protection against port hijacking.\nfunc setReuseAddress(network, addr string, conn syscall.RawConn) error {\n\treturn errors.New(\"address reuse is not supported on Windows\")\n}\n",
      "\t\tforce          bool\n\n\t\texpectLogsSubString  string\n\t\texpectStorageVersion *semver.Version\n\t}{\n\t\t{\n\t\t\tname:                 \"Invalid target version string\",\n\t\t\ttargetVersion:        \"abc\",\n\t\t\texpectLogsSubString:  `Error: wrong target version format, expected \"X.Y\", got \"abc\"`,\n\t\t\texpectStorageVersion: &version.V3_6,\n\t\t},\n\t\t{\n\t\t\tname:                 \"Invalid target version\",\n\t\t\ttargetVersion:        \"3.a\",\n\t\t\texpectLogsSubString:  `Error: failed to parse target version: strconv.ParseInt: parsing \"a\": invalid syntax`,\n\t\t\texpectStorageVersion: &version.V3_6,\n\t\t},\n\t\t{\n\t\t\tname:                 \"Target with only major version is invalid\",\n\t\t\ttargetVersion:        \"3\",\n\t\t\texpectLogsSubString:  `Error: wrong target version format, expected \"X.Y\", got \"3\"`,\n\t\t\texpectStorageVersion: &version.V3_6,\n\t\t},\n\t\t{\n\t\t\tname:                 \"Target with patch version is invalid\",\n\t\t\ttargetVersion:        \"3.6.0\",\n\t\t\texpectLogsSubString:  `Error: wrong target version format, expected \"X.Y\", got \"3.6.0\"`,\n\t\t\texpectStorageVersion: &version.V3_6,\n\t\t},\n\t\t{\n\t\t\tname:                \"Migrate v3.5 to v3.5 is no-op\",\n\t\t\tclusterVersion:      e2e.LastVersion,\n\t\t\ttargetVersion:       \"3.5\",\n\t\t\texpectLogsSubString: \"storage version up-to-date\\t\" + `{\"storage-version\": \"3.5\"}`,\n\t\t},\n\t\t{\n\t\t\tname:                 \"Upgrade v3.5 to v3.6 should work\",\n\t\t\tclusterVersion:       e2e.LastVersion,\n\t\t\ttargetVersion:        \"3.6\",\n\t\t\texpectStorageVersion: &version.V3_6,\n\t\t},\n\t\t{\n\t\t\tname:                 \"Migrate v3.6 to v3.6 is no-op\",\n\t\t\ttargetVersion:        \"3.6\",\n\t\t\texpectLogsSubString:  \"storage version up-to-date\\t\" + `{\"storage-version\": \"3.6\"}`,\n\t\t\texpectStorageVersion: &version.V3_6,\n\t\t},\n\t\t{\n\t\t\tname:                 \"Downgrade v3.6 to v3.5 should fail until it's implemented\",\n\t\t\ttargetVersion:        \"3.5\",\n\t\t\texpectLogsSubString:  \"cannot downgrade storage, WAL contains newer entries\",\n\t\t\texpectStorageVersion: &version.V3_6,\n\t\t},\n\t\t{\n\t\t\tname:                \"Downgrade v3.6 to v3.5 with force should work\",\n\t\t\ttargetVersion:       \"3.5\",\n\t\t\tforce:               true,\n\t\t\texpectLogsSubString: \"forcefully cleared storage version\",\n\t\t},\n\t\t{\n\t\t\tname:                 \"Upgrade v3.6 to v3.7 with force should work\",\n\t\t\ttargetVersion:        \"3.7\",\n\t\t\tforce:                true,\n\t\t\texpectLogsSubString:  \"forcefully set storage version\\t\" + `{\"storage-version\": \"3.7\"}`,"
    ]
  },
  {
    "id": "JuliaLang/julia",
    "org": "JuliaLang",
    "avatarURL": "https://avatars.githubusercontent.com/u/743164?v=4",
    "name": "JuliaLang/julia",
    "url": "https://github.com/JuliaLang/julia",
    "lang": "Julia",
    "desc": "The Julia Language: A fresh approach to technical computing.",
    "star_num": 43095,
    "fork_num": 5276,
    "snippets": [
      "        c in chars && (result = true; continue)\n        result = false; break\n    end\n    !(result && eat) && seek(io, start)\n    return result\nend\n\nblankline(io::IO; eat = true) =\n    linecontains(io, \"\",\n                 allow_whitespace = true,\n                 allowempty = true,\n                 eat = eat)\n\n\"\"\"\nTest if the stream starts with the given string.\n`eat` specifies whether to advance on success (true by default).\n`padding` specifies whether leading whitespace should be ignored.\n\"\"\"\nfunction startswith(stream::IO, s::AbstractString; eat = true, padding = false, newlines = true)\n    start = position(stream)\n    padding && skipwhitespace(stream, newlines = newlines)\n    result = true\n    for char in s\n        !eof(stream) && read(stream, Char) == char ||\n            (result = false; break)\n    end\n    !(result && eat) && seek(stream, start)\n    return result\nend\n\nfunction startswith(stream::IO, c::AbstractChar; eat = true)\n    if !eof(stream) && peek(stream) == UInt8(c)\n        eat && read(stream, Char)\n        return true\n    else\n        return false\n    end\nend\n\nfunction startswith(stream::IO, ss::Vector{<:AbstractString}; kws...)\n    any(s->startswith(stream, s; kws...), ss)\nend\n\nfunction startswith(stream::IO, r::Regex; eat = true, padding = false)\n    @assert Base.startswith(r.pattern, \"^\")\n    start = position(stream)\n    padding && skipwhitespace(stream)\n    line = readline(stream)\n    seek(stream, start)\n    m = match(r, line)\n    m === nothing && return \"\"\n    eat && @dotimes length(m.match) read(stream, Char)\n    return m.match\nend\n\n\"\"\"\nExecutes the block of code, and if the return value is `nothing`,\nreturns the stream to its initial position.\n\"\"\"\nfunction withstream(f, stream)\n    pos = position(stream)\n    result = f()\n    (result ≡ nothing || result ≡ false) && seek(stream, pos)\n    return result",
      "\n@test let ct = current_task()\n    yieldto(@task yieldto(ct, Base.source_path()))\nend === \"\"\n\n@test let ct = current_task()\n    yieldto(@task schedule(ct, Base.source_path()))\nend === \"\"\n\n@test let ct = current_task(),\n    t = @task Base.source_path()\n    schedule(ct)\n    yieldto(t)\n    fetch(t)\nend === \"\"\n\n@test @__FILE__() == path\n",
      "# Write the sys source cache in format readable by Base._read_dependency_src\ncachefile = ARGS[1]\nopen(cachefile, \"w\") do io\n    for (_, filename) in Base._included_files\n        src = read(filename, String)\n        write(io, Int32(sizeof(filename)))\n        write(io, filename)\n        write(io, UInt64(sizeof(src)))\n        write(io, src)\n    end\n    write(io, Int32(0))\nend\n",
      "end\n\nfunction map!(f, iter::ValueIterator{<:Dict})\n    dict = iter.dict\n    vals = dict.vals\n    # @inbounds is here so that it gets propagated to isslotfilled\n    @inbounds for i = dict.idxfloor:lastindex(vals)\n        if isslotfilled(dict, i)\n            vals[i] = f(vals[i])\n        end\n    end\n    return iter\nend\n\nfunction mergewith!(combine, d1::Dict{K, V}, d2::AbstractDict) where {K, V}\n    haslength(d2) && sizehint!(d1, length(d1) + length(d2))\n    for (k, v) in d2\n        i, sh = ht_keyindex2_shorthash!(d1, k)\n        if i > 0\n            d1.vals[i] = combine(d1.vals[i], v)\n        else\n            if !(k isa K)\n                k1 = convert(K, k)::K\n                if !isequal(k, k1)\n                    throw(ArgumentError(\"$(limitrepr(k)) is not a valid key for type $K\"))\n                end\n                k = k1\n            end\n            if !isa(v, V)\n                v = convert(V, v)::V\n            end\n            @inbounds _setindex!(d1, v, k, -i, sh)\n        end\n    end\n    return d1\nend\n\nstruct ImmutableDict{K,V} <: AbstractDict{K,V}\n    parent::ImmutableDict{K,V}\n    key::K\n    value::V\n    ImmutableDict{K,V}() where {K,V} = new() # represents an empty dictionary\n    ImmutableDict{K,V}(key, value) where {K,V} = (empty = new(); new(empty, key, value))\n    ImmutableDict{K,V}(parent::ImmutableDict, key, value) where {K,V} = new(parent, key, value)\nend\n\n\"\"\"\n    ImmutableDict\n\n`ImmutableDict` is a dictionary implemented as an immutable linked list,\nwhich is optimal for small dictionaries that are constructed over many individual insertions.\nNote that it is not possible to remove a value, although it can be partially overridden and hidden\nby inserting a new value with the same key.\n\n    ImmutableDict(KV::Pair)\n\nCreate a new entry in the `ImmutableDict` for a `key => value` pair\n\n - use `(key => value) in dict` to see if this particular combination is in the properties set\n - use `get(dict, key, default)` to retrieve the most recent value for a particular key\n\n\"\"\"\nImmutableDict\nImmutableDict(KV::Pair{K,V}) where {K,V} = ImmutableDict{K,V}(KV[1], KV[2])",
      "\n## Tests for the abstract array interfaces and operations with arrays\n## with different indexing rules\n\n# A custom linear fast array type with 24 elements that doesn't rely upon Array storage\nmutable struct T24Linear{T,N,dims} <: AbstractArray{T,N}\n    v1::T;  v2::T;  v3::T;  v4::T;  v5::T;  v6::T;  v7::T;  v8::T\n    v9::T;  v10::T; v11::T; v12::T; v13::T; v14::T; v15::T; v16::T\n    v17::T; v18::T; v19::T; v20::T; v21::T; v22::T; v23::T; v24::T\n    T24Linear{T,N,d}() where {T,N,d} =\n        (prod(d) == 24 || throw(DimensionMismatch(\"T24Linear must have 24 elements\")); new())\n    function T24Linear{T,N,d}(v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,\n                              v13,v14,v15,v16,v17,v18,v19,v20,v21,v22,v23,v24) where {T,N,d}\n        prod(d) == 24 || throw(DimensionMismatch(\"T24Linear must have 24 elements\"))\n        new(v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16,v17,v18,v19,v20,v21,v22,v23,v24)\n    end\nend\n\nT24Linear(::Type{T}, dims::Int...) where T = T24Linear(T, dims)\nT24Linear(::Type{T}, dims::NTuple{N,Int}) where {T,N} = T24Linear{T,N,dims}()\n\nT24Linear(     X::AbstractArray{T,N}) where {T,N  } = T24Linear{T,N}(X)\nT24Linear{T  }(X::AbstractArray{_,N}) where {T,N,_} = T24Linear{T,N}(X)\nT24Linear{T,N}(X::AbstractArray     ) where {T,N  } = T24Linear{T,N,size(X)}(X...)\n\nBase.size(::T24Linear{T,N,dims}) where {T,N,dims} = dims\nimport Base: IndexLinear\nBase.IndexStyle(::Type{A}) where {A<:T24Linear} = IndexLinear()\nBase.getindex(A::T24Linear, i::Int) = getfield(A, i)\nBase.setindex!(A::T24Linear{T}, v, i::Int) where {T} = setfield!(A, i, convert(T, v))\n\n# A custom linear slow sparse-like array that relies upon Dict for its storage\nstruct TSlow{T,N} <: AbstractArray{T,N}\n    data::Dict{NTuple{N,Int}, T}\n    dims::NTuple{N,Int}\nend\nTSlow(::Type{T}, dims::Int...) where {T} = TSlow(T, dims)\nTSlow(::Type{T}, dims::NTuple{N,Int}) where {T,N} = TSlow{T,N}(Dict{NTuple{N,Int}, T}(), dims)\n\nTSlow{T,N}(X::TSlow{T,N})         where {T,N  } = X\nTSlow(     X::AbstractArray{T,N}) where {T,N  } = TSlow{T,N}(X)\nTSlow{T  }(X::AbstractArray{_,N}) where {T,N,_} = TSlow{T,N}(X)\nTSlow{T,N}(X::AbstractArray     ) where {T,N  } = begin\n    A = TSlow(T, size(X))\n    for I in CartesianIndices(X)\n        A[Tuple(I)...] = X[Tuple(I)...]\n    end\n    A\nend\n\nBase.size(A::TSlow) = A.dims\nBase.similar(A::TSlow, ::Type{T}, dims::Dims) where {T} = TSlow(T, dims)\nBase.IndexStyle(::Type{A}) where {A<:TSlow} = IndexCartesian()\nBase.getindex(A::TSlow{T,N}, i::Vararg{Int,N}) where {T,N} = get(A.data, i, zero(T))\nBase.setindex!(A::TSlow{T,N}, v, i::Vararg{Int,N}) where {T,N} = (A.data[i] = v)\n\n# An array type that just passes through to the parent\nstruct WrapperArray{T,N,A<:AbstractArray{T,N}} <: AbstractArray{T,N}\n    parent::A\nend\nBase.IndexStyle(::Type{WrapperArray{T,N,A}}) where {T,N,A<:AbstractArray{T,N}} = IndexStyle(A)\nBase.parent(A::WrapperArray) = A.parent\nBase.size(A::WrapperArray) = size(A.parent)\nBase.axes(A::WrapperArray) = axes(A.parent)",
      "    `define_editor` was introduced in Julia 1.4.\n\"\"\"\nfunction define_editor(fn::Function, pattern; wait::Bool=false)\n    callback = function (cmd::Cmd, path::AbstractString, line::Integer, column::Integer)\n        editor_matches(pattern, cmd) || return false\n        editor = if !applicable(fn, cmd, path, line, column)\n            # Be backwards compatible with editors that did not define the newly added column argument\n            fn(cmd, path, line)\n        else\n            fn(cmd, path, line, column)\n        end\n        if editor isa Cmd\n            if wait\n                run(editor) # blocks while editor runs\n            else\n                run(pipeline(editor, stderr=stderr), wait=false)\n            end\n            return true\n        elseif editor isa Nothing\n            return false\n        end\n        @warn \"invalid editor value returned\" pattern=pattern editor=editor\n        return false\n    end\n    pushfirst!(EDITOR_CALLBACKS, callback)\nend\n\neditor_matches(p::Regex, cmd::Cmd) = occursin(p, shell_escape(cmd))\neditor_matches(p::String, cmd::Cmd) = p == splitext(basename(first(cmd)))[1]\neditor_matches(ps::AbstractArray, cmd::Cmd) = any(editor_matches(p, cmd) for p in ps)\n\nfunction define_default_editors()\n    # fallback: just call the editor with the path as argument\n    define_editor(r\".*\") do cmd, path, line, column\n        `$cmd $path`\n    end\n    # vim family\n    for (editors, wait) in [\n        [[\"vim\", \"vi\", \"nvim\", \"mvim\"], true],\n        [[r\"\\bgvim\"], false],\n    ]\n        define_editor(editors; wait) do cmd, path, line, column\n            cmd = line == 0 ? `$cmd $path` :\n                column == 0 ? `$cmd +$line $path` :\n                `$cmd \"+normal $(line)G$(column)|\" $path`\n        end\n    end\n    define_editor(\"nano\"; wait=true) do cmd, path, line, column\n        cmd = `$cmd +$line,$column $path`\n    end\n    # emacs (must check that emacs not running in -t/-nw\n    # before regex match for general emacs)\n    for (editors, wait) in [\n        [[r\"\\bemacs\"], false],\n        [[r\"\\bemacs\\b.*\\s(-nw|--no-window-system)\\b\",\n          r\"\\bemacsclient\\b.\\s*-(-?nw|t|-?tty)\\b\"], true],\n    ]\n        define_editor(editors; wait) do cmd, path, line, column\n            `$cmd +$line:$column $path`\n        end\n    end\n    # other editors\n    define_editor(\"gedit\") do cmd, path, line, column\n        `$cmd +$line:$column $path`",
      "# This file is a part of Julia. License is MIT: https://julialang.org/license\n\nusing Test, LinearAlgebra\nlet ambig = detect_ambiguities(LinearAlgebra; recursive=true)\n    @test isempty(ambig)\n    ambig = Set{Any}(((m1.sig, m2.sig) for (m1, m2) in ambig))\n    expect = []\n    good = true\n    while !isempty(ambig)\n        sigs = pop!(ambig)\n        i = findfirst(==(sigs), expect)\n        if i === nothing\n            println(stderr, \"push!(expect, (\", sigs[1], \", \", sigs[2], \"))\")\n            good = false\n            continue\n        end\n        deleteat!(expect, i)\n    end\n    @test isempty(expect)\n    @test good\nend\n",
      "xor(a::Missing, b::Bool) = missing\nxor(b::Bool, a::Missing) = missing\nxor(::Missing, ::Integer) = missing\nxor(::Integer, ::Missing) = missing\n\n*(d::Missing, x::Union{AbstractString,AbstractChar}) = missing\n*(d::Union{AbstractString,AbstractChar}, x::Missing) = missing\n\nfunction float(A::AbstractArray{Union{T, Missing}}) where {T}\n    U = typeof(float(zero(T)))\n    convert(AbstractArray{Union{U, Missing}}, A)\nend\nfloat(A::AbstractArray{Missing}) = A\n\n\"\"\"\n    skipmissing(itr)\n\nReturn an iterator over the elements in `itr` skipping [`missing`](@ref) values.\nThe returned object can be indexed using indices of `itr` if the latter is indexable.\nIndices corresponding to missing values are not valid: they are skipped by [`keys`](@ref)\nand [`eachindex`](@ref), and a `MissingException` is thrown when trying to use them.\n\nUse [`collect`](@ref) to obtain an `Array` containing the non-`missing` values in\n`itr`. Note that even if `itr` is a multidimensional array, the result will always\nbe a `Vector` since it is not possible to remove missings while preserving dimensions\nof the input.\n\nSee also [`coalesce`](@ref), [`ismissing`](@ref), [`something`](@ref).\n\n# Examples\n```jldoctest\njulia> x = skipmissing([1, missing, 2])\nskipmissing(Union{Missing, Int64}[1, missing, 2])\n\njulia> sum(x)\n3\n\njulia> x[1]\n1\n\njulia> x[2]\nERROR: MissingException: the value at index (2,) is missing\n[...]\n\njulia> argmax(x)\n3\n\njulia> collect(keys(x))\n2-element Vector{Int64}:\n 1\n 3\n\njulia> collect(skipmissing([1, missing, 2]))\n2-element Vector{Int64}:\n 1\n 2\n\njulia> collect(skipmissing([1 missing; 2 missing]))\n2-element Vector{Int64}:\n 1\n 2\n```\n\"\"\"\nskipmissing(itr) = SkipMissing(itr)",
      "       S_IRGRP, S_IWGRP, S_IXGRP, S_IRWXG,\n       S_IROTH, S_IWOTH, S_IXOTH, S_IRWXO\n\nimport .Base:\n    IOError, _UVError, _sizeof_uv_fs, check_open, close, eof, eventloop, fd, isopen,\n    bytesavailable, position, read, read!, readavailable, seek, seekend, show,\n    skip, stat, unsafe_read, unsafe_write, write, transcode, uv_error,\n    setup_stdio, rawhandle, OS_HANDLE, INVALID_OS_HANDLE, windowserror, filesize\n\nimport .Base.RefValue\n\nif Sys.iswindows()\n    import .Base: cwstring\nend\n\n# Average buffer size including null terminator for several filesystem operations.\n# On Windows we use the MAX_PATH = 260 value on Win32.\nconst AVG_PATH = Sys.iswindows() ? 260 : 512\n\n# helper function to clean up libuv request\nuv_fs_req_cleanup(req) = ccall(:uv_fs_req_cleanup, Cvoid, (Ptr{Cvoid},), req)\n\ninclude(\"path.jl\")\ninclude(\"stat.jl\")\ninclude(\"file.jl\")\ninclude(string(length(Core.ARGS) >= 2 ? Core.ARGS[2] : \"\", \"file_constants.jl\"))  # include($BUILDROOT/base/file_constants.jl)\n\n## Operations with File (fd) objects ##\n\nabstract type AbstractFile <: IO end\n\nmutable struct File <: AbstractFile\n    open::Bool\n    handle::OS_HANDLE\n    File(fd::OS_HANDLE) = new(true, fd)\nend\nif OS_HANDLE !== RawFD\n    File(fd::RawFD) = File(Libc._get_osfhandle(fd)) # TODO: calling close would now destroy the wrong handle\nend\n\nrawhandle(file::File) = file.handle\nsetup_stdio(file::File, ::Bool) = (file, false)\n\n# Filesystem.open, not Base.open\nfunction open(path::AbstractString, flags::Integer, mode::Integer=0)\n    req = Libc.malloc(_sizeof_uv_fs)\n    local handle\n    try\n        ret = ccall(:uv_fs_open, Int32,\n                    (Ptr{Cvoid}, Ptr{Cvoid}, Cstring, Int32, Int32, Ptr{Cvoid}),\n                    C_NULL, req, path, flags, mode, C_NULL)\n        handle = ccall(:uv_fs_get_result, Cssize_t, (Ptr{Cvoid},), req)\n        uv_fs_req_cleanup(req)\n        ret < 0 && uv_error(\"open($(repr(path)), $flags, $mode)\", ret)\n    finally # conversion to Cstring could cause an exception\n        Libc.free(req)\n    end\n    return File(OS_HANDLE(@static Sys.iswindows() ? Ptr{Cvoid}(handle) : Cint(handle)))\nend\n\nisopen(f::File) = f.open\n\nfunction check_open(f::File)\n    if !isopen(f)",
      "    @test a+1 == b\n    @test isequal(a+1, b)\n    @test b == a+1\n    @test !(b == a)\n    @test b > a\n    @test b >= a\n    @test !(b < a)\n    @test !(b <= a)\n\n    @test typeof(a * 2) == Rational{BigInt}\n    @test a*2 == c\n    @test c-a == a\n    @test c == a + a\n    @test c+1 == a+b\n\n    @test typeof(d) == Rational{BigInt}\n    @test d == -c\n\n\n    @test e == a // b\n\n    @testset \"gmp cmp\" begin\n        @test Base.GMP.MPQ.cmp(b, a) ==  1\n        @test Base.GMP.MPQ.cmp(a, b) == -1\n        @test Base.GMP.MPQ.cmp(a, a) ==  0\n    end\n\n    @testset \"division errors\" begin\n        oz = Rational{BigInt}(0, 1)\n        zo = Rational{BigInt}(1, 0)\n\n        @test oz + oz == 3 * oz == oz\n        @test oz // zo == oz\n        @test zo // oz == zo\n\n        @test_throws DivideError() zo - zo\n        @test_throws DivideError() zo + (-zo)\n        @test_throws DivideError() zo * oz\n        @test_throws DivideError() oz // oz\n        @test_throws DivideError() zo // zo\n    end\n\n    @testset \"big infinities\" begin\n        oz   = Rational{BigInt}(1, 0)\n        zo   = Rational{BigInt}(0, 1)\n        o    = Rational{BigInt}(1, 1)\n\n        @test oz + zo    == oz\n        @test zo - oz    == -oz\n        @test zo + (-oz) == -oz\n        @test -oz + zo   == -oz\n\n        @test (-oz) * (-oz) == oz\n        @test (-oz) * oz    == -oz\n\n        @test o // zo       == oz\n        @test (-o) // zo    == -oz\n\n        @test Rational{BigInt}(-1, 0) == -1//0\n        @test Rational{BigInt}(1, 0) == 1//0\n    end\nend\n\n"
    ]
  },
  {
    "id": "ionic-team/ionic-framework",
    "org": "ionic-team",
    "avatarURL": "https://avatars.githubusercontent.com/u/3171503?v=4",
    "name": "ionic-team/ionic-framework",
    "url": "https://github.com/ionic-team/ionic-framework",
    "lang": "TypeScript",
    "desc": "A powerful cross-platform UI toolkit for building native-quality iOS, Android, and Progressive Web Apps with HTML, CSS, and JavaScript.",
    "star_num": 49488,
    "fork_num": 13665,
    "snippets": [
      "        config\n      );\n\n      const container = page.locator('#container');\n      await expect(container).toHaveScreenshot(screenshot('textarea-cols-autogrow'));\n    });\n  });\n});\n",
      "  d.IonBreadcrumbs,\n  d.IonButton,\n  d.IonButtons,\n  d.IonCard,\n  d.IonCardContent,\n  d.IonCardHeader,\n  d.IonCardSubtitle,\n  d.IonCardTitle,\n  d.IonCheckbox,\n  d.IonChip,\n  d.IonCol,\n  d.IonContent,\n  d.IonDatetime,\n  d.IonDatetimeButton,\n  d.IonFab,\n  d.IonFabButton,\n  d.IonFabList,\n  d.IonFooter,\n  d.IonGrid,\n  d.IonHeader,\n  d.IonIcon,\n  d.IonImg,\n  d.IonInfiniteScroll,\n  d.IonInfiniteScrollContent,\n  d.IonInput,\n  d.IonItem,\n  d.IonItemDivider,\n  d.IonItemGroup,\n  d.IonItemOption,\n  d.IonItemOptions,\n  d.IonItemSliding,\n  d.IonLabel,\n  d.IonList,\n  d.IonListHeader,\n  d.IonLoading,\n  d.IonMenu,\n  d.IonMenuButton,\n  d.IonMenuToggle,\n  d.IonNav,\n  d.IonNavLink,\n  d.IonNote,\n  d.IonPicker,\n  d.IonProgressBar,\n  d.IonRadio,\n  d.IonRadioGroup,\n  d.IonRange,\n  d.IonRefresher,\n  d.IonRefresherContent,\n  d.IonReorder,\n  d.IonReorderGroup,\n  d.IonRippleEffect,\n  d.IonRow,\n  d.IonSearchbar,\n  d.IonSegment,\n  d.IonSegmentButton,\n  d.IonSelect,\n  d.IonSelectOption,\n  d.IonSkeletonText,\n  d.IonSpinner,\n  d.IonSplitPane,\n  d.IonTabBar,\n  d.IonTabButton,\n  d.IonText,\n  d.IonTextarea,",
      "  test.describe(title('nav: nested'), () => {\n    test.beforeEach(async ({ page }) => {\n      await page.goto('/src/components/nav/test/nested', config);\n    });\n\n    test('should push pages with nested ion-nav', async ({ page }) => {\n      const pageOne = page.locator('page-one');\n      const pageTwo = page.locator('page-two');\n\n      const pageTwoButton = page.locator('ion-button:has-text(\"Go to Page 2\")');\n      const pageTwoTwoButton = page.locator('ion-button:has-text(\"Go to Page 2.2\")');\n\n      await pageTwoButton.click();\n      await page.waitForChanges();\n\n      const pageTwoOne = page.locator('page-two-one');\n      const pageTwoTwo = page.locator('page-two-two');\n\n      await expect(pageOne).toHaveCount(1);\n      await expect(pageTwo).toBeVisible();\n      await expect(pageTwoOne).toBeVisible();\n\n      await pageTwoTwoButton.click();\n      await page.waitForChanges();\n\n      await expect(pageTwoOne).toHaveCount(1);\n      await expect(pageTwoTwo).toBeVisible();\n\n      const pageThreeButton = page.locator('ion-button:has-text(\"Go to Page 3\")');\n\n      await pageThreeButton.click();\n      await page.waitForChanges();\n\n      const pageThree = page.locator('page-three');\n\n      await expect(pageThree).toBeVisible();\n      await expect(pageTwo).toHaveCount(1);\n      await expect(pageOne).toHaveCount(1);\n    });\n\n    test.describe('back button', () => {\n      test('should work with nested ion-nav', async ({ page }) => {\n        const pageTwoButton = page.locator('ion-button:has-text(\"Go to Page 2\")');\n        const pageTwoTwoButton = page.locator('ion-button:has-text(\"Go to Page 2.2\")');\n\n        await pageTwoButton.click();\n        await page.waitForChanges();\n\n        const pageTwoOne = page.locator('page-two-one');\n        const pageTwoTwo = page.locator('page-two-two');\n\n        await pageTwoTwoButton.click();\n        await page.waitForChanges();\n\n        const pageThreeButton = page.locator('ion-button:has-text(\"Go to Page 3\")');\n        const pageThreeBackButton = page.locator('page-three ion-back-button');\n\n        await pageThreeButton.click();\n        await page.waitForChanges();\n\n        const pageThree = page.locator('page-three');\n\n        await pageThreeBackButton.click();\n        await page.waitForChanges();",
      "import { expect } from '@playwright/test';\nimport { configs, test } from '@utils/test/playwright';\n\nconfigs().forEach(({ title, screenshot, config }) => {\n  test.describe(title('toast: stacked layout'), () => {\n    test('should render stacked buttons', async ({ page }) => {\n      await page.goto('/src/components/toast/test/layout', config);\n      const ionToastDidPresent = await page.spyOnEvent('ionToastDidPresent');\n\n      await page.click('#stacked');\n      await ionToastDidPresent.next();\n\n      const toastWrapper = page.locator('ion-toast .toast-wrapper');\n      await expect(toastWrapper).toHaveScreenshot(screenshot(`toast-stacked`));\n    });\n  });\n});\n",
      "import { TabsTab3NestedComponent } from './tabs-tab3-nested/tabs-tab3-nested.component';\n\n@NgModule({\n  declarations: [TabsTab3Component, TabsTab3NestedComponent],\n  imports: [\n    CommonModule,\n    IonicModule,\n    TabsLazyRoutingModule\n  ]\n})\nexport class TabsLazyModule { }\n",
      "@Component({\n  selector: 'app-popover-inline',\n  templateUrl: 'popover-inline.component.html'\n})\nexport class PopoverInlineComponent {\n\n  items: string[] = [];\n\n  openPopover(popover: IonPopover) {\n    popover.present();\n\n    setTimeout(() => {\n      this.items = ['A', 'B', 'C', 'D'];\n    }, 1000);\n  }\n\n}\n",
      "      expect(isRTL({ dir: 'rtl' })).toBe(true);\n    });\n\n    it('should return false', () => {\n      expect(isRTL({ dir: 'ltr' })).toBe(false);\n      expect(isRTL({ dir: '' })).toBe(false);\n    });\n  });\n\n  describe('without host element', () => {\n    it('should return true', () => {\n      global.document.dir = 'rtl';\n      expect(isRTL()).toBe(true);\n    });\n\n    it('should return false', () => {\n      global.document.dir = 'ltr';\n      expect(isRTL()).toBe(false);\n    });\n  });\n});\n",
      "import { expect } from '@playwright/test';\nimport { configs, test } from '@utils/test/playwright';\n\n/**\n * This does not test LTR vs. RTL layout.\n */\nconfigs({ directions: ['ltr'] }).forEach(({ title, screenshot, config }) => {\n  test.describe(title('footer: with tabs'), () => {\n    test('should not have extra padding when near a tab bar', async ({ page }) => {\n      await page.goto('/src/components/footer/test/with-tabs', config);\n\n      const footer = page.locator('[tab=\"tab-one\"] ion-footer');\n      await expect(footer).toHaveScreenshot(screenshot(`footer-with-tabs`));\n    });\n  });\n});\n",
      "  });\n\n  it('should correctly replace an item to location history', () => {\n    locationHistory.add({ pathname: '/home' });\n    locationHistory.add({ pathname: '/login', routerAction: 'replace' });\n\n    const current = locationHistory.last();\n    expect(current.pathname).toEqual('/login');\n  });\n\n  it('should correctly pop an item from location history', () => {\n    locationHistory.add({ pathname: '/home' });\n    locationHistory.add({ pathname: '/login', routerAction: 'pop' });\n\n    const current = locationHistory.last();\n    expect(current.pathname).toEqual('/login');\n    expect(locationHistory.canGoBack(1)).toEqual(false);\n  });\n\n  it('should correctly wipe location history when routerDirection is root', () => {\n    locationHistory.add({ pathname: '/home' });\n    locationHistory.add({ pathname: '/login' });\n    locationHistory.add({ pathname: '/logout', routerDirection: 'root' });\n\n    const current = locationHistory.last();\n    expect(current.pathname).toEqual('/logout');\n    expect(locationHistory.canGoBack(1)).toEqual(false);\n  });\n\n  it('should correctly update a route', () => {\n    locationHistory.add({ id: '1', pathname: '/tabs/tab1', tab: 'tab1' });\n    locationHistory.add({ id: '2', pathname: '/tabs/tab2' });\n\n    const current = { ...locationHistory.last() };\n    current.tab = 'tab2';\n\n    locationHistory.update(current);\n\n    const getCurrentAgain = locationHistory.last();\n    expect(getCurrentAgain.tab).toEqual('tab2');\n  });\n\n  it('should correctly get the first route for a tab', () => {\n    locationHistory.add({ id: '1', pathname: '/tabs/tab1', tab: 'tab1' });\n    locationHistory.add({ id: '2', pathname: '/tabs/tab1/child', tab: 'tab1' });\n    locationHistory.add({ id: '2', pathname: '/tabs/tab1/child/1', tab: 'tab1' });\n\n    const first = locationHistory.getFirstRouteInfoForTab('tab1');\n    expect(first.pathname).toEqual('/tabs/tab1');\n  });\n\n  it('should correctly get the current route for a tab', () => {\n    locationHistory.add({ id: '1', pathname: '/tabs/tab1', tab: 'tab1' });\n    locationHistory.add({ id: '2', pathname: '/tabs/tab1/child', tab: 'tab1' });\n    locationHistory.add({ id: '2', pathname: '/tabs/tab1/child/1', tab: 'tab1' });\n\n    const first = locationHistory.getCurrentRouteInfoForTab('tab1');\n    expect(first.pathname).toEqual('/tabs/tab1/child/1');\n  });\n\n  it('should correctly get last route', () => {\n    locationHistory.add({ pathname: '/home' });\n    locationHistory.add({ pathname: '/login' });\n",
      "\n  await ionDidClose.next();\n}\n"
    ]
  },
  {
    "id": "nestjs/nest",
    "org": "nestjs",
    "avatarURL": "https://avatars.githubusercontent.com/u/28507035?v=4",
    "name": "nestjs/nest",
    "url": "https://github.com/nestjs/nest",
    "lang": "TypeScript",
    "desc": "A progressive Node.js framework for building efficient and scalable server-side applications.",
    "star_num": 59354,
    "fork_num": 7110,
    "snippets": [
      "  providers: [\n    {\n      provide: 'MULTI_PROVIDER',\n      useValue: 'A',\n    },\n    {\n      provide: 'REQ_SCOPED_MULTI_PROVIDER',\n      useFactory: () => 'A',\n      scope: Scope.REQUEST,\n    },\n  ],\n})\nexport class AModule {}\n",
      "    });\n  });\n});\n",
      "import { PhotoController } from './photo.controller';\nimport { PhotoService } from './photo.service';\n\ndescribe('Photo Controller', () => {\n  let controller: PhotoController;\n  let service: PhotoService;\n  beforeEach(async () => {\n    const module: TestingModule = await Test.createTestingModule({\n      controllers: [PhotoController],\n      providers: [\n        {\n          provide: PhotoService,\n          useValue: {\n            findAll: jest.fn().mockResolvedValue([\n              {\n                name: 'Photo #1',\n                description: 'Description #1',\n                filename: 'Filename #1',\n                isPublish: true,\n              },\n              {\n                name: 'Photo #2',\n                description: 'Description #2',\n                filename: 'Filename #2',\n                isPublish: true,\n              },\n              {\n                name: 'Photo #3',\n                description: 'Description #3',\n                filename: 'Filename #3',\n                isPublish: false,\n              },\n            ]),\n          },\n        },\n      ],\n    }).compile();\n\n    controller = module.get<PhotoController>(PhotoController);\n    service = module.get<PhotoService>(PhotoService);\n  });\n\n  describe('findAll()', () => {\n    it('should return an array of photos', () => {\n      expect(controller.findAll()).resolves.toEqual([\n        {\n          name: 'Photo #1',\n          description: 'Description #1',\n          filename: 'Filename #1',\n          isPublish: true,\n        },\n        {\n          name: 'Photo #2',\n          description: 'Description #2',\n          filename: 'Filename #2',\n          isPublish: true,\n        },\n        {\n          name: 'Photo #3',\n          description: 'Description #3',\n          filename: 'Filename #3',\n          isPublish: false,\n        },\n      ]);",
      "        data: payload,\n        qos: 1,\n      });\n  });\n\n  afterEach(async () => {\n    await app.close();\n  });\n});\n",
      "\n  describe('root', () => {\n    it('should return \"Hello World!\"', () => {\n      expect(appController.getHello()).toBe('Hello World!');\n      expect(appService.getHello).toHaveBeenCalled();\n    });\n  });\n});\n",
      "export class StreamableFile {\n  private readonly stream: Readable;\n\n  protected handleError: (\n    err: Error,\n    response: StreamableHandlerResponse,\n  ) => void = (err: Error, res) => {\n    if (res.destroyed) {\n      return;\n    }\n    if (res.headersSent) {\n      res.end();\n      return;\n    }\n\n    res.statusCode = HttpStatus.BAD_REQUEST;\n    res.send(err.message);\n  };\n\n  constructor(buffer: Uint8Array, options?: StreamableFileOptions);\n  constructor(readable: Readable, options?: StreamableFileOptions);\n  constructor(\n    bufferOrReadStream: Uint8Array | Readable,\n    readonly options: StreamableFileOptions = {},\n  ) {\n    if (types.isUint8Array(bufferOrReadStream)) {\n      this.stream = new Readable();\n      this.stream.push(bufferOrReadStream);\n      this.stream.push(null);\n      this.options.length ??= bufferOrReadStream.length;\n    } else if (bufferOrReadStream.pipe && isFunction(bufferOrReadStream.pipe)) {\n      this.stream = bufferOrReadStream;\n    }\n  }\n\n  getStream(): Readable {\n    return this.stream;\n  }\n\n  getHeaders() {\n    const {\n      type = 'application/octet-stream',\n      disposition = undefined,\n      length = undefined,\n    } = this.options;\n    return {\n      type,\n      disposition,\n      length,\n    };\n  }\n\n  get errorHandler(): (\n    err: Error,\n    response: StreamableHandlerResponse,\n  ) => void {\n    return this.handleError;\n  }\n\n  setErrorHandler(\n    handler: (err: Error, response: StreamableHandlerResponse) => void,\n  ) {\n    this.handleError = handler;\n    return this;",
      "  });\n  let app: NestFastifyApplication;\n\n  afterEach(async () => {\n    await app.close();\n  });\n\n  describe('application/json', () => {\n    const stringLimit = '{ \"msg\": \"Hello, World\" }';\n    const stringOverLimit = '{ \"msg\": \"Hello, World!\" }';\n\n    beforeEach(async () => {\n      const testFixture = await moduleFixture.compile();\n\n      app = testFixture\n        .createNestApplication<NestFastifyApplication>(new FastifyAdapter(), {\n          rawBody: true,\n          logger: false,\n        })\n        .useBodyParser('application/json', {\n          bodyLimit: Buffer.from(stringLimit).byteLength,\n        });\n\n      await app.init();\n    });\n\n    it('should allow request with matching body limit', async () => {\n      const response = await app.inject({\n        method: 'POST',\n        url: '/',\n        headers: { 'content-type': 'application/json' },\n        payload: stringLimit,\n      });\n\n      expect(JSON.parse(response.body)).to.eql({\n        raw: stringLimit,\n      });\n    });\n\n    it('should fail if post body is larger than limit', async () => {\n      const response = await app.inject({\n        method: 'POST',\n        url: '/',\n        headers: { 'content-type': 'application/json' },\n        payload: stringOverLimit,\n      });\n\n      expect(response.statusCode).to.equal(413);\n    });\n  });\n\n  describe('application/x-www-form-urlencoded', () => {\n    const stringLimit = 'msg=Hello, World';\n    const stringOverLimit = 'msg=Hello, World!';\n\n    beforeEach(async () => {\n      const testFixture = await moduleFixture.compile();\n\n      app = testFixture\n        .createNestApplication<NestFastifyApplication>(new FastifyAdapter(), {\n          rawBody: true,\n          logger: false,\n        })\n        .useBodyParser('application/x-www-form-urlencoded', {",
      "import { Inject, Injectable, Logger, Scope } from '@nestjs/common';\nimport { INQUIRER } from '@nestjs/core';\n\n@Injectable({ scope: Scope.TRANSIENT })\nexport class TransientLogger {\n  @Inject(INQUIRER) inquirer: any = null;\n  config: object;\n\n  constructor(\n    @Inject(INQUIRER) private readonly inquirerInCtor,\n    private readonly logger: Logger,\n  ) {\n    this.config =\n      (inquirerInCtor.constructor && inquirerInCtor.constructor.logger) || {};\n  }\n\n  log(message: string) {\n    this.logger.log({ message, ...this.config });\n  }\n}\n",
      "        // reason: https://github.com/nestjs/nest/pull/10821#issuecomment-1411916533\n        const descriptor = Object.getOwnPropertyDescriptor(prototype, property);\n\n        if (\n          descriptor.set ||\n          descriptor.get ||\n          isConstructor(property) ||\n          !isFunction(prototype[property])\n        ) {\n          continue;\n        }\n\n        const value = callback(property);\n\n        if (isNil(value)) {\n          continue;\n        }\n\n        result.push(value);\n      }\n    } while (\n      (prototype = Reflect.getPrototypeOf(prototype)) &&\n      prototype !== Object.prototype\n    );\n\n    return result;\n  }\n\n  /**\n   * @deprecated\n   * @see {@link getAllMethodNames}\n   * @see getAllMethodNames\n   */\n  public *getAllFilteredMethodNames(\n    prototype: object,\n  ): IterableIterator<string> {\n    yield* this.getAllMethodNames(prototype);\n  }\n\n  public getAllMethodNames(prototype: object | null): string[] {\n    if (!prototype) {\n      return [];\n    }\n\n    if (this.cachedScannedPrototypes.has(prototype)) {\n      return this.cachedScannedPrototypes.get(prototype);\n    }\n\n    const visitedNames = new Map<string, boolean>();\n    const result: string[] = [];\n\n    this.cachedScannedPrototypes.set(prototype, result);\n\n    do {\n      for (const property of Object.getOwnPropertyNames(prototype)) {\n        if (visitedNames.has(property)) {\n          continue;\n        }\n\n        visitedNames.set(property, true);\n\n        // reason: https://github.com/nestjs/nest/pull/10821#issuecomment-1411916533\n        const descriptor = Object.getOwnPropertyDescriptor(prototype, property);\n",
      "\n    app = module.createNestApplication();\n    server = app.getHttpServer();\n    await app.init();\n  });\n\n  [\n    {\n      host: 'example.com',\n      path: '/hello',\n      greeting: 'Hello world!',\n    },\n    {\n      host: 'acme.example.com',\n      path: '/host',\n      greeting: 'Host Greeting! tenant=acme',\n    },\n    {\n      host: 'acme.example1.com',\n      path: '/host-array',\n      greeting: 'Host Greeting! tenant=acme',\n    },\n    {\n      host: 'acme.example2.com',\n      path: '/host-array',\n      greeting: 'Host Greeting! tenant=acme',\n    },\n  ].forEach(({ host, path, greeting }) => {\n    describe(`host=${host}`, () => {\n      describe('/GET', () => {\n        it(`should return \"${greeting}\"`, () => {\n          return request(server)\n            .get(path)\n            .set('Host', host)\n            .expect(200)\n            .expect(greeting);\n        });\n\n        it(`should attach response header`, () => {\n          return request(server)\n            .get(path)\n            .set('Host', host)\n            .expect(200)\n            .expect('Authorization', 'Bearer');\n        });\n      });\n\n      it(`/GET (Promise/async) returns \"${greeting}\"`, () => {\n        return request(server)\n          .get(`${path}/async`)\n          .set('Host', host)\n          .expect(200)\n          .expect(greeting);\n      });\n\n      it(`/GET (Observable stream) \"${greeting}\"`, () => {\n        return request(server)\n          .get(`${path}/stream`)\n          .set('Host', host)\n          .expect(200)\n          .expect(greeting);\n      });\n    });\n  });"
    ]
  },
  {
    "id": "Tencent/wepy",
    "org": "Tencent",
    "avatarURL": "https://avatars.githubusercontent.com/u/18461506?v=4",
    "name": "Tencent/wepy",
    "url": "https://github.com/Tencent/wepy",
    "lang": "JavaScript",
    "desc": "A mini program framework to promote development efficiency.",
    "star_num": 22254,
    "fork_num": 3093,
    "snippets": [
      "      currentScope.declared = currentScope.declared || [];\n      res.for = inMatch[2].trim();\n      currentScope.for = res.for;\n      let alias = inMatch[1].trim().replace(stripParensRE, '');\n      let iteratorMatch = alias.match(forIteratorRE);\n      if (iteratorMatch) {\n        res.alias = alias.replace(forIteratorRE, '').trim();\n        currentScope.declared.push(res.alias);\n        currentScope.alias = res.alias;\n        res.iterator1 = iteratorMatch[1].trim();\n        currentScope.iterator1 = res.iterator1;\n        currentScope.declared.push(res.iterator1);\n        if (iteratorMatch[2]) {\n          res.iterator2 = iteratorMatch[2].trim();\n          currentScope.iterator2 = res.iterator2;\n          currentScope.declared.push(res.iterator2);\n        }\n      } else {\n        res.alias = alias;\n        currentScope.alias = alias;\n        currentScope.declared.push(alias);\n      }\n    }\n    if (scope) {\n      currentScope.parent = scope;\n    }\n\n    let err = Check.checkExpression(res.for, `v-for=\"${expr}\"`);\n    if (err) {\n      this.hookUnique(\n        'error-handler',\n        'template',\n        {\n          ctx: ctx,\n          message: err,\n          type: 'error',\n          title: 'v-for'\n        },\n        {\n          item: item,\n          attr: name,\n          expr: expr\n        }\n      );\n    }\n\n    item['v-for'] = {\n      'wx:for': `{{ ${res.for} }}`,\n      'wx:for-index': `${res.iterator1 || 'index'}`,\n      'wx:for-item': `${res.alias || 'item'}`,\n      'wx:key': `${res.iterator2 || res.iterator1 || 'index'}`\n    };\n\n    return {\n      item,\n      name,\n      expr,\n      modifiers,\n      scope: currentScope,\n      ctx\n    };\n  });\n\n  this.register('template-parse-ast-attr-v-for', function parseDirectivesFor({",
      "      {\n        highlightCode: true,\n        message: msg || ''\n      },\n      options || {}\n    );\n\n    if (!pos.start) {\n      let newpos = {};\n      if (pos.startIndex) {\n        newpos.start = indexToLineColumns(code, pos.startIndex);\n      }\n      if (pos.endIndex) {\n        newpos.end = indexToLineColumns(code, pos.endIndex);\n      }\n      pos = newpos;\n    } else if (pos.sourcemap) {\n      let consumer = SourceMap.SourceMapConsumer(pos.sourcemap);\n      if (pos.start) {\n        pos.start = consumer.originalPositionFor(pos.start);\n      }\n      if (pos.end) {\n        pos.end = consumer.originalPositionFor(pos.end);\n      }\n    }\n\n    return codeFrameColumns(code, pos, options);\n  });\n\n  this.register('gen-code-frame-html', function(code, item, attr, expr, msg, options) {\n    let node = code.substring(item.startIndex, item.endIndex);\n    let i = 0,\n      l = node.length,\n      quotes = [];\n    let word = '';\n    let inQuoteString = '';\n    let startIndex = 0;\n    let endIndex = 0;\n\n    while (i < l) {\n      let c = node[i++];\n      // eslint-disable-next-line\n      if (c === '\"' || c === \"'\") {\n        if (quotes[quotes.length - 1] === c) {\n          quotes.pop();\n        } else {\n          quotes.push(c);\n        }\n        continue;\n      }\n      if (quotes.length === 0) {\n        inQuoteString = '';\n        if (c === ' ') {\n          word = '';\n        } else {\n          word += c;\n        }\n      } else {\n        inQuoteString += c;\n      }\n\n      if (word === attr) {\n        startIndex = i - word.length;\n      }",
      "            } else if (type(to) === 'Error') {\n              reject(to)\n\n              errorHandlers.forEach(errorHandler => errorHandler(to))\n            } else if (type(to) === 'Object') {\n              reject(to)\n            } else if (type(to) === 'Function') {\n              resolve()\n\n              routeManager.setNext(to)\n            } else {\n              resolve()\n            }\n          }\n        }\n      }))\n    } catch (e) {\n      if (type(e) === 'Object') {\n        return runGuard(e)\n      }\n\n      throw e\n    }\n\n    return {\n      pageMethod: jumpMethodName,\n      pageDelta: delta,\n      realPage: to.name,\n      pageData: to.query,\n      pageMeta: to.meta,\n      pageEncode: encode,\n      pages: to.pages\n    }\n\n    function getTo({ name, query, delta, jumpMethodName, encode }) {\n      if (jumpMethodName === 'navigateBack') {\n        return {\n          ...routeManager.routes[routeManager.routes.length - 1 - delta],\n          jumpMethodName,\n          delta,\n          pages: [routeManager.routes[routeManager.routes.length - 1 - delta].name]\n        }\n      }\n\n      const { pageData = {}, pageMeta, pages } = getRealPageInfo({ name, query })\n\n      return {\n        name: pages[pages.length - 1], query: pageData, meta: pageMeta, jumpMethodName, encode, pages\n      }\n    }\n\n    function getGuardQueue(name, pages) {\n      if (name === routeManager.route.name) {\n        const beforeGuards = [...guard.componentGuardMap[name].beforeRouteUpdateGuards]\n\n        return [\n          ...beforeGuards.map(beforeGuard => beforeGuard.bind(routeManager.currentVm))\n        ]\n      }\n\n      const beforeGuards = [\n        ...guard.componentGuardMap[routeManager.route.name].beforeRouteLeaveGuards,\n        ...guard.globalGuard.beforeEachGuards,\n        ...getBeforeEnterGuards(pages)",
      "    for (let name in attrs) {\n      let expr = attrs[name];\n\n      expr = decodeAttr(expr);\n\n      ({ item, name, expr } = this.hookUniqueReturnArg('template-parse-ast-pre-attr-' + name, { item, name, expr }));\n\n      let modifiers = parseModifiers(name);\n\n      if (modifiers) {\n        name = name.replace(modifierRE, '');\n      }\n\n      let hook = 'template-parse-ast-pre-attr-' + name;\n\n      if (!this.hasHook(hook)) {\n        hook = 'template-parse-ast-pre-attr-[other]';\n      }\n\n      ({ item, name, expr, modifiers, scope, ctx } = this.hookUniqueReturnArg(hook, {\n        item,\n        name,\n        expr,\n        modifiers,\n        scope,\n        ctx\n      }));\n\n      cleanAttrs.push({\n        item: item,\n        name: name,\n        expr: expr,\n        modifiers: modifiers\n      });\n    }\n\n    // Apply walk attributes\n    cleanAttrs.forEach(({ item, name, expr, modifiers }) => {\n      let hook = 'template-parse-ast-attr-' + name;\n      if (!this.hasHook(hook)) {\n        hook = 'template-parse-ast-attr-[other]';\n      }\n\n      parsed = this.hookUnique(hook, { item, name, expr, modifiers, scope, ctx, rel });\n      rel = parsed.rel || rel;\n\n      let applyHook = parsed.hook || `template-parse-ast-attr-${name}-apply`;\n      if (!this.hasHook(applyHook)) {\n        applyHook = `template-parse-ast-attr-[other]-apply`;\n      }\n\n      ({ parsed, rel } = this.hookUniqueReturnArg(applyHook, { parsed, rel }));\n\n      if (parsed && parsed.attrs) {\n        parsedAttr = Object.assign(parsedAttr, parsed.attrs);\n      }\n    });\n\n    item.parsedAttr = parsedAttr;\n\n    return [item, scope, rel, ctx];\n  });\n\n  this.register('template-parse-ast-tag', function parseAstTag(item, rel) {",
      "function encodeParams(obj, encode = false) {\n  const results = []\n\n  for (const key in obj) {\n    if (['string', 'number'].indexOf(typeof obj[key]) !== -1) {\n      results.push(`${key}=${encode ? encodeURIComponent(obj[key]) : obj[key]}`)\n    }\n  }\n\n  return results.join('&')\n}\n\n/**\n * 编码url参数，如：{ name: 'xxxxPage', query: {a: 123, b: 345} } => /pages/xxxxPage?a=123&b=456\n * @param obj\n * @param encode = false\n * @param isAbsolutePath = false 是否得到真实的页面（物理页面）\n * return String\n */\nfunction encodeUrl({ name, query = {} }, { encode = false, isAbsolutePath = true } = {}) {\n  const { pagePath, pageData } = isAbsolutePath ? getRealPageInfo({ name, query }) : { pagePath: name, pageData: query }\n  const paramsStr = encodeParams(pageData, encode)\n\n  return paramsStr ? `${pagePath}?${paramsStr}` : pagePath\n}\n\n/**\n * 解码 fullPath 或 params ，如：a=123&b=456 => { a: 123, b: 345 }\n * @param fullPath\n * @param decode = true\n * @returns {*}\n */\nfunction decodeParams(fullPath, decode = true) {\n  const splitArr = fullPath.split('?')\n  const result = {}\n\n  void (splitArr.length > 1 ? splitArr[1] : splitArr[0])\n    .split('&')\n    .map(str => str.split('='))\n    .forEach(([key, value]) => (result[key] = decode ? decodeURIComponent(value) : value))\n\n  return result\n}\n\n/**\n * 解析 /pages/xxxxPage?a=123&b=456 => xxxxPage\n * @param fullPath\n */\nfunction decodePage(fullPath) {\n  const result = fullPath.split('?')[0].split('/')\n\n  return result[result.length - 1]\n}\n\n/**\n * 解析 fullPath 页面和参数。如: /pages/xxxxPage?a=123&b=456 => { name: xxxxPage, query: { a: 123, b: 345 } }\n * @param fullPath\n * @param decode = true\n * @return {{ query: *, name }}\n */\nfunction decodeUrl(fullPath, decode = true) {\n  return {\n    name: decodePage(fullPath),\n    query: decodeParams(fullPath, decode)",
      " * triggers change notification if the property doesn't\n * already exist.\n */\nfunction set(vm, target, key, val) {\n  if (Array.isArray(target) && isValidArrayIndex(key)) {\n    target.length = Math.max(target.length, key);\n    target.splice(key, 1, val);\n    return val;\n  }\n\n  if (key in target && !(key in Object.prototype)) {\n    target[key] = val;\n    return val;\n  }\n\n  var ob = target.__ob__;\n  if (target._isVue || (ob && ob.vmCount)) {\n    \"development\" !== 'production' &&\n      warn(\n        'Avoid adding reactive properties to a Vue instance or its root $data ' +\n          'at runtime - declare it upfront in the data option.'\n      );\n    return val;\n  }\n\n  if (!ob) {\n    target[key] = val;\n    return val;\n  }\n\n  if (isObject(target[key]) && hasOwn(target[key], '__ob__')) {\n    // delete invalid paths\n    cleanPaths(key, target[key].__ob__.op, ob.op);\n  }\n  defineReactive({ vm: vm, obj: ob.value, key: key, value: val, parent: ob.value });\n  if (vm) {\n    // push parent key to dirty, wait to setData\n    if (vm.$dirty && hasOwn(target, '__ob__')) {\n      vm.$dirty.set(target.__ob__.op, key, val);\n    }\n  }\n  ob.dep.notify();\n  return val;\n}\n\n/**\n * Delete a property and trigger change if necessary.\n */\nfunction del(target, key) {\n  if (Array.isArray(target) && isValidArrayIndex(key)) {\n    target.splice(key, 1);\n    return;\n  }\n\n  var ob = target.__ob__;\n  if (target._isVue || (ob && ob.vmCount)) {\n    \"development\" !== 'production' &&\n      warn('Avoid deleting properties on a Vue instance or its root $data ' + '- just set it to null.');\n    return;\n  }\n\n  if (!hasOwn(target, key)) {\n    return;\n  }",
      "  'onReachBottom',\n  'onShareAppMessage',\n  'onAddToFavorites',\n  'onPageScroll',\n  'onResize',\n  'onTabItemTap',\n  'onShareTimeline'\n];\n\nexport const WEAPP_COMPONENT_LIFECYCLE = ['beforeCreate', 'created', 'attached', 'ready', 'moved', 'detached', 'error'];\n\nexport const WEAPP_COMPONENT_PAGE_LIFECYCLE = ['show', 'hide', 'resize'];\n\nexport const WEAPP_LIFECYCLE = []\n  .concat(WEAPP_APP_LIFECYCLE)\n  .concat(WEAPP_PAGE_LIFECYCLE)\n  .concat(WEAPP_COMPONENT_LIFECYCLE)\n  .concat(WEAPP_COMPONENT_PAGE_LIFECYCLE);\n",
      "  };\n\n  let fnNormalBak = instance.resolvers.normal.resolve;\n  instance.resolvers.normal.resolve = function(...args) {\n    return new Promise((resolve, reject) => {\n      args.push(function(err, filepath, meta) {\n        if (err) {\n          reject(err);\n        } else {\n          resolve({ path: filepath, meta: meta });\n        }\n      });\n      fnNormalBak.apply(instance.resolvers.normal, args);\n    });\n  };\n  return instance;\n}\n\nfunction readSpec(id, isFailSpec) {\n  let lessfile = path.join(__dirname, 'fixtures', 'less', id + '.less');\n  let expectfile = isFailSpec ? '' : path.join(__dirname, 'fixtures', 'css', id + '.css');\n\n  return {\n    node: {\n      content: fs.readFileSync(lessfile, 'utf-8')\n    },\n    file: lessfile,\n    expect: isFailSpec ? '' : fs.readFileSync(expectfile, 'utf-8')\n  };\n}\n\nfunction compare(id, done) {\n  let compile = createCompile(specs.getOpt(id), specs.getResolveOpt(id));\n\n  let spec = readSpec(id);\n  return compile\n    .hook('wepy-compiler-less', spec.node, spec.file)\n    .then(node => {\n      let css = node.compiled.code;\n      expect(css).to.equal(spec.expect);\n      done();\n    })\n    .catch(e => {\n      done(e);\n    });\n}\n\nfunction compileFail(id, done) {\n  let compile = createCompile(specs.getOpt(id), specs.getResolveOpt(id));\n\n  let spec = readSpec(id, true);\n\n  let setting = specs.getId(id);\n\n  return compile\n    .hook('wepy-compiler-less', spec.node, spec.file)\n    .then(() => {\n      // e.g. uri-alias, alias is not awared in uri, so treat as compile successfully.\n      if (setting.then) {\n        done();\n      }\n    })\n    .catch(e => {\n      if (setting.error) {",
      "  typeofTest: 'undefined',\n  typeofTest2: 'undefined',\n\n  builtinFuncValue: 666\n};\n",
      "        const baseName = path.basename(file, '.js');\n        const wxmlFile = path.format({\n          dir: dirName,\n          base: baseName + '.wxml'\n        });\n        componentValue = fs.existsSync(wxmlFile);\n      }\n\n      this.assets.add(depFileCtx.file, {\n        npm: depFileCtx.npm,\n        dep: true,\n        component: componentValue,\n        type: depFileCtx.type,\n        wxs: depFileCtx.wxs\n      });\n\n      if (ext === '.js' || ext === '.ts' || ext === '.wxs') {\n        if (depFileCtx.npm && depFileCtx.type !== 'weapp') {\n          // weapp component npm may have import in it.\n          return this.applyCompiler({ type: 'script', lang: 'js', content: fileContent }, depFileCtx);\n        } else {\n          return this.applyCompiler({ type: 'script', lang: node.lang || 'babel', content: fileContent }, depFileCtx);\n        }\n      } else {\n        if (ext === this.options.wpyExt) {\n          // TODO: why they import a wpy file.\n          this.hookUnique(\n            'error-handler',\n            'script',\n            {\n              code: node.compiled.code,\n              ctx: depFileCtx,\n              type: 'error',\n              message: 'Can not import a wepy component, please use \"usingComponents\" to declear a component',\n              title: 'dependence'\n            },\n            node.compiled.map\n              ? {\n                  sourcemap: node.compiled.map,\n                  start: depFileCtx.dep.loc.start,\n                  end: depFileCtx.dep.loc.end\n                }\n              : depFileCtx.dep.loc\n          );\n          throw new Error('EXIT');\n        } else {\n          this.hookUnique(\n            'error-handler',\n            'script',\n            {\n              code: node.compiled ? node.compiled.code : '',\n              ctx: depFileCtx,\n              type: 'error',\n              message: `Unrecognized import extension: ${depFileCtx.file}`,\n              title: 'dependence'\n            },\n            node.compiled && node.compiled.map\n              ? {\n                  sourcemap: node.compiled.map,\n                  start: depFileCtx.dep.loc.start,\n                  end: depFileCtx.dep.loc.end\n                }\n              : depFileCtx.dep.loc\n          );"
    ]
  },
  {
    "id": "vercel/next.js",
    "org": "vercel",
    "avatarURL": "https://avatars.githubusercontent.com/u/14985020?v=4",
    "name": "vercel/next.js",
    "url": "https://github.com/vercel/next.js",
    "lang": "JavaScript",
    "desc": "The React Framework.",
    "star_num": 111207,
    "fork_num": 24686,
    "snippets": [
      "import dynamic from 'next/dynamic'\n\nconst First = dynamic(\n  import('../../components/dynamic-css/many-imports/with-css-1')\n)\nconst Second = dynamic(\n  import('../../components/dynamic-css/many-imports/with-css-2')\n)\nconst Third = dynamic(\n  import('../../components/dynamic-css/many-imports/with-css-3')\n)\n\nexport default function Page() {\n  return (\n    <div>\n      <First />\n      <Second />\n      <Third />\n    </div>\n  )\n}\n",
      "    edgeThenNode,\n    nodeThenEdge,\n    reactServer,\n  })\n}\n",
      "import { useRouter } from 'next/router'\nimport { useLayoutEffect } from 'react'\n\nexport default function Page(props) {\n  const router = useRouter()\n\n  if (typeof window !== 'undefined') {\n    // eslint-disable-next-line react-hooks/rules-of-hooks\n    useLayoutEffect(() => {\n      if (!window.isReadyValues) {\n        window.isReadyValues = []\n      }\n      window.isReadyValues.push(router.isReady)\n    }, [router])\n  }\n\n  return (\n    <>\n      <p id=\"auto-export\">auto-export page</p>\n    </>\n  )\n}\n",
      "  it('should build and trace correctly', async () => {\n    const result = await nextBuild(appDir, undefined, {\n      cwd: appDir,\n      stderr: true,\n      stdout: true,\n    })\n    console.log(result)\n    expect(result.code).toBe(0)\n\n    const appTrace = await fs.readJSON(\n      join(appDir, '.next/server/pages/_app.js.nft.json')\n    )\n    const indexTrace = await fs.readJSON(\n      join(appDir, '.next/server/pages/index.js.nft.json')\n    )\n    const anotherTrace = await fs.readJSON(\n      join(appDir, '.next/server/pages/another.js.nft.json')\n    )\n    const imageTrace = await fs.readJSON(\n      join(appDir, '.next/server/pages/image-import.js.nft.json')\n    )\n\n    const tracedFiles = [\n      ...appTrace.files,\n      ...indexTrace.files,\n      ...anotherTrace.files,\n      ...imageTrace.files,\n    ]\n\n    expect(tracedFiles.some((file) => file.endsWith('hello.json'))).toBe(true)\n    expect(tracedFiles.some((file) => file.includes('some-cms/index.js'))).toBe(\n      true\n    )\n    expect(\n      tracedFiles.some((file) => file === '../../../include-me/hello.txt')\n    ).toBe(true)\n    expect(\n      tracedFiles.some((file) => file === '../../../include-me/second.txt')\n    ).toBe(true)\n    expect(indexTrace.files.some((file) => file.includes('exclude-me'))).toBe(\n      false\n    )\n\n    expect(\n      tracedFiles.some((file) =>\n        file.includes('nested-structure/constants/package.json')\n      )\n    ).toBe(true)\n    expect(\n      tracedFiles.some((file) => file.includes('nested-structure/package.json'))\n    ).toBe(true)\n    expect(\n      tracedFiles.some((file) =>\n        file.includes('nested-structure/dist/constants.js')\n      )\n    ).toBe(true)\n    expect(\n      tracedFiles.some((file) => file.includes('public/another.jpg'))\n    ).toBe(true)\n    expect(tracedFiles.some((file) => file.includes('public/test.jpg'))).toBe(\n      false\n    )\n  })\n})",
      "export default function Layout({ children }) {\n  return (\n    <html>\n      <head>\n        <title>Hello</title>\n      </head>\n      <body>{children}</body>\n    </html>\n  )\n}\n\nexport function generateStaticParams() {\n  return [{ slug: 'slug1' }, { slug: 'slug2' }]\n}\n",
      "  killApp,\n  findPort,\n  renderViaHTTP,\n} from 'next-test-utils'\n\ndescribe('Production Custom Build Directory', () => {\n  describe('With basic usage', () => {\n    it('should render the page', async () => {\n      const result = await runNextCommand(['build', 'build'], {\n        cwd: join(__dirname, '..'),\n        stdout: true,\n        stderr: true,\n      })\n      expect(result.stderr).toBe('')\n\n      const appPort = await findPort()\n      const app = await nextStart(join(__dirname, '../build'), appPort)\n\n      const html = await renderViaHTTP(appPort, '/')\n      expect(html).toMatch(/Hello World/)\n\n      await killApp(app)\n    })\n  })\n})\n",
      "  }\n}\n\ndescribe('Page Config', () => {\n  it('builds without error when export const config is used outside page', async () => {\n    const { stderr } = await nextBuild(appDir, undefined, { stderr: true })\n    expect(stderr).not.toMatch(/Failed to compile\\./)\n  })\n\n  it('shows valid error when page config is a string', async () => {\n    const reset = await uncommentExport('invalid/string-config.js')\n\n    try {\n      const { stderr } = await nextBuild(appDir, undefined, { stderr: true })\n      expect(stderr).toMatch(/\\/invalid-page-config/)\n    } finally {\n      await reset()\n    }\n  })\n\n  it('shows valid error when page config has no init', async () => {\n    const reset = await uncommentExport('invalid/no-init.js')\n\n    try {\n      const { stderr } = await nextBuild(appDir, undefined, { stderr: true })\n      expect(stderr).toMatch(/\\/invalid-page-config/)\n    } finally {\n      await reset()\n    }\n  })\n\n  it('shows error when page config has spread properties', async () => {\n    const reset = await uncommentExport('invalid/spread-config.js')\n\n    try {\n      const { stderr } = await nextBuild(appDir, undefined, { stderr: true })\n      expect(stderr).toMatch(/\\/invalid-page-config/)\n    } finally {\n      await reset()\n    }\n  })\n\n  it('shows error when page config has invalid properties', async () => {\n    const reset = await uncommentExport('invalid/invalid-property.js')\n\n    try {\n      const { stderr } = await nextBuild(appDir, undefined, { stderr: true })\n      expect(stderr).toMatch(/\\/invalid-page-config/)\n    } finally {\n      await reset()\n    }\n  })\n\n  it('shows error when page config has invalid property value', async () => {\n    const reset = await uncommentExport('invalid/invalid-value.js')\n\n    try {\n      const { stderr } = await nextBuild(appDir, undefined, { stderr: true })\n      expect(stderr).toMatch(/\\/invalid-page-config/)\n    } finally {\n      await reset()\n    }\n  })\n",
      "  return (\n    <>\n      <Link as={as} href={href} legacyBehavior>\n        <a {...otherProps} />\n      </Link>\n      <style jsx>{`\n        a {\n          color: tomato;\n        }\n      `}</style>\n    </>\n  )\n}\n",
      "        Take a look at the head to see what has been added, you are looking for\n        a script tag of type \"application/ld+json\".\n      </p>\n    </div>\n  )\n}\n",
      "export default function Layout({ children }) {\n  return (\n    <html>\n      <body>\n        <h1>Group Layout</h1>\n        {children}\n      </body>\n    </html>\n  )\n}\n"
    ]
  },
  {
    "id": "rollup/rollup",
    "org": "rollup",
    "avatarURL": "https://avatars.githubusercontent.com/u/12554859?v=4",
    "name": "rollup/rollup",
    "url": "https://github.com/rollup/rollup",
    "lang": "JavaScript, TypeScript",
    "desc": "Next-generation ES module bundler.",
    "star_num": 23868,
    "fork_num": 1389,
    "snippets": [
      "}));\n",
      "class Main2 {\n  constructor () {\n    fn();\n    fn$3();\n  }\n}\n\nexport { Main2 as default };\n",
      "  return {\n    execute: (function () {\n\n      let C$1 = class C {\n        fn (num) {\n          console.log(num - p);\n        }\n      };\n\n      var p$1 = exports('p', 43);\n\n      new C$1().fn(p$1);\n\n      class C {\n        fn (num) {\n          console.log(num - p$1);\n        }\n      }\n\n      var p = exports('p2', 42);\n\n      new C().fn(p);\n\n    })\n  };\n}));\n",
      "\texport default value;\n}\n\n// external libs\ndeclare module 'rollup-plugin-string' {\n\timport type { PluginImpl } from 'rollup';\n\n\texport const string: PluginImpl;\n}\n\ndeclare module 'acorn-import-assertions' {\n\texport const importAssertions: () => unknown;\n}\n\ndeclare module 'is-reference' {\n\timport type * as estree from 'estree';\n\n\texport default function is_reference(\n\t\tnode: NodeWithFieldDefinition,\n\t\tparent: NodeWithFieldDefinition\n\t): boolean;\n\n\texport type Node =\n\t\t| estree.ArrayExpression\n\t\t| estree.ArrayPattern\n\t\t| estree.ArrowFunctionExpression\n\t\t| estree.AssignmentExpression\n\t\t| estree.AssignmentPattern\n\t\t| estree.AssignmentProperty\n\t\t| estree.AwaitExpression\n\t\t| estree.BinaryExpression\n\t\t| estree.BlockStatement\n\t\t| estree.BreakStatement\n\t\t| estree.CatchClause\n\t\t| estree.ChainExpression\n\t\t| estree.ClassBody\n\t\t| estree.ClassDeclaration\n\t\t| estree.ClassExpression\n\t\t| estree.ConditionalExpression\n\t\t| estree.ContinueStatement\n\t\t| estree.DebuggerStatement\n\t\t| estree.DoWhileStatement\n\t\t| estree.EmptyStatement\n\t\t| estree.ExportAllDeclaration\n\t\t| estree.ExportDefaultDeclaration\n\t\t| estree.ExportNamedDeclaration\n\t\t| estree.ExportSpecifier\n\t\t| estree.ExpressionStatement\n\t\t| estree.ForInStatement\n\t\t| estree.ForOfStatement\n\t\t| estree.ForStatement\n\t\t| estree.FunctionDeclaration\n\t\t| estree.FunctionExpression\n\t\t| estree.IfStatement\n\t\t| estree.Identifier\n\t\t| estree.ImportDeclaration\n\t\t| estree.ImportDefaultSpecifier\n\t\t| estree.ImportExpression\n\t\t| estree.ImportNamespaceSpecifier\n\t\t| estree.ImportSpecifier\n\t\t| estree.LabeledStatement\n\t\t| estree.LogicalExpression\n\t\t| estree.MemberExpression\n\t\t| estree.MetaProperty",
      "module.exports = defineTest({\n\tdescription: 'throws for invalid prebuilt chunks filename',\n\toptions: {\n\t\tplugins: {\n\t\t\tname: 'test-plugin',\n\t\t\tbuildStart() {\n\t\t\t\tthis.emitFile({\n\t\t\t\t\ttype: 'prebuilt-chunk',\n\t\t\t\t\tcode: 'console.log(\"my-chunk\")'\n\t\t\t\t});\n\t\t\t}\n\t\t}\n\t},\n\terror: {\n\t\tcode: 'PLUGIN_ERROR',\n\t\thook: 'buildStart',\n\t\tmessage:\n\t\t\t'The \"fileName\" property of emitted prebuilt chunks must be strings that are neither absolute nor relative paths, received \"undefined\".',\n\t\tplugin: 'test-plugin',\n\t\tpluginCode: 'VALIDATION_ERROR'\n\t}\n});\n",
      "var _missingExportShim = void 0;\n\nconsole.log('This is the output when a missing export is used internally but not reexported');\n\nfunction almostUseUnused(useIt) {\n\tif (useIt) {\n\t\tconsole.log(_missingExportShim);\n\t}\n}\n\nalmostUseUnused(false);\n\nexport { _missingExportShim as missing1 };\n",
      "\t\t}],\n\t\texecute: (function () {\n\n\n\n\t\t})\n\t};\n}));\n",
      "\n\treturn emitter;\n}\n\nasync function watchInternal(configs: MaybeArray<RollupOptions>, emitter: RollupWatcher) {\n\tconst optionsList = await Promise.all(\n\t\tensureArray(configs).map(config => mergeOptions(config, true))\n\t);\n\tconst watchOptionsList = optionsList.filter(config => config.watch !== false);\n\tif (watchOptionsList.length === 0) {\n\t\treturn error(\n\t\t\tlogInvalidOption(\n\t\t\t\t'watch',\n\t\t\t\tURL_WATCH,\n\t\t\t\t'there must be at least one config where \"watch\" is not set to \"false\"'\n\t\t\t)\n\t\t);\n\t}\n\tawait loadFsEvents();\n\tconst { Watcher } = await import('./watch');\n\tnew Watcher(watchOptionsList, emitter);\n}\n",
      "    'use strict';\n\n    var a = () => {\n        console.log('props');\n    };\n\n    a();\n    a();\n\n})();\n"
    ]
  },
  {
    "id": "webpack/webpack",
    "org": "webpack",
    "avatarURL": "https://avatars.githubusercontent.com/u/2105791?v=4",
    "name": "webpack/webpack",
    "url": "https://github.com/webpack/webpack",
    "lang": "JavaScript",
    "desc": "A bundler for JavaScript and friends.",
    "star_num": 63478,
    "fork_num": 8846,
    "snippets": [
      "const crypto = require(\"crypto\");\nconst fs = require(\"fs\");\nconst path = require(\"path\");\n\nit(\"should result in the correct HTML\", () => {\n\tconst content = fs.readFileSync(\n\t\tpath.resolve(__dirname, \"index.html\"),\n\t\t\"utf-8\"\n\t);\n\n\t// check minimized\n\texpect(content).toMatch(/<\\/script> <script/);\n\n\t// check inlined js is minimized\n\texpect(content).toMatch(/For license information please see inline-/);\n\n\t// contains references to normal-[contenthash].js\n\texpect(content).toMatch(/normal-.{20}\\.js/);\n\n\tconst [filename] = /normal-.{20}\\.js/.exec(content);\n\tconst normalJs = fs.readFileSync(path.resolve(__dirname, filename));\n\tconst hash = crypto.createHash(\"sha512\");\n\thash.update(normalJs);\n\tconst digest = hash.digest(\"base64\");\n\n\t// SRI has been updated and matched content\n\texpect(content).toContain(digest);\n});\n",
      "\t\t});\n\n\t\tdescribe(\"when created\", () => {\n\t\t\tbeforeEach(() => {\n\t\t\t\tenv.nullDependencyTemplate = new NullDependency.Template();\n\t\t\t});\n\n\t\t\tit(\"has apply function\", () => {\n\t\t\t\texpect(env.nullDependencyTemplate.apply).toBeTypeOf(\"function\");\n\t\t\t});\n\t\t});\n\t});\n});\n",
      "it(\"should load the component from container\", () => {\n\treturn import(\"./App\").then(({ default: App }) => {\n\t\tconst rendered = App();\n\t\texpect(rendered).toBe(\n\t\t\t\"App rendered with [This is react 2.1.0] and [ComponentC rendered with [This is react 2.1.0] and [ComponentA rendered with [This is react 2.1.0]] and [ComponentB rendered with [This is react 2.1.0]]]\"\n\t\t);\n\t\treturn import(\"./upgrade-react\").then(({ default: upgrade }) => {\n\t\t\tupgrade();\n\t\t\tconst rendered = App();\n\t\t\texpect(rendered).toBe(\n\t\t\t\t\"App rendered with [This is react 9] and [ComponentC rendered with [This is react 9] and [ComponentA rendered with [This is react 9]] and [ComponentB rendered with [This is react 9]]]\"\n\t\t\t);\n\t\t});\n\t});\n});\n",
      "import png from \"./images/file.png\";\nimport svg from \"./images/file.svg\";\nimport jpg from \"./images/file.jpg\";\nimport html from \"./static/file.html\";\nimport text1 from \"data:text/plain;base64,szsaAAdsadasdfafasfasAADas123aasdasd==\";\nimport text2 from \"data:text/plain,asd=\";\nimport text3 from \"data:text/plain,XXXXXXXXXXXXXXXXX\"; // 17 chars\nimport css from \"./a.css\";\nimport source from \"./a.source\";\n",
      "\t\tfor(let i = 1; i < 100; i++)\n\t\t\tview[i] = i;\n\t\tconst result = wasm._Z4sendPcS_m(200, 1, 100);\n\t\tfor(let i = 1; i < 100; i++)\n\t\t\texpect(view[199 + i]).toEqual(i);\n\t});\n});\n",
      "const System = require(\"../../../helpers/fakeSystem\");\n\nmodule.exports = {\n\ttarget: \"web\",\n\tbeforeExecute: () => {\n\t\tSystem.init();\n\t},\n\tmoduleScope(scope) {\n\t\tscope.window.windowExt = \"works\";\n\t\tscope.rootExt = \"works\";\n\t\tscope.varExt = \"works\";\n\t\tscope.System = System;\n\t},\n\tafterExecute: () => {\n\t\tSystem.execute(\"(anonym)\");\n\t}\n};\n",
      "\tmode: \"production\",\n\toptimization: { chunkIds: false },\n\tentry: {\n\t\tentry: \"./entry\"\n\t},\n\tplugins: [new NamedChunkIdsPlugin()]\n};\n",
      "\t\t\tmoduleIds: \"named\",\n\t\t\tchunkIds: \"named\"\n\t\t},\n\t\tplugins: [\n\t\t\tc => {\n\t\t\t\tconst webpack = require(\"..\");\n\t\t\t\tnew webpack.HotModuleReplacementPlugin().apply(c);\n\t\t\t}\n\t\t]\n\t});\n});\n",
      "class ThrowsExceptionInRender {\n\tapply(compiler) {\n\t\tcompiler.hooks.compilation.tap(\"ThrowsException\", compilation => {\n\t\t\tcompilation.mainTemplate.hooks.requireExtensions.tap(\n\t\t\t\t\"ThrowsException\",\n\t\t\t\t() => {\n\t\t\t\t\tthrow new Error(\"Test exception\");\n\t\t\t\t}\n\t\t\t);\n\t\t});\n\t}\n}\n\n/** @type {import(\"../../../../\").Configuration} */\nmodule.exports = {\n\tplugins: [new ThrowsExceptionInRender()]\n};\n",
      "\t},\n\ttarget: \"web\",\n\toutput: {\n\t\tfilename: \"[name].js\"\n\t}\n};\n"
    ]
  },
  {
    "id": "bazelbuild/bazel",
    "org": "bazelbuild",
    "avatarURL": "https://avatars.githubusercontent.com/u/11684617?v=4",
    "name": "bazelbuild/bazel",
    "url": "https://github.com/bazelbuild/bazel",
    "lang": "Java",
    "desc": "A fast, scalable, multi-language and extensible build system.",
    "star_num": 21321,
    "fork_num": 3852,
    "snippets": [
      " * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n * FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for\n * more details.\n *\n * You should have received a copy of the GNU General Public License along\n * with this program; if not, write to the Free Software Foundation, Inc.,\n * 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA\n */\npackage proguard.optimize.peephole;\n\nimport proguard.classfile.*;\nimport proguard.classfile.editor.MethodInvocationFixer;\nimport proguard.classfile.util.*;\nimport proguard.classfile.visitor.MemberVisitor;\nimport proguard.optimize.info.NonPrivateMemberMarker;\n\n/**\n * This MemberVisitor makes all class members that it visits private, unless\n * they have been marked by a NonPrivateMemberMarker. The invocations of\n * privatized methods still have to be fixed.\n *\n * @see NonPrivateMemberMarker\n * @see MethodInvocationFixer\n * @author Eric Lafortune\n */\npublic class MemberPrivatizer\nextends      SimplifiedVisitor\nimplements   MemberVisitor\n{\n    private final MemberVisitor extraMemberVisitor;\n\n\n    /**\n     * Creates a new MemberPrivatizer.\n     */\n    public MemberPrivatizer()\n    {\n        this(null);\n    }\n\n\n    /**\n     * Creates a new MemberPrivatizer.\n     * @param extraMemberVisitor an optional extra visitor for all privatized\n     *                           class members.\n     */\n    public MemberPrivatizer(MemberVisitor extraMemberVisitor)\n    {\n        this.extraMemberVisitor = extraMemberVisitor;\n    }\n\n\n    // Implementations for MemberVisitor.\n\n    public void visitProgramField(ProgramClass programClass, ProgramField programField)\n    {\n        // Is the field unmarked?\n        if (NonPrivateMemberMarker.canBeMadePrivate(programField))\n        {\n            // Make the field private.\n            programField.u2accessFlags =\n                AccessUtil.replaceAccessFlags(programField.u2accessFlags,\n                                              ClassConstants.ACC_PRIVATE);\n",
      "// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//    http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage com.google.devtools.build.lib.rules.java;\n\nimport com.google.auto.value.AutoValue;\nimport com.google.devtools.build.lib.actions.Artifact;\nimport javax.annotation.Nullable;\n\n/** The outputs of a {@link JavaCompileAction}. */\n@AutoValue\npublic abstract class JavaCompileOutputs<T extends Artifact> {\n\n  /** The class jar Artifact to create with the Action */\n  public abstract T output();\n\n  /** The output artifact for the manifest proto emitted from JavaBuilder */\n  public abstract T manifestProto();\n\n  @Nullable\n  public abstract T depsProto();\n\n  /** The generated class jar, or {@code null} if no annotation processing is expected. */\n  @Nullable\n  public abstract T genClass();\n\n  /**\n   * The generated sources jar Artifact to create with the Action (null if no sources will be\n   * generated).\n   */\n  @Nullable\n  public abstract T genSource();\n\n  /** An archive of generated native header files. */\n  @Nullable\n  public abstract T nativeHeader();\n\n  static <T extends Artifact> Builder<T> builder() {\n    return new AutoValue_JavaCompileOutputs.Builder<>();\n  }\n\n  public abstract Builder<T> toBuilder();\n\n  public JavaCompileOutputs<T> withOutput(T output) {\n    return toBuilder().output(output).build();\n  }\n\n  @AutoValue.Builder\n  abstract static class Builder<T extends Artifact> {\n\n    abstract Builder<T> output(T artifact);\n\n    abstract Builder<T> manifestProto(T artifact);\n\n    abstract Builder<T> depsProto(@Nullable T artifact);\n",
      "}\n",
      "// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage com.google.devtools.build.lib.skyframe.serialization.autocodec;\n\nimport com.google.common.base.Preconditions;\nimport com.squareup.javapoet.MethodSpec;\nimport com.squareup.javapoet.TypeName;\nimport javax.annotation.processing.ProcessingEnvironment;\nimport javax.lang.model.type.DeclaredType;\nimport javax.lang.model.type.PrimitiveType;\nimport javax.lang.model.type.TypeMirror;\n\n/**\n * Generates serialize and deserialize code fragments.\n *\n * <p>All methods are logically static and take the {@link ProcessingEnvironment} as a parameter.\n */\ninterface SerializationCodeGenerator {\n  class Context {\n    /** Builder for the method. */\n    public final MethodSpec.Builder builder;\n    /** Type of {@code name}. */\n    public final TypeMirror type;\n    /** Name of variable. */\n    public final String name;\n    /**\n     * Recursion depth.\n     *\n     * <p>Recursion is used to traverse generic types.\n     */\n    public final int depth;\n\n    Context(MethodSpec.Builder builder, TypeMirror type, String name) {\n      this(builder, type, name, 0);\n    }\n\n    private Context(MethodSpec.Builder builder, TypeMirror type, String name, int depth) {\n      this.builder = builder;\n      this.type = type;\n      this.name = name;\n      this.depth = depth;\n    }\n\n    /** Returns a new context with a new type and name at the next recursion depth. */\n    Context with(TypeMirror newType, String newName) {\n      return new Context(builder, newType, newName, depth + 1);\n    }\n\n    TypeName getTypeName() {\n      return TypeName.get(type);\n    }\n\n    DeclaredType getDeclaredType() {\n      Preconditions.checkState(type instanceof DeclaredType, \"Expected DeclaredType, was \" + type);\n      return (DeclaredType) type;\n    }\n\n    boolean isDeclaredType() {\n      return type instanceof DeclaredType;\n    }",
      "// Copyright 2018 The Bazel Authors. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//    http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n/** A class that results in nestmate attributes in Java 11. */\npublic class NestTest {\n  public static class P {\n    public void f() {}\n  }\n\n  public static void main(String[] args) {\n    new P().f();\n  }\n}\n",
      "//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\npackage com.google.devtools.build.android.ziputils;\n\nimport static com.google.common.truth.Truth.assertWithMessage;\nimport static org.junit.Assert.assertThrows;\n\nimport java.nio.ByteBuffer;\nimport java.util.Arrays;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.junit.runners.JUnit4;\n\n/**\n * Unit tests for {@link ScanUtil}.\n */\n@RunWith(JUnit4.class)\npublic class ScanUtilTest {\n\n  @Test\n  public void testScanTo() {\n    assertThrows(NullPointerException.class, () -> assertLocation(null, new byte[] {}, -1));\n    assertThrows(NullPointerException.class, () -> assertLocation(new byte[] {}, null, -1));\n    assertLocation(new byte[] {}, new byte[] {}, -1);\n    assertLocation(new byte[] {}, new byte[] {1}, 0);\n    assertLocation(new byte[] {}, new byte[] {1, 2, 3, 4}, 0);\n    assertLocation(new byte[] {1}, new byte[] {}, -1);\n    assertLocation(new byte[] {1}, new byte[] {1}, 0);\n    assertLocation(new byte[] {1}, new byte[] {1, 2, 3, 4}, 0);\n    assertLocation(new byte[] {1}, new byte[] {5, 4, 1, 2, 3, 4}, 2);\n    assertLocation(new byte[] {1}, new byte[] {4, 2, 3, 1}, 3);\n    assertLocation(new byte[] {1, 2, 3, 4}, new byte[] {}, -1);\n    assertLocation(new byte[] {1, 2, 3, 4}, new byte[] {1}, -1);\n    assertLocation(new byte[] {1, 2, 3, 4}, new byte[] {1, 2, 3, 4}, 0);\n    assertLocation(new byte[] {1, 2, 3, 4}, new byte[] {1, 2, 3, 4, 1, 2, 3, 4}, 0);\n    assertLocation(new byte[] {1, 2, 3, 4}, new byte[] {5, 1, 2, 3, 4}, 1);\n    assertLocation(new byte[] {1, 2, 3, 4}, new byte[] {5, 1, 2, 3, 4, 5}, 1);\n    assertLocation(new byte[] {1, 2, 3, 4}, new byte[] {5, 5, 1, 2, 3, 4}, 2);\n    assertLocation(new byte[] {1, 2, 3, 4}, new byte[] {5, 1, 1, 2, 3, 4}, 2);\n    assertLocation(new byte[] {1, 2, 3, 4}, new byte[] {5, 1, 2, 3, 5, 4}, -1);\n  }\n\n  @Test\n  public void testScanBackwardsTo() {\n    assertThrows(\n        NullPointerException.class, () -> assertBackwardsLocation(null, new byte[] {}, -1));\n    assertThrows(\n        NullPointerException.class, () -> assertBackwardsLocation(new byte[] {}, null, -1));\n    assertBackwardsLocation(new byte[] {}, new byte[] {}, -1);\n    assertBackwardsLocation(new byte[] {}, new byte[] {1}, 0);\n    assertBackwardsLocation(new byte[] {}, new byte[] {1, 2, 3, 4}, 3);\n    assertBackwardsLocation(new byte[] {1}, new byte[] {}, -1);\n    assertBackwardsLocation(new byte[] {1}, new byte[] {1}, 0);\n    assertBackwardsLocation(new byte[] {1}, new byte[] {1, 2, 3, 4}, 0);\n    assertBackwardsLocation(new byte[] {1}, new byte[] {5, 4, 1, 2, 3, 4}, 2);\n    assertBackwardsLocation(new byte[] {1}, new byte[] {4, 2, 3, 1}, 3);\n    assertBackwardsLocation(new byte[] {1, 2, 3, 4}, new byte[] {}, -1);\n    assertBackwardsLocation(new byte[] {1, 2, 3, 4}, new byte[] {1}, -1);\n    assertBackwardsLocation(new byte[] {1, 2, 3, 4}, new byte[] {1, 2, 3, 4}, 0);\n    assertBackwardsLocation(new byte[] {1, 2, 3, 4}, new byte[] {1, 2, 3, 4, 1, 2, 3, 4}, 4);",
      "//\n//    http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\npackage com.google.devtools.build.lib.collect.nestedset;\n\nimport static com.google.common.truth.Truth.assertThat;\nimport static com.google.common.util.concurrent.Futures.immediateFailedFuture;\nimport static com.google.common.util.concurrent.Futures.immediateVoidFuture;\nimport static com.google.common.util.concurrent.MoreExecutors.directExecutor;\nimport static org.junit.Assert.assertThrows;\nimport static org.mockito.ArgumentMatchers.any;\nimport static org.mockito.ArgumentMatchers.eq;\nimport static org.mockito.Mockito.doAnswer;\nimport static org.mockito.Mockito.doReturn;\nimport static org.mockito.Mockito.mock;\nimport static org.mockito.Mockito.spy;\nimport static org.mockito.Mockito.times;\nimport static org.mockito.Mockito.verify;\nimport static org.mockito.Mockito.when;\n\nimport com.google.auto.value.AutoValue;\nimport com.google.common.collect.ImmutableClassToInstanceMap;\nimport com.google.common.collect.ImmutableList;\nimport com.google.common.collect.Lists;\nimport com.google.common.testing.GcFinalization;\nimport com.google.common.util.concurrent.ListenableFuture;\nimport com.google.common.util.concurrent.SettableFuture;\nimport com.google.devtools.build.lib.bugreport.BugReporter;\nimport com.google.devtools.build.lib.collect.nestedset.NestedSetStore.InMemoryNestedSetStorageEndpoint;\nimport com.google.devtools.build.lib.collect.nestedset.NestedSetStore.MissingNestedSetException;\nimport com.google.devtools.build.lib.collect.nestedset.NestedSetStore.NestedSetStorageEndpoint;\nimport com.google.devtools.build.lib.skyframe.serialization.AutoRegistry;\nimport com.google.devtools.build.lib.skyframe.serialization.DeserializationContext;\nimport com.google.devtools.build.lib.skyframe.serialization.ObjectCodec;\nimport com.google.devtools.build.lib.skyframe.serialization.ObjectCodecRegistry;\nimport com.google.devtools.build.lib.skyframe.serialization.ObjectCodecs;\nimport com.google.devtools.build.lib.skyframe.serialization.SerializationContext;\nimport com.google.devtools.build.lib.skyframe.serialization.SerializationDependencyProvider;\nimport com.google.devtools.build.lib.skyframe.serialization.SerializationException;\nimport com.google.devtools.build.lib.skyframe.serialization.SerializationResult;\nimport com.google.devtools.build.lib.util.io.AnsiTerminal.Color;\nimport com.google.protobuf.ByteString;\nimport com.google.protobuf.CodedInputStream;\nimport com.google.protobuf.CodedOutputStream;\nimport java.io.IOException;\nimport java.lang.ref.WeakReference;\nimport java.nio.charset.Charset;\nimport java.util.List;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.function.Function;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.junit.runners.JUnit4;\nimport org.mockito.ArgumentCaptor;\n",
      "@FunctionalInterface\ninterface ArgsPreProcessor {\n  List<String> preProcess(List<String> args) throws OptionsParsingException;\n}\n",
      "  }\n\n  @Override\n  public byte[] get(\n      Supplier<BuildConfigurationValue> configurationSupplier, CommandEnvironment env) {\n    return print(\n        String.format(\n            \"file.encoding = %s, defaultCharset = %s\",\n            System.getProperty(\"file.encoding\", \"unknown\"), Charset.defaultCharset().name()));\n  }\n}\n",
      "  /**\n   * Retrieves yanked versions of the module identified by {@code key.getName()} from the registry.\n   * Returns {@code Optional.empty()} when the information is not found in the registry.\n   */\n  Optional<ImmutableMap<Version, String>> getYankedVersions(\n      String moduleName, ExtendedEventHandler eventHandler)\n      throws IOException, InterruptedException;\n}\n"
    ]
  },
  {
    "id": "apache/cassandra",
    "org": "apache",
    "avatarURL": "https://avatars.githubusercontent.com/u/47359?v=4",
    "name": "apache/cassandra",
    "url": "https://github.com/apache/cassandra",
    "lang": "Java",
    "desc": "The Apache Cassandra database.",
    "star_num": 8184,
    "fork_num": 3461,
    "snippets": [
      "\n    @Override\n    public void execute(NodeProbe probe)\n    {\n        try\n        {\n            probe.move(newToken);\n        } catch (IOException e)\n        {\n            throw new RuntimeException(\"Error during moving node\", e);\n        }\n    }\n}\n",
      "        final IndexDescriptor indexDescriptor = newIndexDescriptor();\n        final String index = newIndex();\n        final IndexContext indexContext = SAITester.createIndexContext(index, UTF8Type.instance);\n        final List<Pair<ByteComparable, LongArrayList>> termsEnum = buildTermsEnum(numTerms, numPostings);\n\n        SegmentMetadata.ComponentMetadataMap indexMetas;\n        try (LiteralIndexWriter writer = new LiteralIndexWriter(indexDescriptor, indexContext))\n        {\n            indexMetas = writer.writeCompleteSegment(new MemtableTermsIterator(null, null, termsEnum.iterator()));\n        }\n\n        FileHandle termsData = indexDescriptor.createPerIndexFileHandle(IndexComponent.TERMS_DATA, indexContext, null);\n        FileHandle postingLists = indexDescriptor.createPerIndexFileHandle(IndexComponent.POSTING_LISTS, indexContext, null);\n\n        long termsFooterPointer = Long.parseLong(indexMetas.get(IndexComponent.TERMS_DATA).attributes.get(SAICodecUtils.FOOTER_POINTER));\n\n        try (LiteralIndexSegmentTermsReader reader = new LiteralIndexSegmentTermsReader(indexContext,\n                                                                                        termsData,\n                                                                                        postingLists,\n                                                                                        indexMetas.get(IndexComponent.TERMS_DATA).root,\n                                                                                        termsFooterPointer))\n        {\n            for (Pair<ByteComparable, LongArrayList> pair : termsEnum)\n            {\n                final byte[] bytes = ByteSourceInverse.readBytes(pair.left.asComparableBytes(ByteComparable.Version.OSS50));\n                QueryEventListener.TrieIndexEventListener listener = mock(QueryEventListener.TrieIndexEventListener.class);\n                when(listener.postingListEventListener()).thenReturn(mock(QueryEventListener.PostingListEventListener.class));\n                try (PostingList actualPostingList = reader.exactMatch(ByteComparable.fixedLength(bytes),\n                                                                       listener,\n                                                                       mock(QueryContext.class)))\n                {\n                    final LongArrayList expectedPostingList = pair.right;\n\n                    assertNotNull(actualPostingList);\n                    assertEquals(expectedPostingList.size(), actualPostingList.size());\n\n                    for (int i = 0; i < expectedPostingList.size(); ++i)\n                    {\n                        final long expectedRowID = expectedPostingList.get(i);\n                        long result = actualPostingList.nextPosting();\n                        assertEquals(String.format(\"row %d mismatch of %d in enum %d\", i, expectedPostingList.size(), termsEnum.indexOf(pair)), expectedRowID, result);\n                    }\n\n                    long lastResult = actualPostingList.nextPosting();\n                    assertEquals(PostingList.END_OF_STREAM, lastResult);\n                }\n\n                // test skipping\n                try (PostingList actualPostingList = reader.exactMatch(ByteComparable.fixedLength(bytes),\n                                                                       listener,\n                                                                       mock(QueryContext.class)))\n                {\n                    final LongArrayList expectedPostingList = pair.right;\n                    // test skipping to the last block\n                    final int idxToSkip = numPostings - 2;\n                    // tokens are equal to their corresponding row IDs\n                    final long tokenToSkip = expectedPostingList.get(idxToSkip);\n\n                    long advanceResult = actualPostingList.advance(tokenToSkip);\n                    assertEquals(tokenToSkip, advanceResult);\n\n                    for (int i = idxToSkip + 1; i < expectedPostingList.size(); ++i)\n                    {\n                        final long expectedRowID = expectedPostingList.get(i);",
      "\n    @Deprecated\n    public static final Meter repairedBackground = Metrics.meter(factory.createMetricName(\"RepairedBackground\"));\n    @Deprecated\n    public static final Meter attempted = Metrics.meter(factory.createMetricName(\"Attempted\"));\n    public static final Meter timedOut = Metrics.meter(factory.createMetricName(\"RepairTimedOut\"));\n\n    // Incremented when additional requests were sent during blocking read repair due to unavailable or slow nodes\n    public static final Meter speculatedRead = Metrics.meter(factory.createMetricName(\"SpeculatedRead\"));\n    public static final Meter speculatedWrite = Metrics.meter(factory.createMetricName(\"SpeculatedWrite\"));\n\n    public static void init() {}\n}\n",
      "\n    @Test\n    public void testConcurrentValidations()\n    {\n        Config conf = new Config();\n        conf.concurrent_compactors = 8;\n        // if concurrent_validations is < 1 (including being unset) it should default to concurrent_compactors\n        assertThat(conf.concurrent_validations).isLessThan(1);\n        DatabaseDescriptor.applyConcurrentValidations(conf);\n        assertThat(conf.concurrent_validations).isEqualTo(conf.concurrent_compactors);\n\n        // otherwise, it must be <= concurrent_compactors\n        conf.concurrent_validations = conf.concurrent_compactors + 1;\n        try\n        {\n            DatabaseDescriptor.applyConcurrentValidations(conf);\n            fail(\"Expected exception\");\n        }\n        catch (ConfigurationException e)\n        {\n            assertThat(e.getMessage()).isEqualTo(\"To set concurrent_validations > concurrent_compactors, \" +\n                                                 \"set the system property -D\" + ALLOW_UNLIMITED_CONCURRENT_VALIDATIONS.getKey() + \"=true\");\n        }\n\n        // unless we disable that check (done with a system property at startup or via JMX)\n        DatabaseDescriptor.allowUnlimitedConcurrentValidations = true;\n        conf.concurrent_validations = conf.concurrent_compactors + 1;\n        DatabaseDescriptor.applyConcurrentValidations(conf);\n        assertThat(conf.concurrent_validations).isEqualTo(conf.concurrent_compactors + 1);\n    }\n\n    @Test\n    public void testRepairCommandPoolSize()\n    {\n        Config conf = new Config();\n        conf.concurrent_validations = 3;\n        // if repair_command_pool_size is < 1 (including being unset) it should default to concurrent_validations\n        assertThat(conf.repair_command_pool_size).isLessThan(1);\n        DatabaseDescriptor.applyRepairCommandPoolSize(conf);\n        assertThat(conf.repair_command_pool_size).isEqualTo(conf.concurrent_validations);\n\n        // but it can be overridden\n        conf.repair_command_pool_size = conf.concurrent_validations + 1;\n        DatabaseDescriptor.applyRepairCommandPoolSize(conf);\n        assertThat(conf.repair_command_pool_size).isEqualTo(conf.concurrent_validations + 1);\n    }\n\n    @Test\n    public void testApplyTokensConfigInitialTokensSetNumTokensSetAndDoesMatch()\n    {\n        Config config = DatabaseDescriptor.loadConfig();\n        config.initial_token = \"0,256,1024\";\n        config.num_tokens = 3;\n\n        try\n        {\n            DatabaseDescriptor.applyTokensConfig(config);\n            Assert.assertEquals(Integer.valueOf(3), config.num_tokens);\n            Assert.assertEquals(3, DatabaseDescriptor.tokensFromString(config.initial_token).size());\n        }\n        catch (ConfigurationException e)\n        {\n            Assert.fail(\"number of tokens in initial_token=0,256,1024 does not match num_tokens = 3\");\n        }",
      "import java.util.Collections;\nimport java.util.HashSet;\nimport java.util.Set;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.apache.cassandra.index.sai.IndexContext;\nimport org.apache.cassandra.index.sai.IndexValidation;\nimport org.apache.cassandra.index.sai.SSTableContext;\nimport org.apache.cassandra.index.sai.disk.SSTableIndex;\nimport org.apache.cassandra.index.sai.StorageAttachedIndexGroup;\nimport org.apache.cassandra.io.sstable.format.SSTableReader;\nimport org.apache.cassandra.utils.Pair;\n\n/**\n * Maintain an atomic view for read requests, so that requests can read all data during concurrent compactions.\n *\n * All per-column {@link SSTableIndex} updates should be proxied by {@link StorageAttachedIndexGroup} to make\n * sure per-sstable {@link SSTableContext} are in-sync.\n */\npublic class IndexViewManager\n{\n    private static final Logger logger = LoggerFactory.getLogger(IndexViewManager.class);\n    \n    private final IndexContext context;\n    private final AtomicReference<View> view = new AtomicReference<>();\n\n    public IndexViewManager(IndexContext context)\n    {\n        this.context = context;\n        this.view.set(new View(context, Collections.emptySet()));\n    }\n\n    public View getView()\n    {\n        return view.get();\n    }\n\n    /**\n     * Replaces old SSTables with new by creating new immutable view.\n     *\n     * @param oldSSTables A set of SSTables to remove.\n     * @param newSSTableContexts A set of SSTableContexts to add to tracker.\n     * @param validation Controls how indexes should be validated\n     *\n     * @return A set of SSTables which have attached to them invalid index components.\n     */\n    public Collection<SSTableContext> update(Collection<SSTableReader> oldSSTables, Collection<SSTableContext> newSSTableContexts, IndexValidation validation)\n    {\n        // Valid indexes on the left and invalid SSTable contexts on the right...\n        Pair<Collection<SSTableIndex>, Collection<SSTableContext>> indexes = context.getBuiltIndexes(newSSTableContexts, validation);\n\n        View currentView, newView;\n        Collection<SSTableIndex> newViewIndexes = new HashSet<>();\n        Collection<SSTableIndex> releasableIndexes = new ArrayList<>();\n\n        do\n        {\n            currentView = view.get();\n            newViewIndexes.clear();\n            releasableIndexes.clear();\n",
      "}\n",
      "    {\n        channel = new TestChannel();\n        streamingChannel = new NettyStreamingChannel(channel, StreamingChannel.Kind.CONTROL);\n        channel.pipeline().addLast(\"stream\", streamingChannel);\n    }\n\n    @After\n    public void tearDown()\n    {\n        if (buf != null)\n        {\n            while (buf.refCnt() > 0)\n                buf.release();\n        }\n\n        channel.close();\n    }\n\n    @Test\n    public void channelRead_WrongObject()\n    {\n        channel.writeInbound(\"homer\");\n        Assert.assertEquals(0, streamingChannel.in.unsafeAvailable());\n        Assert.assertFalse(channel.releaseInbound());\n    }\n\n    @Test\n    public void StreamDeserializingTask_deriveSession_StreamInitMessage()\n    {\n        StreamInitMessage msg = new StreamInitMessage(REMOTE_ADDR, 0, nextTimeUUID(), StreamOperation.REPAIR, nextTimeUUID(), PreviewKind.ALL);\n        StreamDeserializingTask task = new StreamDeserializingTask(null, streamingChannel, MessagingService.current_version);\n        StreamSession session = task.deriveSession(msg);\n        Assert.assertNotNull(session);\n    }\n\n    @Test (expected = UnsupportedOperationException.class)\n    public void StreamDeserializingTask_deriveSession_NoSession()\n    {\n        CompleteMessage msg = new CompleteMessage();\n        StreamDeserializingTask task = new StreamDeserializingTask(null, streamingChannel, MessagingService.current_version);\n        task.deriveSession(msg);\n    }\n\n    @Test (expected = IllegalStateException.class)\n    public void StreamDeserializingTask_deserialize_ISM_NoSession() throws IOException\n    {\n        StreamMessageHeader header = new StreamMessageHeader(TableId.generate(), REMOTE_ADDR, nextTimeUUID(), true,\n                                                             0, 0, 0, nextTimeUUID());\n\n        ByteBuffer temp = ByteBuffer.allocate(1024);\n        DataOutputPlus out = new DataOutputBuffer(temp);\n        StreamMessageHeader.serializer.serialize(header, out, MessagingService.current_version);\n\n        temp.flip();\n        DataInputPlus in = new DataInputBuffer(temp, false);\n        // session not found\n        IncomingStreamMessage.serializer.deserialize(in, MessagingService.current_version);\n    }\n\n    @Test\n    public void StreamDeserializingTask_deserialize_ISM_HasSession()\n    {\n        TimeUUID planId = nextTimeUUID();\n        StreamResultFuture future = StreamResultFuture.createFollower(0, planId, StreamOperation.REPAIR, REMOTE_ADDR, streamingChannel, MessagingService.current_version, nextTimeUUID(), PreviewKind.ALL);",
      "    {\n        TrieMemoryIndex index = newTrieMemoryIndex(Int32Type.instance);\n        for (int i = 0; i < 99; i++)\n        {\n            assertTrue(index.add(key, Clustering.EMPTY, Int32Type.instance.decompose(i)) > 0);\n        }\n    }\n\n    @Test\n    public void randomQueryTest() throws Exception\n    {\n        TrieMemoryIndex index = newTrieMemoryIndex(Int32Type.instance);\n\n        Map<DecoratedKey, Integer> keyMap = new TreeMap<>();\n        Map<Integer, Integer> rowMap = new HashMap<>();\n\n        for (int row = 0; row < getRandom().nextIntBetween(1000, 5000); row++)\n        {\n            int pk = getRandom().nextIntBetween(0, 10000);\n            while (rowMap.containsKey(pk))\n                pk = getRandom().nextIntBetween(0, 10000);\n            int value = getRandom().nextIntBetween(0, 100);\n            rowMap.put(pk, value);\n            DecoratedKey key = Murmur3Partitioner.instance.decorateKey(Int32Type.instance.decompose(pk));\n            index.add(key, Clustering.EMPTY, Int32Type.instance.decompose(value));\n            keyMap.put(key, pk);\n        }\n\n        List<DecoratedKey> keys = new ArrayList<>(keyMap.keySet());\n\n        for (int executionCount = 0; executionCount < 1000; executionCount++)\n        {\n            Expression expression = generateRandomExpression();\n\n            AbstractBounds<PartitionPosition> keyRange = generateRandomBounds(keys);\n\n            Set<Integer> expectedKeys = keyMap.keySet()\n                                              .stream()\n                                              .filter(keyRange::contains)\n                                              .map(keyMap::get)\n                                              .filter(pk -> expression.isSatisfiedBy(Int32Type.instance.decompose(rowMap.get(pk))))\n                                              .collect(Collectors.toSet());\n\n            Set<Integer> foundKeys = new HashSet<>();\n\n            try (KeyRangeIterator iterator = index.search(expression, keyRange))\n            {\n                while (iterator.hasNext())\n                {\n                    int key = Int32Type.instance.compose(iterator.next().partitionKey().getKey());\n                    assertFalse(foundKeys.contains(key));\n                    foundKeys.add(key);\n                }\n            }\n\n            assertEquals(expectedKeys, foundKeys);\n        }\n    }\n\n    private AbstractBounds<PartitionPosition> generateRandomBounds(List<DecoratedKey> keys)\n    {\n        PartitionPosition leftBound = getRandom().nextBoolean() ? Murmur3Partitioner.instance.getMinimumToken().minKeyBound()\n                                                                : keys.get(getRandom().nextIntBetween(0, keys.size() - 1)).getToken().minKeyBound();\n",
      "                                            throw iqe;\n                                    }\n                                    catch (Throwable t)\n                                    {\n                                        throw t;\n                                    }\n                                    break;\n                                case BOUNCE_CLIENT:\n                                    if (System.nanoTime() < reconnectAfter)\n                                        break;\n\n                                    if (!reconnected)\n                                    {\n                                        for (Session s : sessions.values())\n                                            s.close();\n                                        cluster.close();\n                                        cluster = clusterSupplier.get();\n                                        for (int j = 0; j < KEYSPACES; j++)\n                                            sessions.put(\"ks\" + j, cluster.connect(\"ks\" + j));\n                                        qualifiedStatements.clear();\n                                        unqualifiedStatements.clear();\n                                        reconnected = true;\n                                    }\n\n\n                                    break;\n                            }\n                        }\n                    }\n                    catch (Throwable t)\n                    {\n                        interrupt.set(true);\n                        t.printStackTrace();\n                        while (true)\n                        {\n                            Throwable seen = thrown.get();\n                            Throwable merged = Throwables.merge(seen, t);\n                            if (thrown.compareAndSet(seen, merged))\n                                break;\n                        }\n                        throw t;\n                    }\n                    finally\n                    {\n                        logger.info(\"Exiting...\");\n                        if (cluster != null)\n                            cluster.close();\n                    }\n                }));\n            }\n\n            for (Thread thread : threads)\n                thread.start();\n\n            for (Thread thread : threads)\n                thread.join();\n\n            if (thrown.get() != null)\n                throw thrown.get();\n        }\n    }\n\n    private enum Action\n    {",
      "        return runRepair(parentSession, false, executor, commonRanges, cfnames);\n    }\n}\n"
    ]
  },
  {
    "id": "redis/redis",
    "org": "redis",
    "avatarURL": "https://avatars.githubusercontent.com/u/1529926?v=4",
    "name": "redis/redis",
    "url": "https://github.com/redis/redis",
    "lang": "C",
    "desc": "Redis is an in-memory database that persists on disk. It also provides Lua scripting, replication, and more.",
    "star_num": 61372,
    "fork_num": 22833,
    "snippets": [
      " * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n *   * Redistributions of source code must retain the above copyright notice,\n *     this list of conditions and the following disclaimer.\n *   * Redistributions in binary form must reproduce the above copyright\n *     notice, this list of conditions and the following disclaimer in the\n *     documentation and/or other materials provided with the distribution.\n *   * Neither the name of Redis nor the names of its contributors may be used\n *     to endorse or promote products derived from this software without\n *     specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n */\n\n#include \"server.h\"\n#include \"hiredis.h\"\n#if USE_OPENSSL == 1 /* BUILD_YES */\n#include \"openssl/ssl.h\"\n#include \"hiredis_ssl.h\"\n#endif\n#include \"async.h\"\n\n#include <ctype.h>\n#include <arpa/inet.h>\n#include <sys/socket.h>\n#include <sys/wait.h>\n#include <fcntl.h>\n\nextern char **environ;\n\n#if USE_OPENSSL == 1 /* BUILD_YES */\nextern SSL_CTX *redis_tls_ctx;\nextern SSL_CTX *redis_tls_client_ctx;\n#endif\n\n#define REDIS_SENTINEL_PORT 26379\n\n/* ======================== Sentinel global state =========================== */\n\n/* Address object, used to describe an ip:port pair. */\ntypedef struct sentinelAddr {\n    char *hostname;         /* Hostname OR address, as specified */\n    char *ip;               /* Always a resolved address */\n    int port;\n} sentinelAddr;\n\n/* A Sentinel Redis Instance object is monitoring. */\n#define SRI_MASTER  (1<<0)\n#define SRI_SLAVE   (1<<1)\n#define SRI_SENTINEL (1<<2)\n#define SRI_S_DOWN (1<<3)   /* Subjectively down (no quorum). */",
      "\nTEST_BEGIN(test_ticker_tick) {\n#define NREPS 2\n#define NTICKS 3\n\tticker_t ticker;\n\tint32_t i, j;\n\n\tticker_init(&ticker, NTICKS);\n\tfor (i = 0; i < NREPS; i++) {\n\t\tfor (j = 0; j < NTICKS; j++) {\n\t\t\texpect_u_eq(ticker_read(&ticker), NTICKS - j,\n\t\t\t    \"Unexpected ticker value (i=%d, j=%d)\", i, j);\n\t\t\texpect_false(ticker_tick(&ticker),\n\t\t\t    \"Unexpected ticker fire (i=%d, j=%d)\", i, j);\n\t\t}\n\t\texpect_u32_eq(ticker_read(&ticker), 0,\n\t\t    \"Expected ticker depletion\");\n\t\texpect_true(ticker_tick(&ticker),\n\t\t    \"Expected ticker fire (i=%d)\", i);\n\t\texpect_u32_eq(ticker_read(&ticker), NTICKS,\n\t\t    \"Expected ticker reset\");\n\t}\n#undef NTICKS\n}\nTEST_END\n\nTEST_BEGIN(test_ticker_ticks) {\n#define NTICKS 3\n\tticker_t ticker;\n\n\tticker_init(&ticker, NTICKS);\n\n\texpect_u_eq(ticker_read(&ticker), NTICKS, \"Unexpected ticker value\");\n\texpect_false(ticker_ticks(&ticker, NTICKS), \"Unexpected ticker fire\");\n\texpect_u_eq(ticker_read(&ticker), 0, \"Unexpected ticker value\");\n\texpect_true(ticker_ticks(&ticker, NTICKS), \"Expected ticker fire\");\n\texpect_u_eq(ticker_read(&ticker), NTICKS, \"Unexpected ticker value\");\n\n\texpect_true(ticker_ticks(&ticker, NTICKS + 1), \"Expected ticker fire\");\n\texpect_u_eq(ticker_read(&ticker), NTICKS, \"Unexpected ticker value\");\n#undef NTICKS\n}\nTEST_END\n\nTEST_BEGIN(test_ticker_copy) {\n#define NTICKS 3\n\tticker_t ta, tb;\n\n\tticker_init(&ta, NTICKS);\n\tticker_copy(&tb, &ta);\n\texpect_u_eq(ticker_read(&tb), NTICKS, \"Unexpected ticker value\");\n\texpect_true(ticker_ticks(&tb, NTICKS + 1), \"Expected ticker fire\");\n\texpect_u_eq(ticker_read(&tb), NTICKS, \"Unexpected ticker value\");\n\n\tticker_tick(&ta);\n\tticker_copy(&tb, &ta);\n\texpect_u_eq(ticker_read(&tb), NTICKS - 1, \"Unexpected ticker value\");\n\texpect_true(ticker_ticks(&tb, NTICKS), \"Expected ticker fire\");\n\texpect_u_eq(ticker_read(&tb), NTICKS, \"Unexpected ticker value\");\n#undef NTICKS\n}\nTEST_END\n\nTEST_BEGIN(test_ticker_geom) {",
      "\n#include \"lopcodes.h\"\n\n\n/* ORDER OP */\n\nconst char *const luaP_opnames[NUM_OPCODES+1] = {\n  \"MOVE\",\n  \"LOADK\",\n  \"LOADBOOL\",\n  \"LOADNIL\",\n  \"GETUPVAL\",\n  \"GETGLOBAL\",\n  \"GETTABLE\",\n  \"SETGLOBAL\",\n  \"SETUPVAL\",\n  \"SETTABLE\",\n  \"NEWTABLE\",\n  \"SELF\",\n  \"ADD\",\n  \"SUB\",\n  \"MUL\",\n  \"DIV\",\n  \"MOD\",\n  \"POW\",\n  \"UNM\",\n  \"NOT\",\n  \"LEN\",\n  \"CONCAT\",\n  \"JMP\",\n  \"EQ\",\n  \"LT\",\n  \"LE\",\n  \"TEST\",\n  \"TESTSET\",\n  \"CALL\",\n  \"TAILCALL\",\n  \"RETURN\",\n  \"FORLOOP\",\n  \"FORPREP\",\n  \"TFORLOOP\",\n  \"SETLIST\",\n  \"CLOSE\",\n  \"CLOSURE\",\n  \"VARARG\",\n  NULL\n};\n\n\n#define opmode(t,a,b,c,m) (((t)<<7) | ((a)<<6) | ((b)<<4) | ((c)<<2) | (m))\n\nconst lu_byte luaP_opmodes[NUM_OPCODES] = {\n/*       T  A    B       C     mode\t\t   opcode\t*/\n  opmode(0, 1, OpArgR, OpArgN, iABC) \t\t/* OP_MOVE */\n ,opmode(0, 1, OpArgK, OpArgN, iABx)\t\t/* OP_LOADK */\n ,opmode(0, 1, OpArgU, OpArgU, iABC)\t\t/* OP_LOADBOOL */\n ,opmode(0, 1, OpArgR, OpArgN, iABC)\t\t/* OP_LOADNIL */\n ,opmode(0, 1, OpArgU, OpArgN, iABC)\t\t/* OP_GETUPVAL */\n ,opmode(0, 1, OpArgK, OpArgN, iABx)\t\t/* OP_GETGLOBAL */\n ,opmode(0, 1, OpArgR, OpArgK, iABC)\t\t/* OP_GETTABLE */\n ,opmode(0, 0, OpArgK, OpArgN, iABx)\t\t/* OP_SETGLOBAL */\n ,opmode(0, 0, OpArgU, OpArgN, iABC)\t\t/* OP_SETUPVAL */\n ,opmode(0, 0, OpArgK, OpArgK, iABC)\t\t/* OP_SETTABLE */\n ,opmode(0, 1, OpArgU, OpArgU, iABC)\t\t/* OP_NEWTABLE */",
      "\t\t\texpect_true(ckh_search(&ckh, p[j], NULL, NULL),\n\t\t\t    \"Unexpected ckh_search() success\");\n\t\t\texpect_true(ckh_remove(tsd, &ckh, p[j], &q, &r),\n\t\t\t    \"Unexpected ckh_remove() success\");\n\t\t}\n\n\t\t{\n\t\t\tbool seen[NITEMS];\n\t\t\tsize_t tabind;\n\n\t\t\tmemset(seen, 0, sizeof(seen));\n\n\t\t\tfor (tabind = 0; !ckh_iter(&ckh, &tabind, &q, &r);) {\n\t\t\t\tsize_t k;\n\n\t\t\t\texpect_ptr_eq(q, r, \"Key and val not equal\");\n\n\t\t\t\tfor (k = 0; k < NITEMS; k++) {\n\t\t\t\t\tif (p[k] == q) {\n\t\t\t\t\t\texpect_false(seen[k],\n\t\t\t\t\t\t    \"Item %zu already seen\", k);\n\t\t\t\t\t\tseen[k] = true;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor (j = 0; j < i + 1; j++) {\n\t\t\t\texpect_true(seen[j], \"Item %zu not seen\", j);\n\t\t\t}\n\t\t\tfor (; j < NITEMS; j++) {\n\t\t\t\texpect_false(seen[j], \"Item %zu seen\", j);\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (i = 0; i < NITEMS; i++) {\n\t\texpect_false(ckh_search(&ckh, p[i], NULL, NULL),\n\t\t    \"Unexpected ckh_search() failure\");\n\t\texpect_false(ckh_remove(tsd, &ckh, p[i], &q, &r),\n\t\t    \"Unexpected ckh_remove() failure\");\n\t\texpect_ptr_eq(p[i], q, \"Key pointer mismatch\");\n\t\texpect_ptr_eq(p[i], r, \"Value pointer mismatch\");\n\t\texpect_true(ckh_search(&ckh, p[i], NULL, NULL),\n\t\t    \"Unexpected ckh_search() success\");\n\t\texpect_true(ckh_remove(tsd, &ckh, p[i], &q, &r),\n\t\t    \"Unexpected ckh_remove() success\");\n\t\tdallocx(p[i], 0);\n\t}\n\n\texpect_zu_eq(ckh_count(&ckh), 0,\n\t    \"ckh_count() should return %zu, but it returned %zu\",\n\t    ZU(0), ckh_count(&ckh));\n\tckh_delete(tsd, &ckh);\n#undef NITEMS\n}\nTEST_END\n\nint\nmain(void) {\n\treturn test(\n\t    test_new_delete,\n\t    test_count_insert_search_remove,\n\t    test_insert_iter_remove);",
      "        for (i = 0; i < helpEntriesLen; i++) {\n            helpEntry *he = helpEntries+i;\n            if (!strcasecmp(he->argv[0],cmdname))\n                break;\n        }\n        if (i != helpEntriesLen) continue;\n\n        helpEntriesLen++;\n        helpEntries = zrealloc(helpEntries,sizeof(helpEntry)*helpEntriesLen);\n        helpEntry *new = helpEntries+(helpEntriesLen-1);\n\n        new->argc = 1;\n        new->argv = zmalloc(sizeof(sds));\n        new->argv[0] = sdsnew(cmdname);\n        new->full = new->argv[0];\n        new->type = CLI_HELP_COMMAND;\n        sdstoupper(new->argv[0]);\n\n        new->docs.name = new->argv[0];\n        new->docs.args = NULL;\n        new->docs.numargs = 0;\n        new->docs.params = sdsempty();\n        int args = llabs(entry->element[1]->integer);\n        args--; /* Remove the command name itself. */\n        if (entry->element[3]->integer == 1) {\n            new->docs.params = sdscat(new->docs.params,\"key \");\n            args--;\n        }\n        while(args-- > 0) new->docs.params = sdscat(new->docs.params,\"arg \");\n        if (entry->element[1]->integer < 0)\n            new->docs.params = sdscat(new->docs.params,\"...options...\");\n        new->docs.summary = \"Help not available\";\n        new->docs.since = \"Not known\";\n        new->docs.group = \"generic\";\n    }\n    freeReplyObject(reply);\n}\n\n/* Concatenate a string to an sds string, but if it's empty substitute double quote marks. */\nstatic sds sdscat_orempty(sds params, const char *value) {\n    if (value[0] == '\\0') {\n        return sdscat(params, \"\\\"\\\"\");\n    }\n    return sdscat(params, value);\n}\n\nstatic sds makeHint(char **inputargv, int inputargc, int cmdlen, struct commandDocs docs);\n\nstatic void cliAddCommandDocArg(cliCommandArg *cmdArg, redisReply *argMap);\n\nstatic void cliMakeCommandDocArgs(redisReply *arguments, cliCommandArg *result) {\n    for (size_t j = 0; j < arguments->elements; j++) {\n        cliAddCommandDocArg(&result[j], arguments->element[j]);\n    }\n}\n\nstatic void cliAddCommandDocArg(cliCommandArg *cmdArg, redisReply *argMap) {\n    if (argMap->type != REDIS_REPLY_MAP && argMap->type != REDIS_REPLY_ARRAY) {\n        return;\n    }\n\n    for (size_t i = 0; i < argMap->elements; i += 2) {\n        assert(argMap->element[i]->type == REDIS_REPLY_STRING);\n        char *key = argMap->element[i]->str;",
      "TEST_BEGIN(huge_bind_thread) {\n\tunsigned arena1, arena2;\n\tsize_t sz = sizeof(unsigned);\n\n\t/* Bind to a manual arena. */\n\texpect_d_eq(mallctl(\"arenas.create\", &arena1, &sz, NULL, 0), 0,\n\t    \"Failed to create arena\");\n\texpect_d_eq(mallctl(\"thread.arena\", NULL, NULL, &arena1,\n\t    sizeof(arena1)), 0, \"Fail to bind thread\");\n\n\tvoid *ptr = mallocx(HUGE_SZ, 0);\n\texpect_ptr_not_null(ptr, \"Fail to allocate huge size\");\n\texpect_d_eq(mallctl(\"arenas.lookup\", &arena2, &sz, &ptr,\n\t    sizeof(ptr)), 0, \"Unexpected mallctl() failure\");\n\texpect_u_eq(arena1, arena2, \"Wrong arena used after binding\");\n\tdallocx(ptr, 0);\n\n\t/* Switch back to arena 0. */\n\ttest_skip_if(have_percpu_arena &&\n\t    PERCPU_ARENA_ENABLED(opt_percpu_arena));\n\tarena2 = 0;\n\texpect_d_eq(mallctl(\"thread.arena\", NULL, NULL, &arena2,\n\t    sizeof(arena2)), 0, \"Fail to bind thread\");\n\tptr = mallocx(SMALL_SZ, MALLOCX_TCACHE_NONE);\n\texpect_d_eq(mallctl(\"arenas.lookup\", &arena2, &sz, &ptr,\n\t    sizeof(ptr)), 0, \"Unexpected mallctl() failure\");\n\texpect_u_eq(arena2, 0, \"Wrong arena used after binding\");\n\tdallocx(ptr, MALLOCX_TCACHE_NONE);\n\n\t/* Then huge allocation should use the huge arena. */\n\tptr = mallocx(HUGE_SZ, 0);\n\texpect_ptr_not_null(ptr, \"Fail to allocate huge size\");\n\texpect_d_eq(mallctl(\"arenas.lookup\", &arena2, &sz, &ptr,\n\t    sizeof(ptr)), 0, \"Unexpected mallctl() failure\");\n\texpect_u_ne(arena2, 0, \"Wrong arena used after binding\");\n\texpect_u_ne(arena1, arena2, \"Wrong arena used after binding\");\n\tdallocx(ptr, 0);\n}\nTEST_END\n\nTEST_BEGIN(huge_mallocx) {\n\tunsigned arena1, arena2;\n\tsize_t sz = sizeof(unsigned);\n\n\texpect_d_eq(mallctl(\"arenas.create\", &arena1, &sz, NULL, 0), 0,\n\t    \"Failed to create arena\");\n\tvoid *huge = mallocx(HUGE_SZ, MALLOCX_ARENA(arena1));\n\texpect_ptr_not_null(huge, \"Fail to allocate huge size\");\n\texpect_d_eq(mallctl(\"arenas.lookup\", &arena2, &sz, &huge,\n\t    sizeof(huge)), 0, \"Unexpected mallctl() failure\");\n\texpect_u_eq(arena1, arena2, \"Wrong arena used for mallocx\");\n\tdallocx(huge, MALLOCX_ARENA(arena1));\n\n\tvoid *huge2 = mallocx(HUGE_SZ, 0);\n\texpect_ptr_not_null(huge, \"Fail to allocate huge size\");\n\texpect_d_eq(mallctl(\"arenas.lookup\", &arena2, &sz, &huge2,\n\t    sizeof(huge2)), 0, \"Unexpected mallctl() failure\");\n\texpect_u_ne(arena1, arena2,\n\t    \"Huge allocation should not come from the manual arena.\");\n\texpect_u_ne(arena2, 0,\n\t    \"Huge allocation should not come from the arena 0.\");\n\tdallocx(huge2, 0);\n}\nTEST_END",
      "    NULL,           /* update_checksum */\n    0,              /* current checksum */\n    0,              /* flags */\n    0,              /* bytes read or written */\n    0,              /* read/write chunk size */\n    { { NULL, 0 } } /* union for io-specific vars */\n};\n\nvoid rioInitWithFile(rio *r, FILE *fp) {\n    *r = rioFileIO;\n    r->io.file.fp = fp;\n    r->io.file.buffered = 0;\n    r->io.file.autosync = 0;\n    r->io.file.reclaim_cache = 0;\n}\n\n/* ------------------- Connection implementation -------------------\n * We use this RIO implementation when reading an RDB file directly from\n * the connection to the memory via rdbLoadRio(), thus this implementation\n * only implements reading from a connection that is, normally,\n * just a socket. */\n\nstatic size_t rioConnWrite(rio *r, const void *buf, size_t len) {\n    UNUSED(r);\n    UNUSED(buf);\n    UNUSED(len);\n    return 0; /* Error, this target does not yet support writing. */\n}\n\n/* Returns 1 or 0 for success/failure. */\nstatic size_t rioConnRead(rio *r, void *buf, size_t len) {\n    size_t avail = sdslen(r->io.conn.buf)-r->io.conn.pos;\n\n    /* If the buffer is too small for the entire request: realloc. */\n    if (sdslen(r->io.conn.buf) + sdsavail(r->io.conn.buf) < len)\n        r->io.conn.buf = sdsMakeRoomFor(r->io.conn.buf, len - sdslen(r->io.conn.buf));\n\n    /* If the remaining unused buffer is not large enough: memmove so that we\n     * can read the rest. */\n    if (len > avail && sdsavail(r->io.conn.buf) < len - avail) {\n        sdsrange(r->io.conn.buf, r->io.conn.pos, -1);\n        r->io.conn.pos = 0;\n    }\n\n    /* Make sure the caller didn't request to read past the limit.\n     * If they didn't we'll buffer till the limit, if they did, we'll\n     * return an error. */\n    if (r->io.conn.read_limit != 0 && r->io.conn.read_limit < r->io.conn.read_so_far + len) {\n        errno = EOVERFLOW;\n        return 0;\n    }\n\n    /* If we don't already have all the data in the sds, read more */\n    while (len > sdslen(r->io.conn.buf) - r->io.conn.pos) {\n        size_t buffered = sdslen(r->io.conn.buf) - r->io.conn.pos;\n        size_t needs = len - buffered;\n        /* Read either what's missing, or PROTO_IOBUF_LEN, the bigger of\n         * the two. */\n        size_t toread = needs < PROTO_IOBUF_LEN ? PROTO_IOBUF_LEN: needs;\n        if (toread > sdsavail(r->io.conn.buf)) toread = sdsavail(r->io.conn.buf);\n        if (r->io.conn.read_limit != 0 &&\n            r->io.conn.read_so_far + buffered + toread > r->io.conn.read_limit)\n        {\n            toread = r->io.conn.read_limit - r->io.conn.read_so_far - buffered;",
      "\tfxp_t b = xparse_fxp(bstr);\n\tfxp_t result = xparse_fxp(resultstr);\n\texpect_true(fxp_close(fxp_add(a, b), result),\n\t    \"Expected %s + %s == %s\", astr, bstr, resultstr);\n}\n\nTEST_BEGIN(test_add_simple) {\n\texpect_add(\"0\", \"0\", \"0\");\n\texpect_add(\"0\", \"1\", \"1\");\n\texpect_add(\"1\", \"1\", \"2\");\n\texpect_add(\"1.5\", \"1.5\", \"3\");\n\texpect_add(\"0.1\", \"0.1\", \"0.2\");\n\texpect_add(\"123\", \"456\", \"579\");\n}\nTEST_END\n\nstatic void\nexpect_sub(const char *astr, const char *bstr, const char* resultstr) {\n\tfxp_t a = xparse_fxp(astr);\n\tfxp_t b = xparse_fxp(bstr);\n\tfxp_t result = xparse_fxp(resultstr);\n\texpect_true(fxp_close(fxp_sub(a, b), result),\n\t    \"Expected %s - %s == %s\", astr, bstr, resultstr);\n}\n\nTEST_BEGIN(test_sub_simple) {\n\texpect_sub(\"0\", \"0\", \"0\");\n\texpect_sub(\"1\", \"0\", \"1\");\n\texpect_sub(\"1\", \"1\", \"0\");\n\texpect_sub(\"3.5\", \"1.5\", \"2\");\n\texpect_sub(\"0.3\", \"0.1\", \"0.2\");\n\texpect_sub(\"456\", \"123\", \"333\");\n}\nTEST_END\n\nstatic void\nexpect_mul(const char *astr, const char *bstr, const char* resultstr) {\n\tfxp_t a = xparse_fxp(astr);\n\tfxp_t b = xparse_fxp(bstr);\n\tfxp_t result = xparse_fxp(resultstr);\n\texpect_true(fxp_close(fxp_mul(a, b), result),\n\t    \"Expected %s * %s == %s\", astr, bstr, resultstr);\n}\n\nTEST_BEGIN(test_mul_simple) {\n\texpect_mul(\"0\", \"0\", \"0\");\n\texpect_mul(\"1\", \"0\", \"0\");\n\texpect_mul(\"1\", \"1\", \"1\");\n\texpect_mul(\"1.5\", \"1.5\", \"2.25\");\n\texpect_mul(\"100.0\", \"10\", \"1000\");\n\texpect_mul(\".1\", \"10\", \"1\");\n}\nTEST_END\n\nstatic void\nexpect_div(const char *astr, const char *bstr, const char* resultstr) {\n\tfxp_t a = xparse_fxp(astr);\n\tfxp_t b = xparse_fxp(bstr);\n\tfxp_t result = xparse_fxp(resultstr);\n\texpect_true(fxp_close(fxp_div(a, b), result),\n\t    \"Expected %s / %s == %s\", astr, bstr, resultstr);\n}\n\nTEST_BEGIN(test_div_simple) {",
      "/*\n** $Id: ldump.c,v 2.8.1.1 2007/12/27 13:02:25 roberto Exp $\n** save precompiled Lua chunks\n** See Copyright Notice in lua.h\n*/\n\n#include <stddef.h>\n\n#define ldump_c\n#define LUA_CORE\n\n#include \"lua.h\"\n\n#include \"lobject.h\"\n#include \"lstate.h\"\n#include \"lundump.h\"\n\ntypedef struct {\n lua_State* L;\n lua_Writer writer;\n void* data;\n int strip;\n int status;\n} DumpState;\n\n#define DumpMem(b,n,size,D)\tDumpBlock(b,(n)*(size),D)\n#define DumpVar(x,D)\t \tDumpMem(&x,1,sizeof(x),D)\n\nstatic void DumpBlock(const void* b, size_t size, DumpState* D)\n{\n if (D->status==0)\n {\n  lua_unlock(D->L);\n  D->status=(*D->writer)(D->L,b,size,D->data);\n  lua_lock(D->L);\n }\n}\n\nstatic void DumpChar(int y, DumpState* D)\n{\n char x=(char)y;\n DumpVar(x,D);\n}\n\nstatic void DumpInt(int x, DumpState* D)\n{\n DumpVar(x,D);\n}\n\nstatic void DumpNumber(lua_Number x, DumpState* D)\n{\n DumpVar(x,D);\n}\n\nstatic void DumpVector(const void* b, int n, size_t size, DumpState* D)\n{\n DumpInt(n,D);\n DumpMem(b,n,size,D);\n}\n\nstatic void DumpString(const TString* s, DumpState* D)\n{\n if (s==NULL)\n {",
      "    cache_bin_sz_t nfill, cache_bin_sz_t nflush) {\n\tbool success;\n\tassert_true(cache_bin_ncached_get_local(bin, info) == 0, \"\");\n\n\tfor (cache_bin_sz_t i = 0; i < nfill; i++) {\n\t\tsuccess = cache_bin_dalloc_easy(bin, &ptrs[i]);\n\t\texpect_true(success, \"\");\n\t}\n\n\tCACHE_BIN_PTR_ARRAY_DECLARE(arr, nflush);\n\tcache_bin_init_ptr_array_for_flush(bin, info, &arr, nflush);\n\tfor (cache_bin_sz_t i = 0; i < nflush; i++) {\n\t\texpect_ptr_eq(arr.ptr[i], &ptrs[nflush - i - 1], \"\");\n\t}\n\tcache_bin_finish_flush(bin, info, &arr, nflush);\n\n\texpect_true(cache_bin_ncached_get_local(bin, info) == nfill - nflush,\n\t    \"\");\n\twhile (cache_bin_ncached_get_local(bin, info) > 0) {\n\t\tcache_bin_alloc(bin, &success);\n\t}\n}\n\nstatic void\ndo_batch_alloc_test(cache_bin_t *bin, cache_bin_info_t *info, void **ptrs,\n    cache_bin_sz_t nfill, size_t batch) {\n\tassert_true(cache_bin_ncached_get_local(bin, info) == 0, \"\");\n\tCACHE_BIN_PTR_ARRAY_DECLARE(arr, nfill);\n\tcache_bin_init_ptr_array_for_fill(bin, info, &arr, nfill);\n\tfor (cache_bin_sz_t i = 0; i < nfill; i++) {\n\t\tarr.ptr[i] = &ptrs[i];\n\t}\n\tcache_bin_finish_fill(bin, info, &arr, nfill);\n\tassert_true(cache_bin_ncached_get_local(bin, info) == nfill, \"\");\n\tcache_bin_low_water_set(bin);\n\n\tvoid **out = malloc((batch + 1) * sizeof(void *));\n\tsize_t n = cache_bin_alloc_batch(bin, batch, out);\n\tassert_true(n == ((size_t)nfill < batch ? (size_t)nfill : batch), \"\");\n\tfor (cache_bin_sz_t i = 0; i < (cache_bin_sz_t)n; i++) {\n\t\texpect_ptr_eq(out[i], &ptrs[i], \"\");\n\t}\n\texpect_true(cache_bin_low_water_get(bin, info) == nfill -\n\t    (cache_bin_sz_t)n, \"\");\n\twhile (cache_bin_ncached_get_local(bin, info) > 0) {\n\t\tbool success;\n\t\tcache_bin_alloc(bin, &success);\n\t}\n\tfree(out);\n}\n\nstatic void\ntest_bin_init(cache_bin_t *bin, cache_bin_info_t *info) {\n\tsize_t size;\n\tsize_t alignment;\n\tcache_bin_info_compute_alloc(info, 1, &size, &alignment);\n\tvoid *mem = mallocx(size, MALLOCX_ALIGN(alignment));\n\tassert_ptr_not_null(mem, \"Unexpected mallocx failure\");\n\n\tsize_t cur_offset = 0;\n\tcache_bin_preincrement(info, 1, mem, &cur_offset);\n\tcache_bin_init(bin, info, mem, &cur_offset);\n\tcache_bin_postincrement(info, 1, mem, &cur_offset);\n\tassert_zu_eq(cur_offset, size, \"Should use all requested memory\");"
    ]
  }
];